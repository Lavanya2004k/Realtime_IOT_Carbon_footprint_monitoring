{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kafka-python-ng in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (2.2.3)\n"
     ]
    }
   ],
   "source": [
    "! pip install kafka-python-ng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent: {'country': 'Afghanistan', 'year': 1949, 'carbon_emission': 0.014656, 'activity': 0.014656}\n",
      "Sent: {'country': 'Afghanistan', 'year': 1950, 'carbon_emission': 0.021068, 'activity': 0.084272}\n",
      "Sent: {'country': 'Afghanistan', 'year': 1951, 'carbon_emission': 0.025648, 'activity': 0.0916}\n",
      "Sent: {'country': 'Afghanistan', 'year': 1952, 'carbon_emission': 0.031708, 'activity': 0.0916}\n",
      "Sent: {'country': 'Afghanistan', 'year': 1953, 'carbon_emission': 0.037949, 'activity': 0.106256}\n",
      "Sent: {'country': 'Afghanistan', 'year': 1954, 'carbon_emission': 0.042502, 'activity': 0.106256}\n",
      "Sent: {'country': 'Afghanistan', 'year': 1955, 'carbon_emission': 0.062288, 'activity': 0.153888}\n",
      "Sent: {'country': 'Afghanistan', 'year': 1956, 'carbon_emission': 0.062288, 'activity': 0.1832}\n",
      "Sent: {'country': 'Afghanistan', 'year': 1957, 'carbon_emission': 0.076944, 'activity': 0.29312}\n",
      "Sent: {'country': 'Afghanistan', 'year': 1958, 'carbon_emission': 0.0916, 'activity': 0.32976}\n",
      "Sent: {'country': 'Afghanistan', 'year': 1959, 'carbon_emission': 0.10992, 'activity': 0.384571}\n",
      "Sent: {'country': 'Afghanistan', 'year': 1960, 'carbon_emission': 0.127115, 'activity': 0.413885}\n",
      "Sent: {'country': 'Afghanistan', 'year': 1961, 'carbon_emission': 0.175872, 'activity': 0.490798}\n",
      "Sent: {'country': 'Afghanistan', 'year': 1962, 'carbon_emission': 0.296784, 'activity': 0.688594}\n",
      "Sent: {'country': 'Afghanistan', 'year': 1963, 'carbon_emission': 0.263808, 'activity': 0.706736}\n",
      "Sent: {'country': 'Afghanistan', 'year': 1964, 'carbon_emission': 0.300448, 'activity': 0.838551}\n",
      "Sent: {'country': 'Afghanistan', 'year': 1965, 'carbon_emission': 0.381056, 'activity': 1.006917}\n",
      "Sent: {'country': 'Afghanistan', 'year': 1966, 'carbon_emission': 0.428688, 'activity': 1.091159}\n",
      "Sent: {'country': 'Afghanistan', 'year': 1967, 'carbon_emission': 0.399376, 'activity': 1.281865}\n",
      "Sent: {'country': 'Afghanistan', 'year': 1968, 'carbon_emission': 0.332429, 'activity': 1.223391}\n",
      "Sent: {'country': 'Afghanistan', 'year': 1969, 'carbon_emission': 0.362736, 'activity': 0.941232}\n",
      "Sent: {'country': 'Afghanistan', 'year': 1970, 'carbon_emission': 0.436974, 'activity': 1.670397}\n",
      "Sent: {'country': 'Afghanistan', 'year': 1971, 'carbon_emission': 0.359072, 'activity': 1.893554}\n",
      "Sent: {'country': 'Afghanistan', 'year': 1972, 'carbon_emission': 0.190528, 'activity': 1.530347}\n",
      "Sent: {'country': 'Afghanistan', 'year': 1973, 'carbon_emission': 0.310745, 'activity': 1.635454}\n",
      "Sent: {'country': 'Afghanistan', 'year': 1974, 'carbon_emission': 0.304695, 'activity': 1.913152}\n",
      "Sent: {'country': 'Afghanistan', 'year': 1975, 'carbon_emission': 0.398689, 'activity': 2.121383}\n",
      "Sent: {'country': 'Afghanistan', 'year': 1976, 'carbon_emission': 0.425024, 'activity': 1.980859}\n",
      "Sent: {'country': 'Afghanistan', 'year': 1977, 'carbon_emission': 0.450672, 'activity': 2.384175}\n",
      "Sent: {'country': 'Afghanistan', 'year': 1978, 'carbon_emission': 0.576226, 'activity': 2.1533}\n",
      "Sent: {'country': 'Afghanistan', 'year': 1979, 'carbon_emission': 0.351744, 'activity': 2.232754}\n",
      "Sent: {'country': 'Afghanistan', 'year': 1980, 'carbon_emission': 0.315762, 'activity': 1.756302}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 44\u001b[0m\n\u001b[1;32m     42\u001b[0m         future \u001b[38;5;241m=\u001b[39m producer\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcarbonfootprint\u001b[39m\u001b[38;5;124m'\u001b[39m, data)\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSent: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 44\u001b[0m         \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m  \n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError sending data: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from kafka import KafkaProducer\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "\n",
    "\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=['localhost:9092'],\n",
    "    value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    ")\n",
    "\n",
    "\n",
    "def process_csv(file_path):\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        \n",
    "        required_columns = ['Country', 'Year', 'Coal', 'Total']\n",
    "        if not all(col in df.columns for col in required_columns):\n",
    "            raise ValueError(f\"CSV file must contain the following columns: {required_columns}\")\n",
    "        \n",
    "        \n",
    "        df = df[df['Coal'] > 0]\n",
    "        \n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            yield {\n",
    "                \"country\": row['Country'],\n",
    "                \"year\": int(row['Year']),\n",
    "                \"carbon_emission\": float(row['Coal']),  \n",
    "                \"activity\": row['Total']\n",
    "            }\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing CSV: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "try:\n",
    "    file_path = \"/Users/kdn_aikothalavanya/Downloads/GCB2022v27_MtCO2_flat.csv\"\n",
    "    for data in process_csv(file_path):\n",
    "        future = producer.send('carbonfootprint', data)\n",
    "        print(f\"Sent: {data}\")\n",
    "        time.sleep(0.5)  \n",
    "except Exception as e:\n",
    "    print(f\"Error sending data: {e}\")\n",
    "finally:\n",
    "    producer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pycountry in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (24.6.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install pycountry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent: {'country': 'Afghanistan', 'iso_alpha3': 'AFG', 'year': 1949, 'carbon_emission': 0.014656, 'activity': 0.014656}\n",
      "Sent: {'country': 'Afghanistan', 'iso_alpha3': 'AFG', 'year': 1950, 'carbon_emission': 0.021068, 'activity': 0.084272}\n",
      "Sent: {'country': 'Afghanistan', 'iso_alpha3': 'AFG', 'year': 1951, 'carbon_emission': 0.025648, 'activity': 0.0916}\n",
      "Sent: {'country': 'Afghanistan', 'iso_alpha3': 'AFG', 'year': 1952, 'carbon_emission': 0.031708, 'activity': 0.0916}\n",
      "Sent: {'country': 'Afghanistan', 'iso_alpha3': 'AFG', 'year': 1953, 'carbon_emission': 0.037949, 'activity': 0.106256}\n",
      "Sent: {'country': 'Afghanistan', 'iso_alpha3': 'AFG', 'year': 1954, 'carbon_emission': 0.042502, 'activity': 0.106256}\n",
      "Sent: {'country': 'Afghanistan', 'iso_alpha3': 'AFG', 'year': 1955, 'carbon_emission': 0.062288, 'activity': 0.153888}\n",
      "Sent: {'country': 'Afghanistan', 'iso_alpha3': 'AFG', 'year': 1956, 'carbon_emission': 0.062288, 'activity': 0.1832}\n",
      "Sent: {'country': 'Afghanistan', 'iso_alpha3': 'AFG', 'year': 1957, 'carbon_emission': 0.076944, 'activity': 0.29312}\n",
      "Sent: {'country': 'Afghanistan', 'iso_alpha3': 'AFG', 'year': 1958, 'carbon_emission': 0.0916, 'activity': 0.32976}\n",
      "Sent: {'country': 'Afghanistan', 'iso_alpha3': 'AFG', 'year': 1959, 'carbon_emission': 0.10992, 'activity': 0.384571}\n",
      "Sent: {'country': 'Afghanistan', 'iso_alpha3': 'AFG', 'year': 1960, 'carbon_emission': 0.127115, 'activity': 0.413885}\n",
      "Sent: {'country': 'Afghanistan', 'iso_alpha3': 'AFG', 'year': 1961, 'carbon_emission': 0.175872, 'activity': 0.490798}\n",
      "Sent: {'country': 'Afghanistan', 'iso_alpha3': 'AFG', 'year': 1962, 'carbon_emission': 0.296784, 'activity': 0.688594}\n",
      "Sent: {'country': 'Afghanistan', 'iso_alpha3': 'AFG', 'year': 1963, 'carbon_emission': 0.263808, 'activity': 0.706736}\n",
      "Sent: {'country': 'Afghanistan', 'iso_alpha3': 'AFG', 'year': 1964, 'carbon_emission': 0.300448, 'activity': 0.838551}\n",
      "Sent: {'country': 'Afghanistan', 'iso_alpha3': 'AFG', 'year': 1965, 'carbon_emission': 0.381056, 'activity': 1.006917}\n",
      "Sent: {'country': 'Afghanistan', 'iso_alpha3': 'AFG', 'year': 1966, 'carbon_emission': 0.428688, 'activity': 1.091159}\n",
      "Sent: {'country': 'Afghanistan', 'iso_alpha3': 'AFG', 'year': 1967, 'carbon_emission': 0.399376, 'activity': 1.281865}\n",
      "Sent: {'country': 'Afghanistan', 'iso_alpha3': 'AFG', 'year': 1968, 'carbon_emission': 0.332429, 'activity': 1.223391}\n",
      "Sent: {'country': 'Afghanistan', 'iso_alpha3': 'AFG', 'year': 1969, 'carbon_emission': 0.362736, 'activity': 0.941232}\n",
      "Sent: {'country': 'Afghanistan', 'iso_alpha3': 'AFG', 'year': 1970, 'carbon_emission': 0.436974, 'activity': 1.670397}\n",
      "Sent: {'country': 'Afghanistan', 'iso_alpha3': 'AFG', 'year': 1971, 'carbon_emission': 0.359072, 'activity': 1.893554}\n",
      "Sent: {'country': 'Afghanistan', 'iso_alpha3': 'AFG', 'year': 1972, 'carbon_emission': 0.190528, 'activity': 1.530347}\n",
      "Sent: {'country': 'Afghanistan', 'iso_alpha3': 'AFG', 'year': 1973, 'carbon_emission': 0.310745, 'activity': 1.635454}\n",
      "Sent: {'country': 'Afghanistan', 'iso_alpha3': 'AFG', 'year': 1974, 'carbon_emission': 0.304695, 'activity': 1.913152}\n",
      "Sent: {'country': 'Afghanistan', 'iso_alpha3': 'AFG', 'year': 1975, 'carbon_emission': 0.398689, 'activity': 2.121383}\n",
      "Sent: {'country': 'Afghanistan', 'iso_alpha3': 'AFG', 'year': 1976, 'carbon_emission': 0.425024, 'activity': 1.980859}\n",
      "Sent: {'country': 'Afghanistan', 'iso_alpha3': 'AFG', 'year': 1977, 'carbon_emission': 0.450672, 'activity': 2.384175}\n",
      "Sent: {'country': 'Afghanistan', 'iso_alpha3': 'AFG', 'year': 1978, 'carbon_emission': 0.576226, 'activity': 2.1533}\n",
      "Sent: {'country': 'Afghanistan', 'iso_alpha3': 'AFG', 'year': 1979, 'carbon_emission': 0.351744, 'activity': 2.232754}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 59\u001b[0m\n\u001b[1;32m     57\u001b[0m         future \u001b[38;5;241m=\u001b[39m producer\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcarbonfootprint\u001b[39m\u001b[38;5;124m'\u001b[39m, data)\n\u001b[1;32m     58\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSent: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 59\u001b[0m         \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Throttle to avoid overwhelming Kafka\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError sending data: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from kafka import KafkaProducer\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import pycountry\n",
    "\n",
    "# Initialize Kafka Producer\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=['localhost:9092'],\n",
    "    value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    ")\n",
    "\n",
    "# Function to map country names to ISO 3166-1 alpha-3 codes\n",
    "def get_iso_alpha3(country_name):\n",
    "    try:\n",
    "        return pycountry.countries.lookup(country_name).alpha_3\n",
    "    except LookupError:\n",
    "        return None\n",
    "\n",
    "# Function to process CSV and prepare data\n",
    "def process_csv(file_path):\n",
    "    try:\n",
    "        # Read CSV\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Ensure required columns are present\n",
    "        required_columns = ['Country', 'Year', 'Coal', 'Total']\n",
    "        if not all(col in df.columns for col in required_columns):\n",
    "            raise ValueError(f\"CSV file must contain the following columns: {required_columns}\")\n",
    "        \n",
    "        # Filter rows where 'Coal' > 0\n",
    "        df = df[df['Coal'] > 0]\n",
    "        \n",
    "        # Add ISO 3166-1 alpha-3 codes\n",
    "        df['ISO_alpha3'] = df['Country'].apply(get_iso_alpha3)\n",
    "        \n",
    "        # Drop rows with missing ISO alpha-3 codes\n",
    "        df = df.dropna(subset=['ISO_alpha3'])\n",
    "        \n",
    "        # Yield each row as a dictionary\n",
    "        for _, row in df.iterrows():\n",
    "            yield {\n",
    "                \"country\": row['Country'],\n",
    "                \"iso_alpha3\": row['ISO_alpha3'],\n",
    "                \"year\": int(row['Year']),\n",
    "                \"carbon_emission\": float(row['Coal']),\n",
    "                \"activity\": row['Total']\n",
    "            }\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing CSV: {e}\")\n",
    "        return []\n",
    "\n",
    "# Main block to read CSV and send data to Kafka\n",
    "try:\n",
    "    file_path = \"/Users/kdn_aikothalavanya/Downloads/GCB2022v27_MtCO2_flat.csv\"\n",
    "    for data in process_csv(file_path):\n",
    "        future = producer.send('carbonfootprint', data)\n",
    "        print(f\"Sent: {data}\")\n",
    "        time.sleep(0.5)  # Throttle to avoid overwhelming Kafka\n",
    "except Exception as e:\n",
    "    print(f\"Error sending data: {e}\")\n",
    "finally:\n",
    "    producer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kafka-python in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (2.1.2)\n",
      "Requirement already satisfied: gym in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (0.26.2)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (1.26.4)\n",
      "Requirement already satisfied: tensorflow in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (2.11.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from gym) (3.1.1)\n",
      "Requirement already satisfied: gym_notices>=0.0.4 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from gym) (0.0.8)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from tensorflow) (3.13.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from tensorflow) (24.2)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from tensorflow) (4.21.12)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from tensorflow) (75.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from tensorflow) (1.51.1)\n",
      "Requirement already satisfied: tensorboard<2.12,>=2.11 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from tensorflow) (2.11.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from tensorflow) (2.11.0)\n",
      "Requirement already satisfied: keras<2.12,>=2.11.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from tensorflow) (2.11.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.38.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (3.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.32.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2025.1.31)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "! pip install kafka-python gym numpy tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "class CarbonEmissionsEnv(gym.Env):\n",
    "    def __init__(self, kafka_consumer):\n",
    "        super(CarbonEmissionsEnv, self).__init__()\n",
    "        self.kafka_consumer = kafka_consumer\n",
    "        \n",
    "        # Define action and observation space\n",
    "        self.action_space = spaces.Discrete(3)  # Actions: 0 - Reduce coal, 1 - Promote renewables, 2 - Recommend policy\n",
    "        self.observation_space = spaces.Box(low=0, high=np.inf, shape=(4,), dtype=np.float32)  # Example: Country, Year, Carbon, Activity\n",
    "        \n",
    "        # Initialize state\n",
    "        self.state = None\n",
    "        self.done = False\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Apply the chosen action and calculate reward\n",
    "        reward = 0\n",
    "        if action == 0:  # Reduce coal\n",
    "            reward = -self.state[2] * 0.1\n",
    "        elif action == 1:  # Promote renewables\n",
    "            reward = -self.state[3] * 0.2\n",
    "        elif action == 2:  # Recommend policy\n",
    "            reward = -self.state[2] * 0.05\n",
    "        \n",
    "        # Simulate new state from Kafka\n",
    "        self.state = self._get_new_state()\n",
    "        \n",
    "        # Check if episode is done\n",
    "        self.done = self.state is None\n",
    "        \n",
    "        return self.state, reward, self.done, {}\n",
    "    \n",
    "    def reset(self):\n",
    "        # Reset state using Kafka data\n",
    "        self.state = self._get_new_state()\n",
    "        return self.state\n",
    "    \n",
    "    def _get_new_state(self):\n",
    "        # Fetch new data from Kafka\n",
    "        for message in self.kafka_consumer:\n",
    "            data = json.loads(message.value)\n",
    "            return np.array([data['country'], data['year'], data['carbon_emission'], data['activity']])\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class CarbonEmissionsEnv:\n",
    "    def __init__(self, consumer):\n",
    "        self.consumer = consumer  # Kafka consumer instance\n",
    "        self.current_state = None\n",
    "        self.done = False\n",
    "\n",
    "    def reset(self):\n",
    "        # Define the initial state (e.g., initial emissions, activity levels, etc.)\n",
    "        self.current_state = np.zeros(4)  # Example: [country, year, emission, activity]\n",
    "        self.done = False\n",
    "        return self.current_state\n",
    "\n",
    "    def step(self, action):\n",
    "        # Simulate environment dynamics based on the agent's action\n",
    "        # For simplicity, you can use random data to mock state transitions and rewards\n",
    "        next_state = self.current_state + np.random.rand(4)  # Mock transition\n",
    "        reward = -1 if action == 0 else 1  # Mock reward system\n",
    "        self.done = np.random.rand() > 0.95  # Mock end condition\n",
    "        return next_state, reward, self.done, {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "\n",
    "consumer = KafkaConsumer(\n",
    "    'carbonfootprint',\n",
    "    bootstrap_servers=['localhost:9092'],\n",
    "    auto_offset_reset='earliest',\n",
    "    enable_auto_commit=True,\n",
    "    value_deserializer=lambda x: json.loads(x.decode('utf-8'))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CarbonEmissionsEnv(consumer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 15:28:19.930260: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from collections import deque\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tf_env/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0/100, score: 0\n",
      "episode: 1/100, score: 1\n",
      "episode: 2/100, score: 12\n",
      "episode: 3/100, score: 36\n",
      "episode: 4/100, score: 4\n",
      "episode: 5/100, score: 2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 91\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# Replay experiences\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 46\u001b[0m, in \u001b[0;36mDQNAgent.replay\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m     45\u001b[0m     target \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mamax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpredict(next_state, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m---> 46\u001b[0m target_f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m target_f[\u001b[38;5;241m0\u001b[39m][action] \u001b[38;5;241m=\u001b[39m target\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mfit(state, target_f, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py:557\u001b[0m, in \u001b[0;36mTensorFlowTrainer.predict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[1;32m    555\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39mcatch_stop_iteration():\n\u001b[0;32m--> 557\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[1;32m    558\u001b[0m         callbacks\u001b[38;5;241m.\u001b[39mon_predict_batch_begin(step)\n\u001b[1;32m    559\u001b[0m         data \u001b[38;5;241m=\u001b[39m get_data(iterator)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py:734\u001b[0m, in \u001b[0;36mTFEpochIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    733\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_epoch_iterator\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_env/lib/python3.10/site-packages/keras/src/trainers/epoch_iterator.py:102\u001b[0m, in \u001b[0;36mEpochIterator._enumerate_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m steps_per_epoch \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_per_epoch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 102\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_seen \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, steps_per_epoch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_per_execution):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_env/lib/python3.10/site-packages/tensorflow/python/data/ops/dataset_ops.py:501\u001b[0m, in \u001b[0;36mDatasetV2.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly() \u001b[38;5;129;01mor\u001b[39;00m ops\u001b[38;5;241m.\u001b[39minside_function():\n\u001b[1;32m    500\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcolocate_with(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variant_tensor):\n\u001b[0;32m--> 501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43miterator_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOwnedIterator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    503\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.data.Dataset` only supports Python-style \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    504\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miteration in eager mode or within tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_env/lib/python3.10/site-packages/tensorflow/python/data/ops/iterator_ops.py:705\u001b[0m, in \u001b[0;36mOwnedIterator.__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    701\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (components \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m element_spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    702\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    703\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `dataset` is provided, `element_spec` and `components` must \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    704\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot be specified.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 705\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_next_call_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_env/lib/python3.10/site-packages/tensorflow/python/data/ops/iterator_ops.py:744\u001b[0m, in \u001b[0;36mOwnedIterator._create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    741\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(fulltype\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(\n\u001b[1;32m    742\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_output_types)\n\u001b[1;32m    743\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator_resource\u001b[38;5;241m.\u001b[39mop\u001b[38;5;241m.\u001b[39mexperimental_set_type(fulltype)\n\u001b[0;32m--> 744\u001b[0m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_variant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_env/lib/python3.10/site-packages/tensorflow/python/ops/gen_dataset_ops.py:3478\u001b[0m, in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   3476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[1;32m   3477\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3478\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3479\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMakeIterator\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3480\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m   3481\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Define the DQNAgent class\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95  # Discount factor\n",
    "        self.epsilon = 1.0  # Exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "    \n",
    "    def _build_model(self):\n",
    "        model = models.Sequential([\n",
    "            layers.Dense(24, input_dim=self.state_size, activation='relu'),\n",
    "            layers.Dense(24, activation='relu'),\n",
    "            layers.Dense(self.action_size, activation='linear')\n",
    "        ])\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate), loss='mse')\n",
    "        return model\n",
    "    \n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        q_values = self.model.predict(state, verbose=0)\n",
    "        return np.argmax(q_values[0])\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def replay(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target += self.gamma * np.amax(self.model.predict(next_state, verbose=0)[0])\n",
    "            target_f = self.model.predict(state, verbose=0)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "# Define the environment class (for simplicity, replace with your environment)\n",
    "class CarbonEmissionsEnv:\n",
    "    def __init__(self, consumer):\n",
    "        self.consumer = consumer\n",
    "        self.state = None\n",
    "    \n",
    "    def reset(self):\n",
    "        # Simulate environment reset\n",
    "        self.state = np.random.rand(4)  # Replace with actual logic\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Simulate environment step\n",
    "        reward = np.random.rand()  # Replace with actual reward logic\n",
    "        next_state = np.random.rand(4)  # Replace with actual next state logic\n",
    "        done = np.random.rand() < 0.1  # End episode randomly (10% chance)\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "# Initialize environment and agent\n",
    "state_size = 4\n",
    "action_size = 3\n",
    "env = CarbonEmissionsEnv(None)  # Replace `None` with your Kafka consumer\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "# Train the agent\n",
    "episodes = 100\n",
    "for e in range(episodes):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    for time in range(500):\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            print(f\"episode: {e}/{episodes}, score: {time}\")\n",
    "            break\n",
    "    # Replay experiences\n",
    "    agent.replay(32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 01:39:31.464729: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/opt/anaconda3/envs/torch_env/lib/python3.9/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0/100, score: 5, total_reward: 6.81\n",
      "episode: 1/100, score: 4, total_reward: 1.66\n",
      "episode: 2/100, score: 3, total_reward: 4.45\n",
      "episode: 3/100, score: 34, total_reward: 30.57\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 106\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisode: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisodes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, total_reward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_reward\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    105\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     agent\u001b[38;5;241m.\u001b[39madjust_parameters(total_reward)\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# Calculate accuracy\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 47\u001b[0m, in \u001b[0;36mDQNAgent.replay\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     45\u001b[0m target \u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m---> 47\u001b[0m     target \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mamax(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     48\u001b[0m target_f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpredict(state, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     49\u001b[0m target_f[\u001b[38;5;241m0\u001b[39m][action] \u001b[38;5;241m=\u001b[39m target\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch_env/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch_env/lib/python3.9/site-packages/keras/src/backend/tensorflow/trainer.py:557\u001b[0m, in \u001b[0;36mTensorFlowTrainer.predict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[1;32m    555\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39mcatch_stop_iteration():\n\u001b[0;32m--> 557\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[1;32m    558\u001b[0m         callbacks\u001b[38;5;241m.\u001b[39mon_predict_batch_begin(step)\n\u001b[1;32m    559\u001b[0m         data \u001b[38;5;241m=\u001b[39m get_data(iterator)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch_env/lib/python3.9/site-packages/keras/src/backend/tensorflow/trainer.py:734\u001b[0m, in \u001b[0;36mTFEpochIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    733\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_epoch_iterator\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch_env/lib/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:112\u001b[0m, in \u001b[0;36mEpochIterator._enumerate_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m step, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_iterator\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_batches \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_seen \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_batches:\n\u001b[0;32m--> 112\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_seen \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch_env/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py:501\u001b[0m, in \u001b[0;36mDatasetV2.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly() \u001b[38;5;129;01mor\u001b[39;00m ops\u001b[38;5;241m.\u001b[39minside_function():\n\u001b[1;32m    500\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcolocate_with(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variant_tensor):\n\u001b[0;32m--> 501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43miterator_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOwnedIterator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    503\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.data.Dataset` only supports Python-style \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    504\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miteration in eager mode or within tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch_env/lib/python3.9/site-packages/tensorflow/python/data/ops/iterator_ops.py:705\u001b[0m, in \u001b[0;36mOwnedIterator.__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    701\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (components \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m element_spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    702\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    703\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `dataset` is provided, `element_spec` and `components` must \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    704\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot be specified.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 705\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_next_call_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch_env/lib/python3.9/site-packages/tensorflow/python/data/ops/iterator_ops.py:744\u001b[0m, in \u001b[0;36mOwnedIterator._create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    741\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(fulltype\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(\n\u001b[1;32m    742\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_output_types)\n\u001b[1;32m    743\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator_resource\u001b[38;5;241m.\u001b[39mop\u001b[38;5;241m.\u001b[39mexperimental_set_type(fulltype)\n\u001b[0;32m--> 744\u001b[0m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_variant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch_env/lib/python3.9/site-packages/tensorflow/python/ops/gen_dataset_ops.py:3478\u001b[0m, in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   3476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[1;32m   3477\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3478\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3479\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMakeIterator\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3480\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m   3481\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Define the DQNAgent class\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95  # Discount factor\n",
    "        self.epsilon = 1.0  # Exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.reward_threshold = 10  # Initial reward threshold\n",
    "        self.model = self._build_model()\n",
    "        self.optimizations = []\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = models.Sequential([\n",
    "            layers.Dense(24, input_dim=self.state_size, activation='relu'),\n",
    "            layers.Dense(24, activation='relu'),\n",
    "            layers.Dense(self.action_size, activation='linear')\n",
    "        ])\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate), loss='mse')\n",
    "        return model\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        q_values = self.model.predict(state, verbose=0)\n",
    "        return np.argmax(q_values[0])\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target += self.gamma * np.amax(self.model.predict(next_state, verbose=0)[0])\n",
    "            target_f = self.model.predict(state, verbose=0)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "\n",
    "        # Adjust epsilon dynamically\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def adjust_parameters(self, avg_reward):\n",
    "        \"\"\"Adjusts learning parameters based on performance.\"\"\"\n",
    "        if avg_reward > self.reward_threshold:\n",
    "            self.reward_threshold += 5\n",
    "            self.epsilon_decay *= 0.99\n",
    "            self.learning_rate *= 1.05\n",
    "            self.model.optimizer.learning_rate.assign(self.learning_rate)\n",
    "            self.optimizations.append(f\"Adjusted reward threshold to {self.reward_threshold}, epsilon_decay to {self.epsilon_decay:.4f}, learning_rate to {self.learning_rate:.4f}.\")\n",
    "\n",
    "# Define the environment class\n",
    "class CarbonEmissionsEnv:\n",
    "    def __init__(self, consumer):\n",
    "        self.consumer = consumer\n",
    "        self.state = None\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.random.rand(4)  # Replace with actual logic\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        reward = np.random.rand() * (action + 1)  # Simulated reward logic\n",
    "        next_state = np.random.rand(4)  # Replace with actual logic\n",
    "        done = np.random.rand() < 0.1  # End episode randomly\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "# Initialize environment and agent\n",
    "state_size = 4\n",
    "action_size = 3\n",
    "env = CarbonEmissionsEnv(None)\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "# Train the agent\n",
    "episodes = 100\n",
    "successes = 0\n",
    "for e in range(episodes):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    total_reward = 0\n",
    "    for time in range(500):\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            if total_reward > agent.reward_threshold:\n",
    "                successes += 1\n",
    "            print(f\"episode: {e}/{episodes}, score: {time}, total_reward: {total_reward:.2f}\")\n",
    "            break\n",
    "    agent.replay(32)\n",
    "    agent.adjust_parameters(total_reward)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = (successes / episodes) * 100\n",
    "print(f\"Training Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Display optimizations\n",
    "print(\"Optimizations:\")\n",
    "for optimization in agent.optimizations:\n",
    "    print(optimization)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tf_env/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/anaconda3/envs/tf_env/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0/100, score: 21, total_reward: 2.20\n",
      "Optimizations triggered:\n",
      "- Increased epsilon_decay to 0.9960\n",
      "episode: 1/100, score: 28, total_reward: 3.15\n",
      "Optimizations triggered:\n",
      "- Reduced epsilon_decay to 0.9940\n",
      "- Reduced learning_rate to 0.000900\n",
      "episode: 2/100, score: 5, total_reward: 0.60\n",
      "Optimizations triggered:\n",
      "- Increased epsilon_decay to 0.9950\n",
      "- Increased learning_rate to 0.000950\n",
      "episode: 3/100, score: 13, total_reward: 1.65\n",
      "Optimizations triggered:\n",
      "- Increased epsilon_decay to 0.9960\n",
      "- Increased learning_rate to 0.001000\n",
      "episode: 4/100, score: 11, total_reward: 1.95\n",
      "Optimizations triggered:\n",
      "- Increased epsilon_decay to 0.9970\n",
      "episode: 5/100, score: 6, total_reward: -0.40\n",
      "Optimizations triggered:\n",
      "- Increased epsilon_decay to 0.9980\n",
      "episode: 6/100, score: 26, total_reward: 0.55\n",
      "Optimizations triggered:\n",
      "- Increased epsilon_decay to 0.9990\n",
      "episode: 7/100, score: 6, total_reward: 1.40\n",
      "episode: 8/100, score: 3, total_reward: 0.05\n",
      "episode: 9/100, score: 9, total_reward: 0.95\n",
      "episode: 10/100, score: 10, total_reward: -0.35\n",
      "episode: 11/100, score: 6, total_reward: 1.25\n",
      "episode: 12/100, score: 2, total_reward: -0.30\n",
      "episode: 13/100, score: 6, total_reward: 1.25\n",
      "episode: 14/100, score: 2, total_reward: 0.70\n",
      "episode: 15/100, score: 16, total_reward: -0.35\n",
      "episode: 16/100, score: 0, total_reward: 0.35\n",
      "episode: 17/100, score: 1, total_reward: 0.20\n",
      "episode: 18/100, score: 11, total_reward: 0.00\n",
      "episode: 19/100, score: 6, total_reward: 1.10\n",
      "episode: 20/100, score: 0, total_reward: -0.15\n",
      "episode: 21/100, score: 24, total_reward: 1.80\n",
      "episode: 22/100, score: 14, total_reward: 2.15\n",
      "episode: 23/100, score: 4, total_reward: 0.25\n",
      "episode: 24/100, score: 0, total_reward: 0.00\n",
      "episode: 25/100, score: 10, total_reward: 1.45\n",
      "episode: 26/100, score: 7, total_reward: -0.45\n",
      "episode: 27/100, score: 1, total_reward: 0.35\n",
      "episode: 28/100, score: 12, total_reward: 0.95\n",
      "episode: 29/100, score: 6, total_reward: 0.90\n",
      "episode: 30/100, score: 2, total_reward: 1.05\n",
      "episode: 31/100, score: 1, total_reward: 0.20\n",
      "episode: 32/100, score: 2, total_reward: 0.55\n",
      "episode: 33/100, score: 17, total_reward: 1.50\n",
      "episode: 34/100, score: 4, total_reward: 0.40\n",
      "episode: 35/100, score: 19, total_reward: 2.40\n",
      "episode: 36/100, score: 0, total_reward: -0.15\n",
      "episode: 37/100, score: 5, total_reward: 0.60\n",
      "episode: 38/100, score: 31, total_reward: 4.60\n",
      "Optimizations triggered:\n",
      "- Reduced epsilon_decay to 0.9970\n",
      "- Reduced learning_rate to 0.000900\n",
      "episode: 39/100, score: 0, total_reward: 0.00\n",
      "Optimizations triggered:\n",
      "- Reduced epsilon_decay to 0.9950\n",
      "- Reduced learning_rate to 0.000800\n",
      "episode: 40/100, score: 0, total_reward: -0.15\n",
      "Optimizations triggered:\n",
      "- Reduced epsilon_decay to 0.9930\n",
      "- Reduced learning_rate to 0.000700\n",
      "episode: 41/100, score: 2, total_reward: -0.30\n",
      "Optimizations triggered:\n",
      "- Reduced epsilon_decay to 0.9910\n",
      "- Reduced learning_rate to 0.000600\n",
      "episode: 42/100, score: 1, total_reward: 0.35\n",
      "Optimizations triggered:\n",
      "- Increased epsilon_decay to 0.9920\n",
      "- Increased learning_rate to 0.000650\n",
      "episode: 43/100, score: 2, total_reward: 0.70\n",
      "Optimizations triggered:\n",
      "- Increased epsilon_decay to 0.9930\n",
      "- Increased learning_rate to 0.000700\n",
      "episode: 44/100, score: 4, total_reward: 0.25\n",
      "Optimizations triggered:\n",
      "- Increased epsilon_decay to 0.9940\n",
      "- Increased learning_rate to 0.000750\n",
      "episode: 45/100, score: 0, total_reward: 0.00\n",
      "Optimizations triggered:\n",
      "- Increased epsilon_decay to 0.9950\n",
      "- Increased learning_rate to 0.000800\n",
      "episode: 46/100, score: 59, total_reward: 7.05\n",
      "Optimizations triggered:\n",
      "- Reduced epsilon_decay to 0.9930\n",
      "- Reduced learning_rate to 0.000700\n",
      "episode: 47/100, score: 0, total_reward: -0.15\n",
      "Optimizations triggered:\n",
      "- Reduced epsilon_decay to 0.9910\n",
      "- Reduced learning_rate to 0.000600\n",
      "episode: 48/100, score: 19, total_reward: 0.65\n",
      "Optimizations triggered:\n",
      "- Increased epsilon_decay to 0.9920\n",
      "- Increased learning_rate to 0.000650\n",
      "episode: 49/100, score: 0, total_reward: 0.00\n",
      "Optimizations triggered:\n",
      "- Increased epsilon_decay to 0.9930\n",
      "- Increased learning_rate to 0.000700\n",
      "episode: 50/100, score: 6, total_reward: 0.75\n",
      "Optimizations triggered:\n",
      "- Increased epsilon_decay to 0.9940\n",
      "- Increased learning_rate to 0.000750\n",
      "episode: 51/100, score: 1, total_reward: 0.00\n",
      "Optimizations triggered:\n",
      "- Reduced epsilon_decay to 0.9920\n",
      "- Reduced learning_rate to 0.000650\n",
      "episode: 52/100, score: 2, total_reward: 0.70\n",
      "Optimizations triggered:\n",
      "- Reduced epsilon_decay to 0.9900\n",
      "- Reduced learning_rate to 0.000550\n",
      "episode: 53/100, score: 0, total_reward: -0.15\n",
      "Optimizations triggered:\n",
      "- Increased epsilon_decay to 0.9910\n",
      "- Increased learning_rate to 0.000600\n",
      "episode: 54/100, score: 1, total_reward: -0.15\n",
      "Optimizations triggered:\n",
      "- Increased epsilon_decay to 0.9920\n",
      "- Increased learning_rate to 0.000650\n",
      "episode: 55/100, score: 32, total_reward: 4.25\n",
      "Optimizations triggered:\n",
      "- Reduced epsilon_decay to 0.9900\n",
      "- Reduced learning_rate to 0.000550\n",
      "episode: 56/100, score: 0, total_reward: 0.35\n",
      "Optimizations triggered:\n",
      "- Increased epsilon_decay to 0.9910\n",
      "- Increased learning_rate to 0.000600\n",
      "episode: 57/100, score: 28, total_reward: 2.30\n",
      "Optimizations triggered:\n",
      "- Increased epsilon_decay to 0.9920\n",
      "- Increased learning_rate to 0.000650\n",
      "episode: 58/100, score: 41, total_reward: 2.35\n",
      "Optimizations triggered:\n",
      "- Reduced epsilon_decay to 0.9900\n",
      "- Reduced learning_rate to 0.000550\n",
      "episode: 59/100, score: 8, total_reward: 0.30\n",
      "Optimizations triggered:\n",
      "- Reduced learning_rate to 0.000450\n",
      "episode: 60/100, score: 0, total_reward: 0.00\n",
      "episode: 61/100, score: 34, total_reward: 2.90\n",
      "episode: 62/100, score: 5, total_reward: 0.75\n",
      "episode: 63/100, score: 7, total_reward: 1.25\n",
      "episode: 64/100, score: 7, total_reward: 0.60\n",
      "episode: 65/100, score: 3, total_reward: 0.20\n",
      "episode: 66/100, score: 1, total_reward: 0.35\n",
      "episode: 67/100, score: 2, total_reward: 1.05\n",
      "episode: 68/100, score: 37, total_reward: 4.80\n",
      "episode: 69/100, score: 7, total_reward: 2.45\n",
      "episode: 70/100, score: 26, total_reward: 2.00\n",
      "episode: 71/100, score: 11, total_reward: 2.50\n",
      "episode: 72/100, score: 12, total_reward: 1.95\n",
      "episode: 73/100, score: 5, total_reward: 0.90\n",
      "episode: 74/100, score: 6, total_reward: 0.75\n",
      "episode: 75/100, score: 13, total_reward: 2.50\n",
      "episode: 76/100, score: 24, total_reward: 1.30\n",
      "episode: 77/100, score: 1, total_reward: -0.15\n",
      "episode: 78/100, score: 8, total_reward: 1.45\n",
      "episode: 79/100, score: 17, total_reward: 2.70\n",
      "episode: 80/100, score: 7, total_reward: 1.10\n",
      "episode: 81/100, score: 1, total_reward: 0.70\n",
      "episode: 82/100, score: 2, total_reward: 0.05\n",
      "episode: 83/100, score: 4, total_reward: -0.10\n",
      "Optimizations triggered:\n",
      "- Increased epsilon_decay to 0.9910\n",
      "- Increased learning_rate to 0.000500\n",
      "episode: 84/100, score: 3, total_reward: 1.05\n",
      "Optimizations triggered:\n",
      "- Increased epsilon_decay to 0.9920\n",
      "- Increased learning_rate to 0.000550\n",
      "episode: 85/100, score: 4, total_reward: 1.05\n",
      "Optimizations triggered:\n",
      "- Increased epsilon_decay to 0.9930\n",
      "- Increased learning_rate to 0.000600\n",
      "episode: 86/100, score: 27, total_reward: 5.35\n",
      "Optimizations triggered:\n",
      "- Reduced epsilon_decay to 0.9910\n",
      "- Reduced learning_rate to 0.000500\n",
      "episode: 87/100, score: 16, total_reward: 2.35\n",
      "Optimizations triggered:\n",
      "- Reduced epsilon_decay to 0.9890\n",
      "episode: 88/100, score: 20, total_reward: 4.60\n",
      "episode: 89/100, score: 0, total_reward: 0.00\n",
      "episode: 90/100, score: 4, total_reward: 0.90\n",
      "episode: 91/100, score: 6, total_reward: 1.60\n",
      "episode: 92/100, score: 21, total_reward: 2.30\n",
      "episode: 93/100, score: 1, total_reward: 0.20\n",
      "episode: 94/100, score: 8, total_reward: 2.10\n",
      "episode: 95/100, score: 8, total_reward: 0.95\n",
      "episode: 96/100, score: 3, total_reward: 0.40\n",
      "episode: 97/100, score: 6, total_reward: 0.90\n",
      "episode: 98/100, score: 7, total_reward: 1.95\n",
      "Optimizations triggered:\n",
      "- Increased epsilon_decay to 0.9900\n",
      "- Increased learning_rate to 0.000550\n",
      "episode: 99/100, score: 12, total_reward: 2.15\n",
      "Optimizations triggered:\n",
      "- Reduced learning_rate to 0.000450\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Define the DQNAgent class\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95  # Discount factor\n",
    "        self.epsilon = 1.0  # Exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "        self.performance_history = []\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = models.Sequential([\n",
    "            layers.Dense(24, input_dim=self.state_size, activation='relu'),\n",
    "            layers.Dense(24, activation='relu'),\n",
    "            layers.Dense(self.action_size, activation='linear')\n",
    "        ])\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate), loss='mse')\n",
    "        return model\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        q_values = self.model.predict(state, verbose=0)\n",
    "        return np.argmax(q_values[0])\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target += self.gamma * np.amax(self.model.predict(next_state, verbose=0)[0])\n",
    "            target_f = self.model.predict(state, verbose=0)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def monitor_and_optimize(self, recent_rewards):\n",
    "        # Monitor performance\n",
    "        avg_reward = np.mean(recent_rewards)\n",
    "        self.performance_history.append(avg_reward)\n",
    "        \n",
    "        # Dynamically adjust parameters based on performance\n",
    "        optimizations = []\n",
    "        if avg_reward > np.mean(self.performance_history[:-1]):\n",
    "            if self.epsilon_decay > 0.990:\n",
    "                self.epsilon_decay -= 0.002  # Gradually reduce decay\n",
    "                optimizations.append(f\"Reduced epsilon_decay to {self.epsilon_decay:.4f}\")\n",
    "            if self.learning_rate > 0.0005:\n",
    "                self.learning_rate -= 0.0001  # Gradually reduce learning rate\n",
    "                optimizations.append(f\"Reduced learning_rate to {self.learning_rate:.6f}\")\n",
    "        else:\n",
    "            if self.epsilon_decay < 0.999:\n",
    "                self.epsilon_decay += 0.001  # Gradually increase decay for stability\n",
    "                optimizations.append(f\"Increased epsilon_decay to {self.epsilon_decay:.4f}\")\n",
    "            if self.learning_rate < 0.001:\n",
    "                self.learning_rate += 0.00005  # Gradually increase learning rate\n",
    "                optimizations.append(f\"Increased learning_rate to {self.learning_rate:.6f}\")\n",
    "        \n",
    "        return optimizations\n",
    "\n",
    "# Define the environment class\n",
    "class CarbonEmissionsEnv:\n",
    "    def __init__(self):\n",
    "        self.state = None\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = np.random.rand(4)  # Simulate environment reset\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Simulate environment step\n",
    "        emission_reduction = action * 0.1  # Example: action scales emission reduction\n",
    "        cost_penalty = (3 - action) * 0.05  # Example: cost penalty for less efficient actions\n",
    "        efficiency_bonus = 0.2 if action == 2 else 0.0  # Reward for optimal actions\n",
    "        reward = emission_reduction - cost_penalty + efficiency_bonus\n",
    "        next_state = np.random.rand(4)\n",
    "        done = np.random.rand() < 0.1  # End episode randomly (10% chance)\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "# Initialize environment and agent\n",
    "state_size = 4\n",
    "action_size = 3\n",
    "env = CarbonEmissionsEnv()\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "# Train the agent\n",
    "episodes = 100\n",
    "batch_size = 32\n",
    "reward_history = deque(maxlen=10)\n",
    "\n",
    "for e in range(episodes):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    total_reward = 0\n",
    "    for time in range(500):\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    reward_history.append(total_reward)\n",
    "    optimizations = agent.monitor_and_optimize(reward_history)\n",
    "    print(f\"episode: {e}/{episodes}, score: {time}, total_reward: {total_reward:.2f}\")\n",
    "    if optimizations:\n",
    "        print(\"Optimizations triggered:\")\n",
    "        for opt in optimizations:\n",
    "            print(f\"- {opt}\")\n",
    "    agent.replay(batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: elasticsearch in ./myenv/lib/python3.10/site-packages (8.17.2)\n",
      "Requirement already satisfied: elastic-transport<9,>=8.15.1 in ./myenv/lib/python3.10/site-packages (from elasticsearch) (8.17.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.2 in ./myenv/lib/python3.10/site-packages (from elastic-transport<9,>=8.15.1->elasticsearch) (2.3.0)\n",
      "Requirement already satisfied: certifi in ./myenv/lib/python3.10/site-packages (from elastic-transport<9,>=8.15.1->elasticsearch) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "! pip install --upgrade elasticsearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8m/8kxc8bdn5dzfsqxv0r5qsqxw0000gp/T/ipykernel_24936/3271290548.py:44: DeprecationWarning: Received 'size' via a specific parameter in the presence of a 'body' parameter, which is deprecated and will be removed in a future version. Instead, use only 'body' or only specific parameters.\n",
      "  response = es_client.search(index=index, body=query, size=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n",
      "Error fetching data: ApiError(406, 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported', 'Content-Type header [application/vnd.elasticsearch+json; compatible-with=8] is not supported')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 83\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError during processing: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 83\u001b[0m     \u001b[43mmain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 58\u001b[0m, in \u001b[0;36mmain_loop\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmain_loop\u001b[39m():\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         sensor_data \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_real_time_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m sensor_data:\n\u001b[1;32m     60\u001b[0m             \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m                 \u001b[38;5;66;03m# Extract features and scale the state\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 44\u001b[0m, in \u001b[0;36mfetch_real_time_data\u001b[0;34m(es_client, index)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m     query \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatch_all\u001b[39m\u001b[38;5;124m\"\u001b[39m: {}}}\n\u001b[0;32m---> 44\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mes_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhits\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhits\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_source\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Desktop/KPMG Projects/carbonfootprint/myenv/lib/python3.10/site-packages/elasticsearch/_sync/client/utils.py:428\u001b[0m, in \u001b[0;36m_rewrite_parameters.<locals>.wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    426\u001b[0m     kwargs[body_name] \u001b[38;5;241m=\u001b[39m body\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m body_fields \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 428\u001b[0m     mixed_body_and_params \u001b[38;5;241m=\u001b[39m \u001b[43m_merge_body_fields_no_duplicates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody_fields\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    431\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m body\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m parameter_aliases \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(body, (\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbytes\u001b[39m)):\n",
      "File \u001b[0;32m~/Desktop/KPMG Projects/carbonfootprint/myenv/lib/python3.10/site-packages/elasticsearch/_sync/client/utils.py:330\u001b[0m, in \u001b[0;36m_merge_body_fields_no_duplicates\u001b[0;34m(body, kwargs, body_fields)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m body:\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived multiple values for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, specify parameters \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musing either body or parameters, not both.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    323\u001b[0m     )\n\u001b[1;32m    325\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m via a specific parameter in the presence of a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m parameter, which is deprecated and will be removed in a future \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    328\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mversion. Instead, use only \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or only specific parameters.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    329\u001b[0m     category\u001b[38;5;241m=\u001b[39m\u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[0;32m--> 330\u001b[0m     stacklevel\u001b[38;5;241m=\u001b[39m\u001b[43mwarn_stacklevel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    331\u001b[0m )\n\u001b[1;32m    332\u001b[0m body[key] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(key)\n\u001b[1;32m    333\u001b[0m mixed_body_and_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/KPMG Projects/carbonfootprint/myenv/lib/python3.10/site-packages/elasticsearch/compat.py:54\u001b[0m, in \u001b[0;36mwarn_stacklevel\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m     module_path \u001b[38;5;241m=\u001b[39m module_path\u001b[38;5;241m.\u001b[39mparent\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Look through frames until we find a file that\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# isn't a part of our module, then return that stacklevel.\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m level, frame \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43minspect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;66;03m# Garbage collecting frames\u001b[39;00m\n\u001b[1;32m     56\u001b[0m     frame_filename \u001b[38;5;241m=\u001b[39m Path(frame\u001b[38;5;241m.\u001b[39mfilename)\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m frame\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_env/lib/python3.10/inspect.py:1673\u001b[0m, in \u001b[0;36mstack\u001b[0;34m(context)\u001b[0m\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstack\u001b[39m(context\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m   1672\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a list of records for the stack above the caller's frame.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1673\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgetouterframes\u001b[49m\u001b[43m(\u001b[49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getframe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_env/lib/python3.10/inspect.py:1650\u001b[0m, in \u001b[0;36mgetouterframes\u001b[0;34m(frame, context)\u001b[0m\n\u001b[1;32m   1648\u001b[0m framelist \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   1649\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m frame:\n\u001b[0;32m-> 1650\u001b[0m     frameinfo \u001b[38;5;241m=\u001b[39m (frame,) \u001b[38;5;241m+\u001b[39m \u001b[43mgetframeinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1651\u001b[0m     framelist\u001b[38;5;241m.\u001b[39mappend(FrameInfo(\u001b[38;5;241m*\u001b[39mframeinfo))\n\u001b[1;32m   1652\u001b[0m     frame \u001b[38;5;241m=\u001b[39m frame\u001b[38;5;241m.\u001b[39mf_back\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_env/lib/python3.10/inspect.py:1624\u001b[0m, in \u001b[0;36mgetframeinfo\u001b[0;34m(frame, context)\u001b[0m\n\u001b[1;32m   1622\u001b[0m start \u001b[38;5;241m=\u001b[39m lineno \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m context\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m   1623\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1624\u001b[0m     lines, lnum \u001b[38;5;241m=\u001b[39m \u001b[43mfindsource\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1625\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m   1626\u001b[0m     lines \u001b[38;5;241m=\u001b[39m index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_env/lib/python3.10/inspect.py:940\u001b[0m, in \u001b[0;36mfindsource\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfindsource\u001b[39m(\u001b[38;5;28mobject\u001b[39m):\n\u001b[1;32m    933\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the entire source file and starting line number for an object.\u001b[39;00m\n\u001b[1;32m    934\u001b[0m \n\u001b[1;32m    935\u001b[0m \u001b[38;5;124;03m    The argument may be a module, class, method, function, traceback, frame,\u001b[39;00m\n\u001b[1;32m    936\u001b[0m \u001b[38;5;124;03m    or code object.  The source code is returned as a list of all the lines\u001b[39;00m\n\u001b[1;32m    937\u001b[0m \u001b[38;5;124;03m    in the file and the line number indexes a line in that list.  An OSError\u001b[39;00m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;124;03m    is raised if the source code cannot be retrieved.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 940\u001b[0m     file \u001b[38;5;241m=\u001b[39m \u001b[43mgetsourcefile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    941\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file:\n\u001b[1;32m    942\u001b[0m         \u001b[38;5;66;03m# Invalidate cache if needed.\u001b[39;00m\n\u001b[1;32m    943\u001b[0m         linecache\u001b[38;5;241m.\u001b[39mcheckcache(file)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_env/lib/python3.10/inspect.py:829\u001b[0m, in \u001b[0;36mgetsourcefile\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m filename\n\u001b[1;32m    828\u001b[0m \u001b[38;5;66;03m# only return a non-existent filename if the module has a PEP 302 loader\u001b[39;00m\n\u001b[0;32m--> 829\u001b[0m module \u001b[38;5;241m=\u001b[39m \u001b[43mgetmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__loader__\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    831\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m filename\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_env/lib/python3.10/inspect.py:869\u001b[0m, in \u001b[0;36mgetmodule\u001b[0;34m(object, _filename)\u001b[0m\n\u001b[1;32m    866\u001b[0m \u001b[38;5;66;03m# Update the filename to module name cache and check yet again\u001b[39;00m\n\u001b[1;32m    867\u001b[0m \u001b[38;5;66;03m# Copy sys.modules in order to cope with changes while iterating\u001b[39;00m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m modname, module \u001b[38;5;129;01min\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mmodules\u001b[38;5;241m.\u001b[39mcopy()\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 869\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ismodule(module) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28;43mhasattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m__file__\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    870\u001b[0m         f \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__file__\u001b[39m\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;241m==\u001b[39m _filesbymodname\u001b[38;5;241m.\u001b[39mget(modname, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    872\u001b[0m             \u001b[38;5;66;03m# Have already mapped this module, so skip it\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from elasticsearch import Elasticsearch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "\n",
    "# Elasticsearch connection\n",
    "es = Elasticsearch(\"http://localhost:9200\")\n",
    "index_name = \"iot_sensor\"\n",
    "\n",
    "# Load the pre-trained model and scaler\n",
    "best_model = joblib.load(\"/Users/kdn_aikothalavanya/Desktop/KPMG Projects/carbonfootprint/aqi_predictor_tuned.pkl\")  # Replace with the path to your saved XGBoost model\n",
    "scaler = joblib.load(\"/Users/kdn_aikothalavanya/Desktop/KPMG Projects/carbonfootprint/scaler.pkl\")  # Replace with the path to your saved scaler\n",
    "\n",
    "# Deep RL Agent class\n",
    "class RLAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(64, activation='relu', input_shape=(self.state_size,)),\n",
    "            tf.keras.layers.Dense(64, activation='relu'),\n",
    "            tf.keras.layers.Dense(self.action_size, activation='softmax')  # Action probabilities\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "        return model\n",
    "\n",
    "    def predict_action(self, state):\n",
    "        action_probs = self.model.predict(state, verbose=0)\n",
    "        return np.argmax(action_probs)\n",
    "\n",
    "    def train(self, state, action, reward):\n",
    "        target = np.zeros((1, self.action_size))\n",
    "        target[0, action] = reward\n",
    "        self.model.fit(state, target, verbose=0)\n",
    "\n",
    "# Fetch real-time data from Elasticsearch\n",
    "def fetch_real_time_data(es_client, index):\n",
    "    try:\n",
    "        query = {\"query\": {\"match_all\": {}}}\n",
    "        response = es_client.search(index=index, body=query, size=1)\n",
    "        return response['hits']['hits'][0]['_source']\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data: {e}\")\n",
    "        return None\n",
    "\n",
    "# Define feature columns used in the model\n",
    "features = [\"feature1\", \"feature2\", \"feature3\", \"feature4\", \"feature5\", \"feature6\", \"feature7\", \"feature8\"]\n",
    "\n",
    "# Main loop for RL agent interaction\n",
    "agent = RLAgent(state_size=8, action_size=3)  # Adjust sizes based on features and actions\n",
    "\n",
    "def main_loop():\n",
    "    while True:\n",
    "        sensor_data = fetch_real_time_data(es, index_name)\n",
    "        if sensor_data:\n",
    "            try:\n",
    "                # Extract features and scale the state\n",
    "                state = np.array([[sensor_data[col] for col in features]])\n",
    "                scaled_state = scaler.transform(state)\n",
    "\n",
    "                # Predict AQI using the pre-trained model\n",
    "                predicted_aqi = best_model.predict(scaled_state)[0]\n",
    "\n",
    "                # Determine action using the RL agent\n",
    "                action = agent.predict_action(scaled_state)\n",
    "\n",
    "                # Define reward based on AQI thresholds\n",
    "                reward = 1 if predicted_aqi < 50 else -1\n",
    "\n",
    "                # Train the RL agent\n",
    "                agent.train(scaled_state, action, reward)\n",
    "\n",
    "                print(f\"State: {state}, Predicted AQI: {predicted_aqi}, Action: {action}, Reward: {reward}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error during processing: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_loop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting pytorch\n",
      "  Using cached pytorch-1.0.2.tar.gz (689 bytes)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pandas\n",
      "  Using cached pandas-2.2.3-cp39-cp39-macosx_10_9_x86_64.whl.metadata (89 kB)\n",
      "Collecting filelock (from transformers)\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.26.0 (from transformers)\n",
      "  Using cached huggingface_hub-0.29.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/envs/torch_env/lib/python3.9/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/torch_env/lib/python3.9/site-packages (from transformers) (24.2)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Using cached PyYAML-6.0.2-cp39-cp39-macosx_10_9_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2024.11.6-cp39-cp39-macosx_10_9_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/torch_env/lib/python3.9/site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Using cached tokenizers-0.21.1-cp39-abi3-macosx_10_12_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-macosx_10_12_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/torch_env/lib/python3.9/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.26.0->transformers)\n",
      "  Using cached fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/envs/torch_env/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/torch_env/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/torch_env/lib/python3.9/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/torch_env/lib/python3.9/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/torch_env/lib/python3.9/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/torch_env/lib/python3.9/site-packages (from requests->transformers) (2025.1.31)\n",
      "Using cached transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
      "Using cached pandas-2.2.3-cp39-cp39-macosx_10_9_x86_64.whl (12.6 MB)\n",
      "Using cached huggingface_hub-0.29.3-py3-none-any.whl (468 kB)\n",
      "Using cached pytz-2025.1-py2.py3-none-any.whl (507 kB)\n",
      "Using cached PyYAML-6.0.2-cp39-cp39-macosx_10_9_x86_64.whl (184 kB)\n",
      "Using cached regex-2024.11.6-cp39-cp39-macosx_10_9_x86_64.whl (287 kB)\n",
      "Using cached safetensors-0.5.3-cp38-abi3-macosx_10_12_x86_64.whl (436 kB)\n",
      "Using cached tokenizers-0.21.1-cp39-abi3-macosx_10_12_x86_64.whl (2.8 MB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached tzdata-2025.1-py2.py3-none-any.whl (346 kB)\n",
      "Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Using cached fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "Building wheels for collected packages: pytorch\n",
      "  Building wheel for pytorch (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[6 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 2, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/8m/8kxc8bdn5dzfsqxv0r5qsqxw0000gp/T/pip-install-hf8c5hdt/pytorch_6824c15f423c4be99cb38964eef384de/setup.py\", line 15, in <module>\n",
      "  \u001b[31m   \u001b[0m     raise Exception(message)\n",
      "  \u001b[31m   \u001b[0m Exception: You tried to install \"pytorch\". The package named for PyTorch is \"torch\"\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for pytorch\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25h  Running setup.py clean for pytorch\n",
      "Failed to build pytorch\n",
      "\u001b[31mERROR: Failed to build installable wheels for some pyproject.toml based projects (pytorch)\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install transformers pytorch pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-2.2.3-cp39-cp39-macosx_10_9_x86_64.whl.metadata (89 kB)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /opt/anaconda3/envs/torch_env/lib/python3.9/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/torch_env/lib/python3.9/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/torch_env/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Using cached pandas-2.2.3-cp39-cp39-macosx_10_9_x86_64.whl (12.6 MB)\n",
      "Using cached pytz-2025.1-py2.py3-none-any.whl (507 kB)\n",
      "Using cached tzdata-2025.1-py2.py3-none-any.whl (346 kB)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "Successfully installed pandas-2.2.3 pytz-2025.1 tzdata-2025.1\n"
     ]
    }
   ],
   "source": [
    "! pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.1-cp39-cp39-macosx_10_9_x86_64.whl.metadata (31 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /opt/anaconda3/envs/torch_env/lib/python3.9/site-packages (from scikit-learn) (1.26.4)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.13.1-cp39-cp39-macosx_10_9_x86_64.whl.metadata (60 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.6.1-cp39-cp39-macosx_10_9_x86_64.whl (12.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading scipy-1.13.1-cp39-cp39-macosx_10_9_x86_64.whl (39.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.4/39.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.6.1 scipy-1.13.1 threadpoolctl-3.6.0\n"
     ]
    }
   ],
   "source": [
    "! pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting filelock (from transformers)\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.26.0 (from transformers)\n",
      "  Using cached huggingface_hub-0.29.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/envs/torch_env/lib/python3.9/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/torch_env/lib/python3.9/site-packages (from transformers) (24.2)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Using cached PyYAML-6.0.2-cp39-cp39-macosx_10_9_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2024.11.6-cp39-cp39-macosx_10_9_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/torch_env/lib/python3.9/site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Using cached tokenizers-0.21.1-cp39-abi3-macosx_10_12_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-macosx_10_12_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.26.0->transformers)\n",
      "  Using cached fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/envs/torch_env/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/torch_env/lib/python3.9/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/torch_env/lib/python3.9/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/torch_env/lib/python3.9/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/torch_env/lib/python3.9/site-packages (from requests->transformers) (2025.1.31)\n",
      "Using cached transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
      "Using cached huggingface_hub-0.29.3-py3-none-any.whl (468 kB)\n",
      "Using cached PyYAML-6.0.2-cp39-cp39-macosx_10_9_x86_64.whl (184 kB)\n",
      "Using cached regex-2024.11.6-cp39-cp39-macosx_10_9_x86_64.whl (287 kB)\n",
      "Using cached safetensors-0.5.3-cp38-abi3-macosx_10_12_x86_64.whl (436 kB)\n",
      "Using cached tokenizers-0.21.1-cp39-abi3-macosx_10_12_x86_64.whl (2.8 MB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Using cached fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "Installing collected packages: tqdm, safetensors, regex, pyyaml, fsspec, filelock, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed filelock-3.18.0 fsspec-2025.3.0 huggingface-hub-0.29.3 pyyaml-6.0.2 regex-2024.11.6 safetensors-0.5.3 tokenizers-0.21.1 tqdm-4.67.1 transformers-4.49.0\n"
     ]
    }
   ],
   "source": [
    "! pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
      "Requirement already satisfied: torch in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: torchvision in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (0.17.2)\n",
      "Requirement already satisfied: torchaudio in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (4.49.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from transformers) (0.29.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from requests->transformers) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "! pip install -U transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PolynomialFeatures transformer has been saved as 'poly_transform.pkl'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import joblib\n",
    "\n",
    "# Simulate a dataset resembling your real-time monitoring data\n",
    "# Example fields: ['year', 'carbon_emission', 'activity']\n",
    "data = {\n",
    "    'year': np.random.randint(2000, 2025, size=100),  # Random years between 2000 and 2025\n",
    "    'carbon_emission': np.random.uniform(0.1, 500.0, size=100),  # Random emissions between 0.1 and 500.0\n",
    "    'activity': np.random.uniform(0.1, 1000.0, size=100),  # Random activity levels\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Define features to use for polynomial transformation\n",
    "features = ['year', 'carbon_emission', 'activity']\n",
    "\n",
    "# Prepare the PolynomialFeatures transformer\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "\n",
    "# Fit and transform the features\n",
    "poly.fit(df[features])\n",
    "\n",
    "# Save the transformer\n",
    "joblib.dump(poly, \"poly_transform.pkl\")\n",
    "\n",
    "print(\"PolynomialFeatures transformer has been saved as 'poly_transform.pkl'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (4.49.0)\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.4.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: torch in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from transformers) (0.29.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-19.0.1-cp310-cp310-macosx_12_0_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from datasets) (2.2.3)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-macosx_10_9_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.11.14-cp310-cp310-macosx_10_9_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from torch) (3.1.6)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp->datasets)\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->datasets)\n",
      "  Using cached attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.5.0-cp310-cp310-macosx_10_9_x86_64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.2.0-cp310-cp310-macosx_10_9_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets)\n",
      "  Downloading propcache-0.3.0-cp310-cp310-macosx_10_9_x86_64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.18.3-cp310-cp310-macosx_10_9_x86_64.whl.metadata (69 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading datasets-3.4.1-py3-none-any.whl (487 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "Downloading aiohttp-3.11.14-cp310-cp310-macosx_10_9_x86_64.whl (469 kB)\n",
      "Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "Downloading pyarrow-19.0.1-cp310-cp310-macosx_12_0_x86_64.whl (32.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.1/32.1 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-macosx_10_9_x86_64.whl (31 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Using cached attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Downloading frozenlist-1.5.0-cp310-cp310-macosx_10_9_x86_64.whl (54 kB)\n",
      "Downloading multidict-6.2.0-cp310-cp310-macosx_10_9_x86_64.whl (29 kB)\n",
      "Downloading propcache-0.3.0-cp310-cp310-macosx_10_9_x86_64.whl (45 kB)\n",
      "Downloading yarl-1.18.3-cp310-cp310-macosx_10_9_x86_64.whl (94 kB)\n",
      "Installing collected packages: xxhash, pyarrow, propcache, multidict, fsspec, frozenlist, dill, attrs, async-timeout, aiohappyeyeballs, yarl, multiprocess, aiosignal, aiohttp, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.0\n",
      "    Uninstalling fsspec-2025.3.0:\n",
      "      Successfully uninstalled fsspec-2025.3.0\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.11.14 aiosignal-1.3.2 async-timeout-5.0.1 attrs-25.3.0 datasets-3.4.1 dill-0.3.8 frozenlist-1.5.0 fsspec-2024.12.0 multidict-6.2.0 multiprocess-0.70.16 propcache-0.3.0 pyarrow-19.0.1 xxhash-3.5.0 yarl-1.18.3\n"
     ]
    }
   ],
   "source": [
    "! pip install transformers datasets torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tf_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cpu.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "\n",
    "# Load tokenizer and model\n",
    "model_name = \"gpt2\"  # You can replace this with a larger model if needed (like GPT-Neo)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Enable CUDA if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Model loaded on {device}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent: {'country': 'Afghanistan', 'iso_alpha3': 'AFG', 'year': 1949, 'carbon_emission': 0.014656, 'activity': 0.014656}\n",
      "Sent: {'country': 'Afghanistan', 'iso_alpha3': 'AFG', 'year': 1950, 'carbon_emission': 0.021068, 'activity': 0.084272}\n",
      "Sent: {'country': 'Afghanistan', 'iso_alpha3': 'AFG', 'year': 1951, 'carbon_emission': 0.025648, 'activity': 0.0916}\n",
      "Sent: {'country': 'Afghanistan', 'iso_alpha3': 'AFG', 'year': 1952, 'carbon_emission': 0.031708, 'activity': 0.0916}\n",
      "Sent: {'country': 'Afghanistan', 'iso_alpha3': 'AFG', 'year': 1953, 'carbon_emission': 0.037949, 'activity': 0.106256}\n",
      "Sent: {'country': 'Albania', 'iso_alpha3': 'ALB', 'year': 1946, 'carbon_emission': 0.021984, 'activity': 0.483648}\n",
      "Sent: {'country': 'Albania', 'iso_alpha3': 'ALB', 'year': 1948, 'carbon_emission': 0.007328, 'activity': 0.703488}\n",
      "Sent: {'country': 'Albania', 'iso_alpha3': 'ALB', 'year': 1950, 'carbon_emission': 0.043432, 'activity': 0.296725}\n",
      "Sent: {'country': 'Albania', 'iso_alpha3': 'ALB', 'year': 1951, 'carbon_emission': 0.07328, 'activity': 0.402981}\n",
      "Sent: {'country': 'Albania', 'iso_alpha3': 'ALB', 'year': 1952, 'carbon_emission': 0.076197, 'activity': 0.373669}\n",
      "Sent: {'country': 'India', 'iso_alpha3': 'IND', 'year': 1858, 'carbon_emission': 0.394488, 'activity': 0.394488}\n",
      "Sent: {'country': 'India', 'iso_alpha3': 'IND', 'year': 1859, 'carbon_emission': 0.636486, 'activity': 0.636486}\n",
      "Sent: {'country': 'India', 'iso_alpha3': 'IND', 'year': 1860, 'carbon_emission': 0.643116, 'activity': 0.643116}\n",
      "Sent: {'country': 'India', 'iso_alpha3': 'IND', 'year': 1861, 'carbon_emission': 0.497254, 'activity': 0.497254}\n",
      "Sent: {'country': 'India', 'iso_alpha3': 'IND', 'year': 1862, 'carbon_emission': 0.550295, 'activity': 0.550295}\n",
      "Sent: {'country': 'Germany', 'iso_alpha3': 'DEU', 'year': 1792, 'carbon_emission': 0.468992, 'activity': 0.468992}\n",
      "Sent: {'country': 'Germany', 'iso_alpha3': 'DEU', 'year': 1793, 'carbon_emission': 0.479984, 'activity': 0.479984}\n",
      "Sent: {'country': 'Germany', 'iso_alpha3': 'DEU', 'year': 1794, 'carbon_emission': 0.443344, 'activity': 0.443344}\n",
      "Sent: {'country': 'Germany', 'iso_alpha3': 'DEU', 'year': 1795, 'carbon_emission': 0.447008, 'activity': 0.447008}\n",
      "Sent: {'country': 'Germany', 'iso_alpha3': 'DEU', 'year': 1796, 'carbon_emission': 0.534944, 'activity': 0.534944}\n"
     ]
    }
   ],
   "source": [
    "from kafka import KafkaProducer\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import pycountry\n",
    "\n",
    "# Initialize Kafka Producer\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=['localhost:9092'],\n",
    "    value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    ")\n",
    "\n",
    "# Function to map country names to ISO 3166-1 alpha-3 codes\n",
    "def get_iso_alpha3(country_name):\n",
    "    try:\n",
    "        return pycountry.countries.lookup(country_name).alpha_3\n",
    "    except LookupError:\n",
    "        return None\n",
    "\n",
    "# Function to process CSV and prepare data\n",
    "def process_csv(file_path, countries_with_limit):\n",
    "    try:\n",
    "        # Read CSV\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Ensure required columns are present\n",
    "        required_columns = ['Country', 'Year', 'Coal', 'Total']\n",
    "        if not all(col in df.columns for col in required_columns):\n",
    "            raise ValueError(f\"CSV file must contain the following columns: {required_columns}\")\n",
    "\n",
    "        # Filter rows where 'Coal' > 0\n",
    "        df = df[df['Coal'] > 0]\n",
    "\n",
    "        # Add ISO 3166-1 alpha-3 codes\n",
    "        df['ISO_alpha3'] = df['Country'].apply(get_iso_alpha3)\n",
    "\n",
    "        # Drop rows with missing ISO alpha-3 codes\n",
    "        df = df.dropna(subset=['ISO_alpha3'])\n",
    "\n",
    "        # Filter data for specific countries and limits\n",
    "        filtered_data = []\n",
    "        for country, limit in countries_with_limit.items():\n",
    "            country_data = df[df['Country'] == country].head(limit)\n",
    "            filtered_data.append(country_data)\n",
    "\n",
    "        # Concatenate filtered data\n",
    "        result_df = pd.concat(filtered_data)\n",
    "\n",
    "        # Yield each row as a dictionary\n",
    "        for _, row in result_df.iterrows():\n",
    "            yield {\n",
    "                \"country\": row['Country'],\n",
    "                \"iso_alpha3\": row['ISO_alpha3'],\n",
    "                \"year\": int(row['Year']),\n",
    "                \"carbon_emission\": float(row['Coal']),\n",
    "                \"activity\": row['Total']\n",
    "            }\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing CSV: {e}\")\n",
    "        return []\n",
    "\n",
    "# Main block to read CSV and send data to Kafka\n",
    "try:\n",
    "    file_path = \"/Users/kdn_aikothalavanya/Downloads/GCB2022v27_MtCO2_flat.csv\"\n",
    "\n",
    "    # Specify countries and limits\n",
    "    countries_with_limit = {\n",
    "        \"Afghanistan\": 5,\n",
    "        \"Albania\": 5,\n",
    "        \"India\": 5,\n",
    "        \"Germany\": 5\n",
    "    }\n",
    "\n",
    "    for data in process_csv(file_path, countries_with_limit):\n",
    "        future = producer.send('carbonfootprint', data)\n",
    "        print(f\"Sent: {data}\")\n",
    "        time.sleep(0.5)  # Throttle to avoid overwhelming Kafka\n",
    "except Exception as e:\n",
    "    print(f\"Error sending data: {e}\")\n",
    "finally:\n",
    "    producer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting elasticsearch\n",
      "  Downloading elasticsearch-8.17.2-py3-none-any.whl.metadata (8.8 kB)\n",
      "Collecting elastic-transport<9,>=8.15.1 (from elasticsearch)\n",
      "  Downloading elastic_transport-8.17.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.2 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from elastic-transport<9,>=8.15.1->elasticsearch) (2.3.0)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from elastic-transport<9,>=8.15.1->elasticsearch) (2025.1.31)\n",
      "Downloading elasticsearch-8.17.2-py3-none-any.whl (717 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m718.0/718.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading elastic_transport-8.17.1-py3-none-any.whl (64 kB)\n",
      "Installing collected packages: elastic-transport, elasticsearch\n",
      "Successfully installed elastic-transport-8.17.1 elasticsearch-8.17.2\n"
     ]
    }
   ],
   "source": [
    "! pip install elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error sending data: Connection error caused by: ConnectionError(Connection error caused by: NewConnectionError(<urllib3.connection.HTTPConnection object at 0x151c82080>: Failed to establish a new connection: [Errno 61] Connection refused))\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import pycountry\n",
    "\n",
    "# Initialize Elasticsearch Client\n",
    "es = Elasticsearch(\"http://localhost:9200\")  # Replace with your Elasticsearch URL if hosted elsewhere\n",
    "\n",
    "# Function to map country names to ISO 3166-1 alpha-3 codes\n",
    "def get_iso_alpha3(country_name):\n",
    "    try:\n",
    "        return pycountry.countries.lookup(country_name).alpha_3\n",
    "    except LookupError:\n",
    "        return None\n",
    "\n",
    "# Function to process CSV and prepare data\n",
    "def process_csv(file_path, countries_with_limit):\n",
    "    try:\n",
    "        # Read CSV\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Ensure required columns are present\n",
    "        required_columns = ['Country', 'Year', 'Coal', 'Total']\n",
    "        if not all(col in df.columns for col in required_columns):\n",
    "            raise ValueError(f\"CSV file must contain the following columns: {required_columns}\")\n",
    "\n",
    "        # Filter rows where 'Coal' > 0\n",
    "        df = df[df['Coal'] > 0]\n",
    "\n",
    "        # Add ISO 3166-1 alpha-3 codes\n",
    "        df['ISO_alpha3'] = df['Country'].apply(get_iso_alpha3)\n",
    "\n",
    "        # Drop rows with missing ISO alpha-3 codes\n",
    "        df = df.dropna(subset=['ISO_alpha3'])\n",
    "\n",
    "        # Filter data for specific countries and limits\n",
    "        filtered_data = []\n",
    "        for country, limit in countries_with_limit.items():\n",
    "            country_data = df[df['Country'] == country].head(limit)\n",
    "            filtered_data.append(country_data)\n",
    "\n",
    "        # Concatenate filtered data\n",
    "        result_df = pd.concat(filtered_data)\n",
    "\n",
    "        # Yield each row as a dictionary\n",
    "        for _, row in result_df.iterrows():\n",
    "            yield {\n",
    "                \"country\": row['Country'],\n",
    "                \"iso_alpha3\": row['ISO_alpha3'],\n",
    "                \"year\": int(row['Year']),\n",
    "                \"carbon_emission\": float(row['Coal']),\n",
    "                \"activity\": row['Total']\n",
    "            }\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing CSV: {e}\")\n",
    "        return []\n",
    "\n",
    "# Function to index data in Elasticsearch\n",
    "def index_to_elasticsearch(index_name, data):\n",
    "    try:\n",
    "        # Index the data into Elasticsearch\n",
    "        response = es.index(index=index_name, body=data)\n",
    "        print(f\"Document indexed. ID: {response['_id']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error indexing document: {e}\")\n",
    "\n",
    "# Main block to read CSV and send data to Elasticsearch\n",
    "try:\n",
    "    file_path = \"/Users/kdn_aikothalavanya/Downloads/GCB2022v27_MtCO2_flat.csv\"\n",
    "\n",
    "    # Specify countries and limits\n",
    "    countries_with_limit = {\n",
    "        \"Afghanistan\": 5,\n",
    "        \"Albania\": 5,\n",
    "        \"India\": 5,\n",
    "        \"Germany\": 5\n",
    "    }\n",
    "\n",
    "    index_name = \"carbonfootprint\"  # Elasticsearch index name\n",
    "\n",
    "    # Create index if it doesn't exist\n",
    "    if not es.indices.exists(index=index_name):\n",
    "        es.indices.create(index=index_name)\n",
    "        print(f\"Created Elasticsearch index: {index_name}\")\n",
    "\n",
    "    # Process CSV and send data to Elasticsearch\n",
    "    for data in process_csv(file_path, countries_with_limit):\n",
    "        index_to_elasticsearch(index_name, data)\n",
    "        time.sleep(0.5)  # Throttle to avoid overwhelming Elasticsearch\n",
    "except Exception as e:\n",
    "    print(f\"Error sending data: {e}\")\n",
    "finally:\n",
    "    es.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent: {'timestamp': '2025-03-22 22:18:49', 'co2_ppm': 548.606014687361, 'no2_ppm': 9.916281225491558, 'so2_ppm': 1.0403628061379906, 'temperature_c': 11.633600152610738, 'humidity_percent': 25.74550592759281, 'aqi': 5, 'environmental_score': 72.6752611103381, 'social_score': 68.55838175232356, 'governance_score': 73.97769473652139}\n",
      "Sent: {'timestamp': '2025-03-22 22:22:25', 'co2_ppm': 4042.507826442367, 'no2_ppm': 3.8811434858951226, 'so2_ppm': 2.17174804405351, 'temperature_c': 10.892209271710792, 'humidity_percent': 21.947806541725253, 'aqi': 3, 'environmental_score': 56.73257793373792, 'social_score': 85.43385040728906, 'governance_score': 83.41359404561356}\n",
      "Sent: {'timestamp': '2025-03-22 22:22:28', 'co2_ppm': 2113.433202346877, 'no2_ppm': 9.487869562551154, 'so2_ppm': 13.260369291039805, 'temperature_c': 14.98161692420653, 'humidity_percent': 70.22854343990032, 'aqi': 11, 'environmental_score': 86.87958824345276, 'social_score': 93.98969293653772, 'governance_score': 94.10240900574682}\n",
      "Sent: {'timestamp': '2025-03-22 22:21:08', 'co2_ppm': 2649.863336489712, 'no2_ppm': 4.616146593645984, 'so2_ppm': 2.050842029037262, 'temperature_c': 18.138708917022782, 'humidity_percent': 45.753685494787945, 'aqi': 3, 'environmental_score': 72.39262282389235, 'social_score': 78.44983268985763, 'governance_score': 74.06574263137972}\n",
      "Sent: {'timestamp': '2025-03-22 22:16:59', 'co2_ppm': 2559.726884328139, 'no2_ppm': 5.867326496727233, 'so2_ppm': 3.410072842590965, 'temperature_c': 32.62479967398987, 'humidity_percent': 56.04941684861812, 'aqi': 4, 'environmental_score': 62.40374419068303, 'social_score': 84.02592185028125, 'governance_score': 91.8539416551973}\n",
      "Sent: {'timestamp': '2025-03-22 22:21:26', 'co2_ppm': 2027.5330877674053, 'no2_ppm': 13.5044524223126, 'so2_ppm': 6.721068359769908, 'temperature_c': 38.80074902707496, 'humidity_percent': 77.96417293344713, 'aqi': 10, 'environmental_score': 87.16541749805435, 'social_score': 68.01503002804736, 'governance_score': 50.04443381291835}\n",
      "Sent: {'timestamp': '2025-03-22 22:20:34', 'co2_ppm': 1336.441205875631, 'no2_ppm': 8.464969799368124, 'so2_ppm': 5.035778658749465, 'temperature_c': 30.63781040003608, 'humidity_percent': 20.818329119670032, 'aqi': 6, 'environmental_score': 69.04404292215901, 'social_score': 51.84270332744079, 'governance_score': 81.34986794644207}\n",
      "Sent: {'timestamp': '2025-03-22 22:18:41', 'co2_ppm': 4735.861831418409, 'no2_ppm': 9.003863322561438, 'so2_ppm': 4.036257382132527, 'temperature_c': 14.863558141416858, 'humidity_percent': 34.70056051297634, 'aqi': 6, 'environmental_score': 84.47091454180071, 'social_score': 91.96892376552007, 'governance_score': 67.96189732813859}\n",
      "Sent: {'timestamp': '2025-03-22 22:24:27', 'co2_ppm': 1061.0548352341943, 'no2_ppm': 19.09297677464767, 'so2_ppm': 10.73073030220898, 'temperature_c': 32.23754840754769, 'humidity_percent': 31.650533186795048, 'aqi': 14, 'environmental_score': 74.91987781077401, 'social_score': 53.83690000249132, 'governance_score': 69.13119199420395}\n",
      "Sent: {'timestamp': '2025-03-22 22:12:24', 'co2_ppm': 3515.755886876567, 'no2_ppm': 17.78790227661813, 'so2_ppm': 9.854142754383536, 'temperature_c': 36.33346656544849, 'humidity_percent': 28.437545260793232, 'aqi': 13, 'environmental_score': 51.9323258973441, 'social_score': 70.77189454065883, 'governance_score': 50.30702143048877}\n",
      "Sent: {'timestamp': '2025-03-22 22:23:39', 'co2_ppm': 3168.729086430389, 'no2_ppm': 6.6338451419460345, 'so2_ppm': 1.9552050572124569, 'temperature_c': 23.42308368570681, 'humidity_percent': 81.4515276192987, 'aqi': 4, 'environmental_score': 76.26375786138703, 'social_score': 88.54417251885371, 'governance_score': 98.01603423142744}\n",
      "Sent: {'timestamp': '2025-03-22 22:11:24', 'co2_ppm': 3275.6337673850408, 'no2_ppm': 4.734779217557265, 'so2_ppm': 4.140975424054764, 'temperature_c': 20.875499722883816, 'humidity_percent': 46.0274465380766, 'aqi': 4, 'environmental_score': 95.68744513908234, 'social_score': 87.97949949642594, 'governance_score': 77.34436559836647}\n",
      "Sent: {'timestamp': '2025-03-22 22:20:44', 'co2_ppm': 2589.5836487497504, 'no2_ppm': 14.926000825774343, 'so2_ppm': 9.365224630584557, 'temperature_c': 38.0129191813909, 'humidity_percent': 24.20879435068499, 'aqi': 12, 'environmental_score': 90.43700211649586, 'social_score': 87.25032013006428, 'governance_score': 59.37398274001}\n",
      "Sent: {'timestamp': '2025-03-22 22:26:21', 'co2_ppm': 745.5236311250394, 'no2_ppm': 5.145655989540688, 'so2_ppm': 3.2729154497152524, 'temperature_c': 10.130670807547602, 'humidity_percent': 79.46559428192379, 'aqi': 4, 'environmental_score': 73.56510972425059, 'social_score': 71.5326205273287, 'governance_score': 91.08162286618048}\n",
      "Sent: {'timestamp': '2025-03-22 22:25:46', 'co2_ppm': 3816.774187627254, 'no2_ppm': 5.08789923090447, 'so2_ppm': 4.435616661705975, 'temperature_c': 15.87314456815449, 'humidity_percent': 34.17447115179564, 'aqi': 4, 'environmental_score': 71.67051745510328, 'social_score': 57.32579050734851, 'governance_score': 73.77484055867585}\n",
      "Sent: {'timestamp': '2025-03-22 22:25:07', 'co2_ppm': 4245.73566072826, 'no2_ppm': 9.721594636423896, 'so2_ppm': 3.1723634196223727, 'temperature_c': 10.272307365198277, 'humidity_percent': 78.49670598002433, 'aqi': 6, 'environmental_score': 75.34243130328437, 'social_score': 81.25808165736223, 'governance_score': 98.09470638931468}\n",
      "Sent: {'timestamp': '2025-03-22 22:14:48', 'co2_ppm': 4044.569920227213, 'no2_ppm': 6.820979164668195, 'so2_ppm': 5.513080998878947, 'temperature_c': 27.040725567321783, 'humidity_percent': 79.98303998574269, 'aqi': 6, 'environmental_score': 95.5508120140446, 'social_score': 70.93050763408658, 'governance_score': 57.0198320758121}\n",
      "Sent: {'timestamp': '2025-03-22 22:24:51', 'co2_ppm': 1853.442169887888, 'no2_ppm': 9.598852598756936, 'so2_ppm': 3.753283979477385, 'temperature_c': 22.41442080764194, 'humidity_percent': 62.49190339291064, 'aqi': 6, 'environmental_score': 52.17906167288271, 'social_score': 54.66696152320097, 'governance_score': 87.05771597575347}\n",
      "Sent: {'timestamp': '2025-03-22 22:22:49', 'co2_ppm': 1253.39514484887, 'no2_ppm': 8.357440246853777, 'so2_ppm': 4.053422115248499, 'temperature_c': 34.02441779819715, 'humidity_percent': 52.06236600489416, 'aqi': 6, 'environmental_score': 52.1067252998978, 'social_score': 87.89526148381779, 'governance_score': 96.28828346310944}\n",
      "Sent: {'timestamp': '2025-03-22 22:15:27', 'co2_ppm': 1465.483053007763, 'no2_ppm': 10.822303314742117, 'so2_ppm': 5.303325184517293, 'temperature_c': 12.501205173863797, 'humidity_percent': 42.94018331652179, 'aqi': 8, 'environmental_score': 80.02347469632858, 'social_score': 90.1066738503052, 'governance_score': 70.56348898392494}\n",
      "Sent: {'timestamp': '2025-03-22 22:19:17', 'co2_ppm': 1109.2816901036745, 'no2_ppm': 7.95885643693517, 'so2_ppm': 6.880111694299692, 'temperature_c': 12.949550073702785, 'humidity_percent': 50.20490285201178, 'aqi': 7, 'environmental_score': 82.1485058217497, 'social_score': 66.66451593634136, 'governance_score': 79.44013498974384}\n",
      "Sent: {'timestamp': '2025-03-22 22:13:02', 'co2_ppm': 4623.6829363574925, 'no2_ppm': 13.109901679583572, 'so2_ppm': 3.5523832304120835, 'temperature_c': 31.92816866242679, 'humidity_percent': 88.76418883284678, 'aqi': 8, 'environmental_score': 96.64277352801508, 'social_score': 83.22668143365641, 'governance_score': 62.74547250402316}\n",
      "Sent: {'timestamp': '2025-03-22 22:16:19', 'co2_ppm': 2493.618202287258, 'no2_ppm': 6.004320013943711, 'so2_ppm': 1.906158869523713, 'temperature_c': 19.58780346037166, 'humidity_percent': 53.00059761899008, 'aqi': 3, 'environmental_score': 84.75604881240679, 'social_score': 93.77741417751346, 'governance_score': 81.6893330969801}\n",
      "Sent: {'timestamp': '2025-03-22 22:18:55', 'co2_ppm': 4268.721490788724, 'no2_ppm': 4.032018459323104, 'so2_ppm': 8.633604227573779, 'temperature_c': 26.112791996453463, 'humidity_percent': 71.74065391594394, 'aqi': 6, 'environmental_score': 92.08299038551708, 'social_score': 95.15422702056644, 'governance_score': 70.05836642096698}\n",
      "Sent: {'timestamp': '2025-03-22 22:13:38', 'co2_ppm': 3921.3742981765904, 'no2_ppm': 12.8945850898425, 'so2_ppm': 5.937372031798522, 'temperature_c': 31.32759745299034, 'humidity_percent': 78.65293405471206, 'aqi': 9, 'environmental_score': 72.91505709391404, 'social_score': 64.4551145269514, 'governance_score': 62.07067598381437}\n",
      "Sent: {'timestamp': '2025-03-22 22:14:03', 'co2_ppm': 4674.235566698722, 'no2_ppm': 8.924913097727739, 'so2_ppm': 6.276658506467278, 'temperature_c': 25.99056126020263, 'humidity_percent': 22.55788162231832, 'aqi': 7, 'environmental_score': 75.04232537653228, 'social_score': 77.07232563400294, 'governance_score': 93.29430200444348}\n",
      "Sent: {'timestamp': '2025-03-22 22:11:49', 'co2_ppm': 1052.5235335345535, 'no2_ppm': 6.178212432684412, 'so2_ppm': 7.052416991488138, 'temperature_c': 16.687966831340795, 'humidity_percent': 83.0937816490072, 'aqi': 6, 'environmental_score': 70.61623862365053, 'social_score': 82.19695452710735, 'governance_score': 85.07946773450142}\n",
      "Sent: {'timestamp': '2025-03-22 22:26:34', 'co2_ppm': 2829.700678350686, 'no2_ppm': 8.162528024165454, 'so2_ppm': 1.5647762173944144, 'temperature_c': 16.683117333447697, 'humidity_percent': 52.57283778411298, 'aqi': 4, 'environmental_score': 92.51439909602294, 'social_score': 96.56570009718764, 'governance_score': 96.40480160430658}\n",
      "Sent: {'timestamp': '2025-03-22 22:25:10', 'co2_ppm': 4316.838682798598, 'no2_ppm': 13.281423077766544, 'so2_ppm': 6.092021264267391, 'temperature_c': 10.419342173552746, 'humidity_percent': 50.70212783384598, 'aqi': 9, 'environmental_score': 88.19569901350184, 'social_score': 87.74753177600414, 'governance_score': 93.57764587303518}\n",
      "Sent: {'timestamp': '2025-03-22 22:25:55', 'co2_ppm': 2278.005871684849, 'no2_ppm': 7.664965891858906, 'so2_ppm': 1.2757445452695646, 'temperature_c': 30.48515833784541, 'humidity_percent': 63.45074953096064, 'aqi': 4, 'environmental_score': 96.43500209414208, 'social_score': 91.67794116943412, 'governance_score': 81.50310218256267}\n",
      "Sent: {'timestamp': '2025-03-22 22:15:54', 'co2_ppm': 3257.735115886926, 'no2_ppm': 12.417177357546166, 'so2_ppm': 5.916191692810107, 'temperature_c': 21.149836408246767, 'humidity_percent': 22.31973173539335, 'aqi': 9, 'environmental_score': 63.58746315296243, 'social_score': 73.95622609275559, 'governance_score': 70.81185769099666}\n",
      "Sent: {'timestamp': '2025-03-22 22:12:27', 'co2_ppm': 925.78189417899, 'no2_ppm': 16.956634034649714, 'so2_ppm': 11.004156382313823, 'temperature_c': 14.785884887676476, 'humidity_percent': 82.37723311972742, 'aqi': 13, 'environmental_score': 85.0588890477647, 'social_score': 91.63127545343472, 'governance_score': 74.71925903771553}\n",
      "Sent: {'timestamp': '2025-03-22 22:20:29', 'co2_ppm': 4405.373395953118, 'no2_ppm': 10.148556210261756, 'so2_ppm': 2.5774124817879027, 'temperature_c': 29.58049942492876, 'humidity_percent': 59.77284337355183, 'aqi': 6, 'environmental_score': 70.35194581147648, 'social_score': 94.38020805575806, 'governance_score': 56.49304418476621}\n",
      "Sent: {'timestamp': '2025-03-22 22:18:27', 'co2_ppm': 2684.828543241604, 'no2_ppm': 12.047669233948607, 'so2_ppm': 7.991835399795396, 'temperature_c': 16.771536649134212, 'humidity_percent': 69.64554389851699, 'aqi': 10, 'environmental_score': 77.503800719216, 'social_score': 93.06044602881632, 'governance_score': 72.54079047453351}\n",
      "Sent: {'timestamp': '2025-03-22 22:16:18', 'co2_ppm': 987.2733509251548, 'no2_ppm': 12.470050902871746, 'so2_ppm': 6.663662969681787, 'temperature_c': 26.31153678558468, 'humidity_percent': 85.80592654105196, 'aqi': 9, 'environmental_score': 63.58715050108678, 'social_score': 68.89112882023747, 'governance_score': 55.46598100857622}\n",
      "Sent: {'timestamp': '2025-03-22 22:13:26', 'co2_ppm': 2708.7896452190607, 'no2_ppm': 16.586214687159135, 'so2_ppm': 12.848040118793586, 'temperature_c': 38.03710923018734, 'humidity_percent': 84.18904081942172, 'aqi': 14, 'environmental_score': 78.99786872381162, 'social_score': 78.37813907153276, 'governance_score': 91.8800134460588}\n",
      "Sent: {'timestamp': '2025-03-22 22:21:35', 'co2_ppm': 3931.588332164564, 'no2_ppm': 12.009043530619351, 'so2_ppm': 5.322905266174805, 'temperature_c': 19.9354090728284, 'humidity_percent': 77.08777010397816, 'aqi': 8, 'environmental_score': 82.861030589384, 'social_score': 55.66249525196205, 'governance_score': 76.0521410572428}\n",
      "Sent: {'timestamp': '2025-03-22 22:19:52', 'co2_ppm': 3890.38895907256, 'no2_ppm': 11.9336730119958, 'so2_ppm': 3.620211263169094, 'temperature_c': 18.246514801525034, 'humidity_percent': 53.72228765414697, 'aqi': 7, 'environmental_score': 86.58437304157648, 'social_score': 94.55425390042204, 'governance_score': 62.073419172935125}\n",
      "Sent: {'timestamp': '2025-03-22 22:25:09', 'co2_ppm': 3905.705052077592, 'no2_ppm': 13.253131770003472, 'so2_ppm': 12.80253067213212, 'temperature_c': 20.971422147313422, 'humidity_percent': 61.52501351530764, 'aqi': 13, 'environmental_score': 82.45035196361917, 'social_score': 55.03223263762764, 'governance_score': 87.75719882107296}\n",
      "Sent: {'timestamp': '2025-03-22 22:11:07', 'co2_ppm': 4129.512752877859, 'no2_ppm': 3.600722158524469, 'so2_ppm': 2.0648402504253283, 'temperature_c': 30.956225555523552, 'humidity_percent': 85.85160685059832, 'aqi': 2, 'environmental_score': 92.92723108953808, 'social_score': 60.167684393542615, 'governance_score': 50.6518074835429}\n",
      "Sent: {'timestamp': '2025-03-22 22:15:36', 'co2_ppm': 3731.707280430706, 'no2_ppm': 3.4758656044974785, 'so2_ppm': 2.8397180426763025, 'temperature_c': 30.79515180337707, 'humidity_percent': 21.46292781145407, 'aqi': 3, 'environmental_score': 71.48355127201967, 'social_score': 88.01519817346252, 'governance_score': 52.15172836291796}\n",
      "Sent: {'timestamp': '2025-03-22 22:11:44', 'co2_ppm': 1987.4896709533296, 'no2_ppm': 5.19993137022564, 'so2_ppm': 1.6612117954727883, 'temperature_c': 29.75896258145695, 'humidity_percent': 27.53355370974265, 'aqi': 3, 'environmental_score': 84.86843821259828, 'social_score': 62.91331878360594, 'governance_score': 92.44389244356933}\n",
      "Sent: {'timestamp': '2025-03-22 22:15:20', 'co2_ppm': 4052.31381884502, 'no2_ppm': 15.805942291674654, 'so2_ppm': 4.332842000159152, 'temperature_c': 22.12636973626952, 'humidity_percent': 60.88615645485791, 'aqi': 10, 'environmental_score': 92.51343305506644, 'social_score': 57.9607106525307, 'governance_score': 93.03267642404396}\n",
      "Sent: {'timestamp': '2025-03-22 22:26:22', 'co2_ppm': 779.3593430886858, 'no2_ppm': 3.494456145412804, 'so2_ppm': 4.135243988707242, 'temperature_c': 16.697526839286045, 'humidity_percent': 48.94052292439197, 'aqi': 3, 'environmental_score': 58.40931986757351, 'social_score': 57.81259464023636, 'governance_score': 60.62473819508966}\n",
      "Sent: {'timestamp': '2025-03-22 22:15:07', 'co2_ppm': 4837.426597473891, 'no2_ppm': 5.579359727629551, 'so2_ppm': 5.30047931825437, 'temperature_c': 19.02205266474865, 'humidity_percent': 70.02651043515692, 'aqi': 5, 'environmental_score': 53.498143112136816, 'social_score': 80.55279004655671, 'governance_score': 69.13703177388871}\n",
      "Sent: {'timestamp': '2025-03-22 22:14:45', 'co2_ppm': 3918.220012829032, 'no2_ppm': 21.40616651634604, 'so2_ppm': 2.354672264088481, 'temperature_c': 18.63797619589348, 'humidity_percent': 66.28278043804747, 'aqi': 11, 'environmental_score': 83.77687379234263, 'social_score': 84.92853878442989, 'governance_score': 76.30804358626804}\n",
      "Sent: {'timestamp': '2025-03-22 22:25:32', 'co2_ppm': 4200.514771562773, 'no2_ppm': 7.340132699095748, 'so2_ppm': 9.241946889485806, 'temperature_c': 24.98861943051132, 'humidity_percent': 28.66904313611642, 'aqi': 8, 'environmental_score': 65.45170908469294, 'social_score': 89.19477411155216, 'governance_score': 81.95089030354636}\n",
      "Sent: {'timestamp': '2025-03-22 22:20:09', 'co2_ppm': 2973.022573266912, 'no2_ppm': 13.16281051215469, 'so2_ppm': 1.7388834882154758, 'temperature_c': 30.930153734999585, 'humidity_percent': 25.00953835732298, 'aqi': 7, 'environmental_score': 63.77743230926278, 'social_score': 84.36874749449004, 'governance_score': 56.88186089148554}\n",
      "Sent: {'timestamp': '2025-03-22 22:17:27', 'co2_ppm': 4550.964645319837, 'no2_ppm': 15.460774092978484, 'so2_ppm': 4.254818313898477, 'temperature_c': 38.288681189042514, 'humidity_percent': 87.26560111683145, 'aqi': 9, 'environmental_score': 73.34272402181463, 'social_score': 59.412041103253365, 'governance_score': 82.72007523665641}\n",
      "Sent: {'timestamp': '2025-03-22 22:24:05', 'co2_ppm': 4196.329459765671, 'no2_ppm': 18.718593473348328, 'so2_ppm': 11.02915029503667, 'temperature_c': 20.379381524334025, 'humidity_percent': 49.13633263299157, 'aqi': 14, 'environmental_score': 67.59420298054556, 'social_score': 65.49391907748834, 'governance_score': 80.64927848571945}\n",
      "Sent: {'timestamp': '2025-03-22 22:19:38', 'co2_ppm': 3134.5138725199013, 'no2_ppm': 16.374199695381353, 'so2_ppm': 8.207056914349904, 'temperature_c': 20.29215762893225, 'humidity_percent': 62.00327414670791, 'aqi': 12, 'environmental_score': 81.59236298801275, 'social_score': 82.0470640389409, 'governance_score': 68.94718638556995}\n",
      "Sent: {'timestamp': '2025-03-22 22:24:47', 'co2_ppm': 4050.3188119927, 'no2_ppm': 10.672537574069688, 'so2_ppm': 9.431944599647728, 'temperature_c': 10.82918174048399, 'humidity_percent': 85.90602580058533, 'aqi': 10, 'environmental_score': 76.68482376717301, 'social_score': 89.28259011595561, 'governance_score': 90.29924156155955}\n",
      "Sent: {'timestamp': '2025-03-22 22:14:29', 'co2_ppm': 2825.0031308110138, 'no2_ppm': 11.57627502243503, 'so2_ppm': 6.133292604947864, 'temperature_c': 37.1976985572902, 'humidity_percent': 54.7503695749942, 'aqi': 8, 'environmental_score': 81.08600542790926, 'social_score': 78.38563519549122, 'governance_score': 50.15710045359532}\n",
      "Sent: {'timestamp': '2025-03-22 22:19:46', 'co2_ppm': 1680.7213283183387, 'no2_ppm': 20.41143460571074, 'so2_ppm': 9.008268225160071, 'temperature_c': 17.60790297948918, 'humidity_percent': 59.39921549435967, 'aqi': 14, 'environmental_score': 70.78202246162583, 'social_score': 50.83153327503525, 'governance_score': 71.06267149544998}\n",
      "Sent: {'timestamp': '2025-03-22 22:10:31', 'co2_ppm': 1518.7102086749674, 'no2_ppm': 2.574177023738026, 'so2_ppm': 2.239999108804385, 'temperature_c': 13.577895293970212, 'humidity_percent': 29.799051278618336, 'aqi': 2, 'environmental_score': 70.6951112000786, 'social_score': 50.71436562434273, 'governance_score': 83.02866154558518}\n",
      "Sent: {'timestamp': '2025-03-22 22:10:38', 'co2_ppm': 3224.1844541234304, 'no2_ppm': 17.65700767302462, 'so2_ppm': 6.544333818818026, 'temperature_c': 28.13957494663289, 'humidity_percent': 67.11857747167697, 'aqi': 12, 'environmental_score': 79.10441807757864, 'social_score': 60.40061163963901, 'governance_score': 85.74276642882447}\n",
      "Sent: {'timestamp': '2025-03-22 22:20:25', 'co2_ppm': 450.3740588190934, 'no2_ppm': 7.06216173541911, 'so2_ppm': 3.1123377678503426, 'temperature_c': 22.221611413000154, 'humidity_percent': 62.47512348045388, 'aqi': 5, 'environmental_score': 90.11616775162211, 'social_score': 71.22570165351279, 'governance_score': 60.46939677933972}\n",
      "Sent: {'timestamp': '2025-03-22 22:10:18', 'co2_ppm': 479.14386535522175, 'no2_ppm': 9.41290542030278, 'so2_ppm': 5.633766213813517, 'temperature_c': 39.31935349150929, 'humidity_percent': 53.98547992251512, 'aqi': 7, 'environmental_score': 50.76149371366434, 'social_score': 50.68317726877153, 'governance_score': 86.16929564208033}\n",
      "Sent: {'timestamp': '2025-03-22 22:13:49', 'co2_ppm': 3690.267424555923, 'no2_ppm': 12.924748279579187, 'so2_ppm': 6.775851697578562, 'temperature_c': 26.44890129756003, 'humidity_percent': 53.99872088164974, 'aqi': 9, 'environmental_score': 56.92495082216384, 'social_score': 87.03759236125916, 'governance_score': 50.30094063141563}\n",
      "Sent: {'timestamp': '2025-03-22 22:23:48', 'co2_ppm': 2012.8553320665744, 'no2_ppm': 5.929551494893745, 'so2_ppm': 2.479102407202745, 'temperature_c': 16.87032440153593, 'humidity_percent': 77.18594924656554, 'aqi': 4, 'environmental_score': 86.7452879533336, 'social_score': 88.11349613179857, 'governance_score': 87.85127175333537}\n",
      "Sent: {'timestamp': '2025-03-22 22:15:04', 'co2_ppm': 4414.730183660153, 'no2_ppm': 11.178162302362356, 'so2_ppm': 2.176976023224398, 'temperature_c': 37.71379434683787, 'humidity_percent': 74.9425617694223, 'aqi': 6, 'environmental_score': 95.82792278926615, 'social_score': 96.72321560686622, 'governance_score': 79.44336156983857}\n",
      "Sent: {'timestamp': '2025-03-22 22:11:02', 'co2_ppm': 3956.116823959151, 'no2_ppm': 6.0379089359786775, 'so2_ppm': 2.6601852140690645, 'temperature_c': 30.67716430110401, 'humidity_percent': 75.98072465978544, 'aqi': 4, 'environmental_score': 54.24002444952031, 'social_score': 50.88551866581004, 'governance_score': 73.03830769827468}\n",
      "Sent: {'timestamp': '2025-03-22 22:19:10', 'co2_ppm': 812.8884313806033, 'no2_ppm': 15.545745047224903, 'so2_ppm': 6.772821031845237, 'temperature_c': 18.48202630561409, 'humidity_percent': 59.75054711223947, 'aqi': 11, 'environmental_score': 91.62266016545422, 'social_score': 54.22241489434242, 'governance_score': 91.25781400988416}\n",
      "Sent: {'timestamp': '2025-03-22 22:13:37', 'co2_ppm': 4835.077390194306, 'no2_ppm': 3.8029924567848754, 'so2_ppm': 7.149401277078544, 'temperature_c': 23.279163188184945, 'humidity_percent': 45.5879290018146, 'aqi': 5, 'environmental_score': 64.02039310091553, 'social_score': 92.81452780278202, 'governance_score': 74.99745380850051}\n",
      "Sent: {'timestamp': '2025-03-22 22:20:12', 'co2_ppm': 479.3440522688102, 'no2_ppm': 15.474817877109276, 'so2_ppm': 3.1207720794805383, 'temperature_c': 20.939199730149703, 'humidity_percent': 35.72888746440016, 'aqi': 9, 'environmental_score': 61.3832527446338, 'social_score': 93.83890026645857, 'governance_score': 64.76966214348353}\n",
      "Sent: {'timestamp': '2025-03-22 22:21:40', 'co2_ppm': 4354.315513868391, 'no2_ppm': 12.825375293431062, 'so2_ppm': 7.106934270264118, 'temperature_c': 23.794281477185244, 'humidity_percent': 25.42314034462069, 'aqi': 9, 'environmental_score': 53.92673732626649, 'social_score': 95.46562938464292, 'governance_score': 94.58205940796827}\n",
      "Sent: {'timestamp': '2025-03-22 22:21:10', 'co2_ppm': 523.2368408842307, 'no2_ppm': 3.4078489307954025, 'so2_ppm': 0.6963630929521877, 'temperature_c': 20.29500902198005, 'humidity_percent': 45.00955419234978, 'aqi': 2, 'environmental_score': 97.34450376688903, 'social_score': 89.92937919377312, 'governance_score': 51.44193222536369}\n",
      "Sent: {'timestamp': '2025-03-22 22:24:34', 'co2_ppm': 1616.406934209086, 'no2_ppm': 14.693946307323788, 'so2_ppm': 4.756479353187179, 'temperature_c': 18.939858121050428, 'humidity_percent': 67.25991158625101, 'aqi': 9, 'environmental_score': 75.99300294859077, 'social_score': 60.85646763720427, 'governance_score': 95.69011665717304}\n",
      "Sent: {'timestamp': '2025-03-22 22:11:18', 'co2_ppm': 3844.285472285419, 'no2_ppm': 11.27763134510952, 'so2_ppm': 9.02507737878515, 'temperature_c': 31.833912190738097, 'humidity_percent': 42.28069578419308, 'aqi': 10, 'environmental_score': 69.45622717795118, 'social_score': 72.1566088841886, 'governance_score': 59.68370241953698}\n",
      "Sent: {'timestamp': '2025-03-22 22:19:11', 'co2_ppm': 3183.440399621604, 'no2_ppm': 6.22184728269588, 'so2_ppm': 2.052079420177124, 'temperature_c': 10.87903499371278, 'humidity_percent': 52.32336725054131, 'aqi': 4, 'environmental_score': 64.39417632692614, 'social_score': 65.4886040860689, 'governance_score': 96.48816930007438}\n",
      "Sent: {'timestamp': '2025-03-22 22:11:55', 'co2_ppm': 1183.5189372692446, 'no2_ppm': 5.912422556238729, 'so2_ppm': 3.473746985512734, 'temperature_c': 28.529900622901263, 'humidity_percent': 28.50965337255668, 'aqi': 4, 'environmental_score': 79.11994899037832, 'social_score': 81.83589084116421, 'governance_score': 53.8435381864792}\n",
      "Sent: {'timestamp': '2025-03-22 22:18:21', 'co2_ppm': 2636.2730226524704, 'no2_ppm': 2.6685720515560503, 'so2_ppm': 5.249055887868805, 'temperature_c': 20.009595013876456, 'humidity_percent': 76.61268224118687, 'aqi': 3, 'environmental_score': 63.13179672243977, 'social_score': 70.06179636000141, 'governance_score': 71.4061494982766}\n",
      "Sent: {'timestamp': '2025-03-22 22:19:58', 'co2_ppm': 3943.50476729388, 'no2_ppm': 8.990660379245924, 'so2_ppm': 1.4844014190138048, 'temperature_c': 21.42019782662179, 'humidity_percent': 47.41741623812602, 'aqi': 5, 'environmental_score': 84.41354647819756, 'social_score': 69.98570699228485, 'governance_score': 76.46477401273184}\n",
      "Sent: {'timestamp': '2025-03-22 22:22:29', 'co2_ppm': 4420.169410293713, 'no2_ppm': 15.172814844809777, 'so2_ppm': 10.20500503813958, 'temperature_c': 16.60964648443553, 'humidity_percent': 79.05152230357851, 'aqi': 12, 'environmental_score': 98.6681101897637, 'social_score': 60.714138504868586, 'governance_score': 51.70412392756366}\n",
      "Sent: {'timestamp': '2025-03-22 22:15:00', 'co2_ppm': 903.872142200313, 'no2_ppm': 3.62248846879342, 'so2_ppm': 4.820192518651239, 'temperature_c': 33.92927424007905, 'humidity_percent': 87.61449698054841, 'aqi': 4, 'environmental_score': 61.78420286271607, 'social_score': 97.69127469132246, 'governance_score': 67.27303755901069}\n",
      "Sent: {'timestamp': '2025-03-22 22:14:57', 'co2_ppm': 4781.0870349710885, 'no2_ppm': 7.740093908799213, 'so2_ppm': 7.565231478089677, 'temperature_c': 32.19733016678579, 'humidity_percent': 37.39599469792552, 'aqi': 7, 'environmental_score': 89.91762829581717, 'social_score': 78.02578846935758, 'governance_score': 50.57723978146365}\n",
      "Sent: {'timestamp': '2025-03-22 22:21:00', 'co2_ppm': 2158.96433686768, 'no2_ppm': 6.51508036048262, 'so2_ppm': 7.598688662034669, 'temperature_c': 17.778642050072072, 'humidity_percent': 59.40476836070378, 'aqi': 7, 'environmental_score': 56.38568631031569, 'social_score': 54.4449638725072, 'governance_score': 77.94547259139357}\n",
      "Sent: {'timestamp': '2025-03-22 22:10:47', 'co2_ppm': 701.2515118050444, 'no2_ppm': 9.15296973675034, 'so2_ppm': 6.371151849785695, 'temperature_c': 11.762402409625924, 'humidity_percent': 37.60334686190807, 'aqi': 7, 'environmental_score': 62.282108835052895, 'social_score': 61.62782991779609, 'governance_score': 61.14547607566524}\n",
      "Sent: {'timestamp': '2025-03-22 22:19:57', 'co2_ppm': 435.52161876608704, 'no2_ppm': 19.5047266227024, 'so2_ppm': 2.4719220844503136, 'temperature_c': 12.86239677090992, 'humidity_percent': 27.34617291796293, 'aqi': 10, 'environmental_score': 62.7842436940715, 'social_score': 60.98300008317311, 'governance_score': 78.85937640282698}\n",
      "Sent: {'timestamp': '2025-03-22 22:15:15', 'co2_ppm': 3946.5025190970496, 'no2_ppm': 5.6358772686544905, 'so2_ppm': 3.783322097414637, 'temperature_c': 27.894039597060885, 'humidity_percent': 67.34719508245091, 'aqi': 4, 'environmental_score': 86.65332502936019, 'social_score': 60.89934421652772, 'governance_score': 61.405927968823626}\n",
      "Sent: {'timestamp': '2025-03-22 22:21:27', 'co2_ppm': 521.9773649915599, 'no2_ppm': 11.029702125946544, 'so2_ppm': 6.057558408743052, 'temperature_c': 24.799598394686218, 'humidity_percent': 64.8189997552883, 'aqi': 8, 'environmental_score': 58.34501894345679, 'social_score': 90.50662633974032, 'governance_score': 61.90062671911216}\n",
      "Sent: {'timestamp': '2025-03-22 22:11:14', 'co2_ppm': 4580.92359745142, 'no2_ppm': 13.30657659808243, 'so2_ppm': 9.336244871852296, 'temperature_c': 39.65474062146794, 'humidity_percent': 72.74179604024397, 'aqi': 11, 'environmental_score': 97.12318496918265, 'social_score': 64.67632330694606, 'governance_score': 82.01279403234435}\n",
      "Sent: {'timestamp': '2025-03-22 22:14:43', 'co2_ppm': 3634.8450572212873, 'no2_ppm': 18.137543698205008, 'so2_ppm': 13.084463364941753, 'temperature_c': 10.242117221890108, 'humidity_percent': 51.67496696318642, 'aqi': 15, 'environmental_score': 75.988151351827, 'social_score': 97.1692848484576, 'governance_score': 92.3071350193403}\n",
      "Sent: {'timestamp': '2025-03-22 22:11:15', 'co2_ppm': 4998.380459149806, 'no2_ppm': 10.938451395849093, 'so2_ppm': 2.666000283505497, 'temperature_c': 15.201525730075025, 'humidity_percent': 45.191360119057606, 'aqi': 6, 'environmental_score': 89.39742336185203, 'social_score': 74.78697107728911, 'governance_score': 85.38541994450385}\n",
      "Sent: {'timestamp': '2025-03-22 22:15:26', 'co2_ppm': 1395.1309419667928, 'no2_ppm': 13.780754462278768, 'so2_ppm': 5.338867230973416, 'temperature_c': 13.3737451251086, 'humidity_percent': 62.31668860049921, 'aqi': 9, 'environmental_score': 68.28035607057696, 'social_score': 88.47884385752099, 'governance_score': 60.3173696399441}\n",
      "Sent: {'timestamp': '2025-03-22 22:19:16', 'co2_ppm': 3178.2091569300346, 'no2_ppm': 2.3475571094048906, 'so2_ppm': 2.7277615350328506, 'temperature_c': 26.9231215231508, 'humidity_percent': 38.66745021331753, 'aqi': 2, 'environmental_score': 89.85315725591636, 'social_score': 75.66764104382712, 'governance_score': 62.78238261344713}\n",
      "Sent: {'timestamp': '2025-03-22 22:26:46', 'co2_ppm': 1316.5745523480805, 'no2_ppm': 4.238063520890242, 'so2_ppm': 2.180605632442433, 'temperature_c': 23.23971556197628, 'humidity_percent': 39.91758252640189, 'aqi': 3, 'environmental_score': 56.86197690582314, 'social_score': 57.02883828882072, 'governance_score': 96.51468962240844}\n",
      "Sent: {'timestamp': '2025-03-22 22:22:02', 'co2_ppm': 442.2778329058346, 'no2_ppm': 2.756498051333446, 'so2_ppm': 3.712449744880773, 'temperature_c': 15.56807951115776, 'humidity_percent': 87.38507093696614, 'aqi': 3, 'environmental_score': 88.81660611230608, 'social_score': 78.81715729627903, 'governance_score': 95.38569278251155}\n",
      "Sent: {'timestamp': '2025-03-22 22:22:41', 'co2_ppm': 533.3929690129654, 'no2_ppm': 2.7727717659233315, 'so2_ppm': 1.097555713888528, 'temperature_c': 35.69507275981336, 'humidity_percent': 72.54322536875424, 'aqi': 1, 'environmental_score': 92.63681362992877, 'social_score': 82.01963358795098, 'governance_score': 67.29285402239115}\n",
      "Sent: {'timestamp': '2025-03-22 22:15:35', 'co2_ppm': 653.4736296454233, 'no2_ppm': 18.37646436504977, 'so2_ppm': 2.1565335047202474, 'temperature_c': 18.1018223243535, 'humidity_percent': 82.72612437842899, 'aqi': 10, 'environmental_score': 93.14314587558168, 'social_score': 95.91283561991006, 'governance_score': 91.38585547372662}\n",
      "Sent: {'timestamp': '2025-03-22 22:16:30', 'co2_ppm': 2105.722691655518, 'no2_ppm': 22.70244811049232, 'so2_ppm': 7.239770310854105, 'temperature_c': 25.85003342922151, 'humidity_percent': 77.55666083627654, 'aqi': 14, 'environmental_score': 57.09041556560358, 'social_score': 82.6561761953307, 'governance_score': 79.8868807571834}\n",
      "Sent: {'timestamp': '2025-03-22 22:17:39', 'co2_ppm': 3501.8757729482463, 'no2_ppm': 4.761085755414244, 'so2_ppm': 10.757598567704788, 'temperature_c': 26.01008674377871, 'humidity_percent': 54.85251853788211, 'aqi': 7, 'environmental_score': 84.52574561265207, 'social_score': 75.0787951643534, 'governance_score': 97.24885793735255}\n",
      "Sent: {'timestamp': '2025-03-22 22:18:50', 'co2_ppm': 2759.0176362460284, 'no2_ppm': 3.00290690254217, 'so2_ppm': 2.856253598552234, 'temperature_c': 10.954469631631902, 'humidity_percent': 24.29233515620509, 'aqi': 2, 'environmental_score': 97.81865381919752, 'social_score': 70.42666707614273, 'governance_score': 54.84615425826873}\n",
      "Sent: {'timestamp': '2025-03-22 22:13:46', 'co2_ppm': 1846.720012675692, 'no2_ppm': 9.657687269554216, 'so2_ppm': 4.046895602292971, 'temperature_c': 21.626580782745123, 'humidity_percent': 72.60148521808028, 'aqi': 6, 'environmental_score': 56.15448049275592, 'social_score': 78.29016873242699, 'governance_score': 63.72414646422111}\n",
      "Sent: {'timestamp': '2025-03-22 22:23:15', 'co2_ppm': 4630.297222582515, 'no2_ppm': 6.0660738407621, 'so2_ppm': 3.507279532319461, 'temperature_c': 25.05141656415728, 'humidity_percent': 53.97356756151427, 'aqi': 4, 'environmental_score': 62.8478302319184, 'social_score': 73.09398895875862, 'governance_score': 90.28568151007472}\n",
      "Sent: {'timestamp': '2025-03-22 22:17:24', 'co2_ppm': 844.2409221666217, 'no2_ppm': 3.7626583678325134, 'so2_ppm': 3.1882041632737628, 'temperature_c': 23.278536075304785, 'humidity_percent': 85.93973974024267, 'aqi': 3, 'environmental_score': 66.61449857870231, 'social_score': 79.73535972790101, 'governance_score': 71.88322172088343}\n",
      "Sent: {'timestamp': '2025-03-22 22:22:52', 'co2_ppm': 4549.148228703995, 'no2_ppm': 8.410482672209632, 'so2_ppm': 7.2657078425230255, 'temperature_c': 15.410887823192263, 'humidity_percent': 42.61241230214421, 'aqi': 7, 'environmental_score': 61.61474315611192, 'social_score': 91.11679568987245, 'governance_score': 58.22421608050111}\n",
      "Sent: {'timestamp': '2025-03-22 22:11:36', 'co2_ppm': 612.8815134550518, 'no2_ppm': 12.18226503364244, 'so2_ppm': 11.49455653588341, 'temperature_c': 33.884790030974166, 'humidity_percent': 21.170483858712767, 'aqi': 11, 'environmental_score': 78.34013385451736, 'social_score': 51.30349227762628, 'governance_score': 63.48669697445667}\n",
      "Sent: {'timestamp': '2025-03-22 22:11:11', 'co2_ppm': 3232.976095537385, 'no2_ppm': 7.812734609136183, 'so2_ppm': 2.5309491269883684, 'temperature_c': 33.81466257913461, 'humidity_percent': 46.39046853800002, 'aqi': 5, 'environmental_score': 61.09511981229602, 'social_score': 90.33348022220468, 'governance_score': 83.49354489605122}\n",
      "Sent: {'timestamp': '2025-03-22 22:23:54', 'co2_ppm': 3300.288001754862, 'no2_ppm': 8.025149633050525, 'so2_ppm': 4.45856717039571, 'temperature_c': 12.870449538681836, 'humidity_percent': 63.99476323367496, 'aqi': 6, 'environmental_score': 82.99634716965473, 'social_score': 70.06029235637139, 'governance_score': 80.10213784924365}\n",
      "Sent: {'timestamp': '2025-03-22 22:22:04', 'co2_ppm': 4325.49523868341, 'no2_ppm': 9.60375385205854, 'so2_ppm': 4.997746855083566, 'temperature_c': 38.10931619199658, 'humidity_percent': 70.27603680664413, 'aqi': 7, 'environmental_score': 78.97607480623702, 'social_score': 81.88505451528682, 'governance_score': 69.56306299954049}\n",
      "Sent: {'timestamp': '2025-03-22 22:15:59', 'co2_ppm': 3321.8158904227985, 'no2_ppm': 5.833905781069828, 'so2_ppm': 2.3375640805750626, 'temperature_c': 16.059485408086317, 'humidity_percent': 41.375379077989415, 'aqi': 4, 'environmental_score': 61.063413039408026, 'social_score': 92.62390992371706, 'governance_score': 79.23068431377335}\n",
      "Sent: {'timestamp': '2025-03-22 22:25:44', 'co2_ppm': 2199.820515195579, 'no2_ppm': 10.72779199508943, 'so2_ppm': 6.208617785954285, 'temperature_c': 22.34322969509233, 'humidity_percent': 43.88381395128376, 'aqi': 8, 'environmental_score': 97.59159025308584, 'social_score': 62.17492303194771, 'governance_score': 55.024276390064905}\n",
      "Sent: {'timestamp': '2025-03-22 22:14:24', 'co2_ppm': 3585.8475759910943, 'no2_ppm': 14.22929443666794, 'so2_ppm': 3.9763458886992185, 'temperature_c': 15.689417723825056, 'humidity_percent': 56.19470891312673, 'aqi': 9, 'environmental_score': 77.73184588697634, 'social_score': 58.82839101925442, 'governance_score': 69.65564456630581}\n",
      "Sent: {'timestamp': '2025-03-22 22:20:43', 'co2_ppm': 878.5864359092981, 'no2_ppm': 1.4918860499095217, 'so2_ppm': 1.5338743457788435, 'temperature_c': 34.747883406564114, 'humidity_percent': 32.27150209137484, 'aqi': 1, 'environmental_score': 83.71602155171409, 'social_score': 51.424391702086766, 'governance_score': 72.3427157408893}\n",
      "Sent: {'timestamp': '2025-03-22 22:20:52', 'co2_ppm': 2290.417622368824, 'no2_ppm': 7.811397088548997, 'so2_ppm': 6.627846156808736, 'temperature_c': 12.608570875160051, 'humidity_percent': 88.09754322319583, 'aqi': 7, 'environmental_score': 97.71140475778222, 'social_score': 78.4911809949379, 'governance_score': 71.6641944869617}\n",
      "Sent: {'timestamp': '2025-03-22 22:19:22', 'co2_ppm': 2261.444048008296, 'no2_ppm': 6.3611734734738885, 'so2_ppm': 7.915029078601991, 'temperature_c': 10.559777585721047, 'humidity_percent': 38.99785026377703, 'aqi': 7, 'environmental_score': 90.43642027787875, 'social_score': 73.98112127827302, 'governance_score': 91.7853156157158}\n",
      "Sent: {'timestamp': '2025-03-22 22:26:07', 'co2_ppm': 3141.346729730718, 'no2_ppm': 18.52853235218184, 'so2_ppm': 5.295754896819359, 'temperature_c': 35.01634980549044, 'humidity_percent': 68.35584079561428, 'aqi': 11, 'environmental_score': 70.63975933298816, 'social_score': 94.9733235057374, 'governance_score': 78.32629775269606}\n",
      "Sent: {'timestamp': '2025-03-22 22:12:56', 'co2_ppm': 3396.85287539664, 'no2_ppm': 2.5740219482395053, 'so2_ppm': 4.246071002114396, 'temperature_c': 12.060748710840398, 'humidity_percent': 27.38306915657389, 'aqi': 3, 'environmental_score': 74.73308719390073, 'social_score': 94.4088650079627, 'governance_score': 73.49420022398976}\n",
      "Sent: {'timestamp': '2025-03-22 22:25:25', 'co2_ppm': 4819.215640026492, 'no2_ppm': 7.115729022163439, 'so2_ppm': 2.147722189849928, 'temperature_c': 30.59256290559881, 'humidity_percent': 20.00633618505108, 'aqi': 4, 'environmental_score': 74.84134358966143, 'social_score': 84.62032314451656, 'governance_score': 50.69137910858473}\n",
      "Sent: {'timestamp': '2025-03-22 22:18:56', 'co2_ppm': 936.2756590494482, 'no2_ppm': 14.712982535124729, 'so2_ppm': 10.451303780864398, 'temperature_c': 37.94460062696783, 'humidity_percent': 84.9057991089008, 'aqi': 12, 'environmental_score': 77.4805930441031, 'social_score': 96.285457157544, 'governance_score': 79.63416618242557}\n",
      "Sent: {'timestamp': '2025-03-22 22:23:51', 'co2_ppm': 810.7763653694236, 'no2_ppm': 21.489687872580376, 'so2_ppm': 5.436961570496642, 'temperature_c': 24.399050726799373, 'humidity_percent': 52.36200246161679, 'aqi': 13, 'environmental_score': 88.05915948843656, 'social_score': 85.53123271737954, 'governance_score': 87.51054089074677}\n",
      "Sent: {'timestamp': '2025-03-22 22:26:33', 'co2_ppm': 4521.745078933069, 'no2_ppm': 6.606222257732865, 'so2_ppm': 7.719075550219488, 'temperature_c': 19.83160933632356, 'humidity_percent': 65.29796633643764, 'aqi': 7, 'environmental_score': 71.53231722268535, 'social_score': 87.0478483773773, 'governance_score': 94.88312501388266}\n",
      "Sent: {'timestamp': '2025-03-22 22:23:44', 'co2_ppm': 3804.1007811366258, 'no2_ppm': 7.811854064443214, 'so2_ppm': 6.46566512120814, 'temperature_c': 37.402874294380254, 'humidity_percent': 68.07280768896305, 'aqi': 7, 'environmental_score': 90.07025012307963, 'social_score': 86.69825645690061, 'governance_score': 77.9265877335526}\n",
      "Sent: {'timestamp': '2025-03-22 22:11:34', 'co2_ppm': 4798.344092385888, 'no2_ppm': 11.671565045303645, 'so2_ppm': 9.277131123113866, 'temperature_c': 31.33283256804433, 'humidity_percent': 52.68114573085944, 'aqi': 10, 'environmental_score': 76.44257057755429, 'social_score': 82.47206960094988, 'governance_score': 81.20547771074297}\n",
      "Sent: {'timestamp': '2025-03-22 22:17:20', 'co2_ppm': 4186.023428242919, 'no2_ppm': 11.07247097466852, 'so2_ppm': 4.56495837824476, 'temperature_c': 25.27885234236695, 'humidity_percent': 79.31328122902401, 'aqi': 7, 'environmental_score': 60.17477669669717, 'social_score': 54.971088471429525, 'governance_score': 96.69492398342888}\n",
      "Sent: {'timestamp': '2025-03-22 22:13:12', 'co2_ppm': 4474.924329909332, 'no2_ppm': 4.194548575764949, 'so2_ppm': 2.3354813864719555, 'temperature_c': 30.172658738153885, 'humidity_percent': 71.97160514395559, 'aqi': 3, 'environmental_score': 75.30738965551605, 'social_score': 81.64789157130254, 'governance_score': 73.20303376998781}\n",
      "Sent: {'timestamp': '2025-03-22 22:26:26', 'co2_ppm': 3608.666449088549, 'no2_ppm': 4.3940653385786295, 'so2_ppm': 11.688685914500438, 'temperature_c': 14.365134195123243, 'humidity_percent': 89.1388787610544, 'aqi': 8, 'environmental_score': 59.263125631015896, 'social_score': 66.71663988557312, 'governance_score': 87.1180854636501}\n",
      "Sent: {'timestamp': '2025-03-22 22:19:02', 'co2_ppm': 4712.571595858267, 'no2_ppm': 7.784003399671016, 'so2_ppm': 5.67886599687649, 'temperature_c': 26.40130545325495, 'humidity_percent': 87.7491855286518, 'aqi': 6, 'environmental_score': 64.23967517348078, 'social_score': 77.26003406746369, 'governance_score': 80.56455575700984}\n",
      "Sent: {'timestamp': '2025-03-22 22:15:02', 'co2_ppm': 4490.424185156445, 'no2_ppm': 4.75536500251487, 'so2_ppm': 10.243845631789638, 'temperature_c': 18.330042242144472, 'humidity_percent': 56.71190009147678, 'aqi': 7, 'environmental_score': 90.81491966339912, 'social_score': 81.85271496973397, 'governance_score': 87.9423197188903}\n",
      "Sent: {'timestamp': '2025-03-22 22:25:00', 'co2_ppm': 783.1510345689207, 'no2_ppm': 9.306131034385654, 'so2_ppm': 1.930123774141271, 'temperature_c': 30.05363718622469, 'humidity_percent': 47.02543905395504, 'aqi': 5, 'environmental_score': 74.10785404886396, 'social_score': 53.36387064553897, 'governance_score': 71.10016072702231}\n",
      "Sent: {'timestamp': '2025-03-22 22:17:13', 'co2_ppm': 2743.0133429897537, 'no2_ppm': 9.720897610014688, 'so2_ppm': 3.4821937721664007, 'temperature_c': 31.166665962709345, 'humidity_percent': 34.74705774513247, 'aqi': 6, 'environmental_score': 56.70777277954415, 'social_score': 83.36573966789594, 'governance_score': 91.01701858284277}\n",
      "Sent: {'timestamp': '2025-03-22 22:22:01', 'co2_ppm': 3771.235094393408, 'no2_ppm': 8.768500540218023, 'so2_ppm': 5.220496129318249, 'temperature_c': 22.28150164079353, 'humidity_percent': 83.04410229977313, 'aqi': 6, 'environmental_score': 59.16332482575851, 'social_score': 77.96228278021225, 'governance_score': 57.23417799749631}\n",
      "Sent: {'timestamp': '2025-03-22 22:14:28', 'co2_ppm': 4191.385023410865, 'no2_ppm': 3.4455809730860603, 'so2_ppm': 3.0396677563110925, 'temperature_c': 12.245020801020212, 'humidity_percent': 51.089543202211615, 'aqi': 3, 'environmental_score': 72.76234297565358, 'social_score': 59.14803124173741, 'governance_score': 75.94262275152101}\n",
      "Sent: {'timestamp': '2025-03-22 22:14:05', 'co2_ppm': 1744.432941252716, 'no2_ppm': 1.38606298205362, 'so2_ppm': 1.2563921471152986, 'temperature_c': 34.477299573884665, 'humidity_percent': 76.06337339915979, 'aqi': 1, 'environmental_score': 74.56959833631404, 'social_score': 83.55353320038807, 'governance_score': 58.25746227475828}\n",
      "Sent: {'timestamp': '2025-03-22 22:19:27', 'co2_ppm': 1434.2166936512515, 'no2_ppm': 3.708154713486637, 'so2_ppm': 4.017950276451115, 'temperature_c': 19.253722123459216, 'humidity_percent': 28.70200369014248, 'aqi': 3, 'environmental_score': 62.02637708137927, 'social_score': 52.99604109055955, 'governance_score': 71.99391160587561}\n",
      "Sent: {'timestamp': '2025-03-22 22:19:51', 'co2_ppm': 1330.1021216824429, 'no2_ppm': 2.8808359223375386, 'so2_ppm': 3.3040898918784536, 'temperature_c': 39.85705264938558, 'humidity_percent': 41.78477746577756, 'aqi': 3, 'environmental_score': 69.54208323264899, 'social_score': 99.42248707361068, 'governance_score': 76.7687875318609}\n",
      "Sent: {'timestamp': '2025-03-22 22:17:33', 'co2_ppm': 1942.3184877633405, 'no2_ppm': 1.458820687850882, 'so2_ppm': 1.3747891217031163, 'temperature_c': 18.84203350854679, 'humidity_percent': 47.66374410411339, 'aqi': 1, 'environmental_score': 86.87440722600164, 'social_score': 64.57462699819041, 'governance_score': 79.45692980215289}\n",
      "Sent: {'timestamp': '2025-03-22 22:24:35', 'co2_ppm': 3576.09773451162, 'no2_ppm': 18.591005610672955, 'so2_ppm': 8.648332242610156, 'temperature_c': 21.39895545892375, 'humidity_percent': 51.768759900447286, 'aqi': 13, 'environmental_score': 82.84680537881563, 'social_score': 73.78579699719849, 'governance_score': 76.85222984027664}\n",
      "Sent: {'timestamp': '2025-03-22 22:23:28', 'co2_ppm': 962.3936645118896, 'no2_ppm': 6.977089254918937, 'so2_ppm': 3.2278054856804648, 'temperature_c': 22.790300047604955, 'humidity_percent': 32.76190668881231, 'aqi': 5, 'environmental_score': 84.67127882806903, 'social_score': 84.00643942181298, 'governance_score': 82.03392093125638}\n",
      "Sent: {'timestamp': '2025-03-22 22:20:07', 'co2_ppm': 4867.434490727268, 'no2_ppm': 7.064203304146003, 'so2_ppm': 5.91081327063077, 'temperature_c': 33.395866319165435, 'humidity_percent': 31.25436550111932, 'aqi': 6, 'environmental_score': 99.4260141159456, 'social_score': 86.27257460750643, 'governance_score': 59.09077695309611}\n",
      "Sent: {'timestamp': '2025-03-22 22:24:17', 'co2_ppm': 1410.5934812933433, 'no2_ppm': 3.8497728553063815, 'so2_ppm': 3.3000804979137377, 'temperature_c': 24.092539268252125, 'humidity_percent': 48.9206188902255, 'aqi': 3, 'environmental_score': 61.75729204966549, 'social_score': 61.90837353516767, 'governance_score': 52.30576114310732}\n",
      "Sent: {'timestamp': '2025-03-22 22:14:33', 'co2_ppm': 4948.336472920757, 'no2_ppm': 5.840706932272406, 'so2_ppm': 8.07865919877744, 'temperature_c': 25.125164578593072, 'humidity_percent': 72.85326091556523, 'aqi': 6, 'environmental_score': 68.62491530857992, 'social_score': 61.06119682032269, 'governance_score': 58.35250794877337}\n",
      "Sent: {'timestamp': '2025-03-22 22:26:43', 'co2_ppm': 4477.445902003026, 'no2_ppm': 2.287858792971642, 'so2_ppm': 2.979777801799592, 'temperature_c': 34.4317481532175, 'humidity_percent': 20.220422586285046, 'aqi': 2, 'environmental_score': 79.71883357938812, 'social_score': 59.43643018831675, 'governance_score': 76.0048314231073}\n",
      "Sent: {'timestamp': '2025-03-22 22:18:57', 'co2_ppm': 4071.324506311629, 'no2_ppm': 9.517832546070771, 'so2_ppm': 6.956012532654199, 'temperature_c': 19.440075936511786, 'humidity_percent': 85.01041332996093, 'aqi': 8, 'environmental_score': 99.92582397444068, 'social_score': 77.22842251631354, 'governance_score': 69.15337037380894}\n",
      "Sent: {'timestamp': '2025-03-22 22:11:03', 'co2_ppm': 3244.17489279912, 'no2_ppm': 6.699228612341006, 'so2_ppm': 4.735955757422283, 'temperature_c': 36.818675373505634, 'humidity_percent': 40.88454072600181, 'aqi': 5, 'environmental_score': 69.4238195433932, 'social_score': 63.31065464064376, 'governance_score': 62.5320522847428}\n",
      "Sent: {'timestamp': '2025-03-22 22:12:08', 'co2_ppm': 713.956711076935, 'no2_ppm': 4.4157571450267135, 'so2_ppm': 1.339525501531251, 'temperature_c': 30.12199153019898, 'humidity_percent': 53.40747801775313, 'aqi': 2, 'environmental_score': 71.64003651708444, 'social_score': 51.36729562570834, 'governance_score': 50.56600489555471}\n",
      "Sent: {'timestamp': '2025-03-22 22:13:43', 'co2_ppm': 1806.640223960202, 'no2_ppm': 3.28453136551576, 'so2_ppm': 2.645780230463214, 'temperature_c': 31.446968837335987, 'humidity_percent': 60.16746567680923, 'aqi': 2, 'environmental_score': 61.27209218147948, 'social_score': 52.19050773987685, 'governance_score': 98.8191046036974}\n",
      "Sent: {'timestamp': '2025-03-22 22:10:33', 'co2_ppm': 4999.735838825597, 'no2_ppm': 4.905125562019912, 'so2_ppm': 6.360023198917505, 'temperature_c': 21.450333511345548, 'humidity_percent': 67.34642769185632, 'aqi': 5, 'environmental_score': 50.68757419574482, 'social_score': 92.61178913717204, 'governance_score': 73.51100960514547}\n",
      "Sent: {'timestamp': '2025-03-22 22:11:20', 'co2_ppm': 1593.9164253750666, 'no2_ppm': 5.324344798208631, 'so2_ppm': 1.7428054085657605, 'temperature_c': 20.218151390655592, 'humidity_percent': 35.48373777421657, 'aqi': 3, 'environmental_score': 67.07269014013883, 'social_score': 94.01903053815356, 'governance_score': 55.20672759205728}\n",
      "Sent: {'timestamp': '2025-03-22 22:10:52', 'co2_ppm': 734.6559315872316, 'no2_ppm': 11.87111494256627, 'so2_ppm': 7.757136526134562, 'temperature_c': 35.77816358337348, 'humidity_percent': 88.58316164075008, 'aqi': 9, 'environmental_score': 77.83888558526927, 'social_score': 99.45026532829124, 'governance_score': 81.57398687427715}\n",
      "Sent: {'timestamp': '2025-03-22 22:14:15', 'co2_ppm': 1658.0292709803373, 'no2_ppm': 15.205195549170002, 'so2_ppm': 7.787932209353388, 'temperature_c': 37.02754421163138, 'humidity_percent': 79.13321595604593, 'aqi': 11, 'environmental_score': 87.86567883326707, 'social_score': 61.55254492261773, 'governance_score': 63.76361040736619}\n",
      "Sent: {'timestamp': '2025-03-22 22:22:09', 'co2_ppm': 1430.649002918623, 'no2_ppm': 2.0214600326905483, 'so2_ppm': 1.1822495959517614, 'temperature_c': 15.205292804865628, 'humidity_percent': 36.95477272709379, 'aqi': 1, 'environmental_score': 72.85214067064547, 'social_score': 51.84728667965068, 'governance_score': 87.80177713994499}\n",
      "Sent: {'timestamp': '2025-03-22 22:14:49', 'co2_ppm': 4913.985352602445, 'no2_ppm': 12.970332874086235, 'so2_ppm': 5.6731180811369954, 'temperature_c': 26.75201490600138, 'humidity_percent': 52.76677588573297, 'aqi': 9, 'environmental_score': 68.38874252102667, 'social_score': 89.69537148915771, 'governance_score': 66.12846063405864}\n",
      "Sent: {'timestamp': '2025-03-22 22:25:01', 'co2_ppm': 3044.1715392620563, 'no2_ppm': 2.85251075783407, 'so2_ppm': 3.837268511604514, 'temperature_c': 10.915933520834916, 'humidity_percent': 36.24228839655065, 'aqi': 3, 'environmental_score': 64.18094568581733, 'social_score': 50.463364391727936, 'governance_score': 80.45411545353181}\n",
      "Sent: {'timestamp': '2025-03-22 22:25:22', 'co2_ppm': 3453.04171846945, 'no2_ppm': 6.210686816941451, 'so2_ppm': 5.197224532418478, 'temperature_c': 10.339780449347732, 'humidity_percent': 70.93093985996171, 'aqi': 5, 'environmental_score': 66.78673942020454, 'social_score': 50.4929753419482, 'governance_score': 81.17343934258959}\n",
      "Sent: {'timestamp': '2025-03-22 22:23:38', 'co2_ppm': 4592.996217566541, 'no2_ppm': 16.077431476975825, 'so2_ppm': 2.473793417597331, 'temperature_c': 20.636828192241584, 'humidity_percent': 34.174242719781034, 'aqi': 9, 'environmental_score': 78.38194965049159, 'social_score': 89.37029159179406, 'governance_score': 93.76064673704948}\n",
      "Sent: {'timestamp': '2025-03-22 22:14:12', 'co2_ppm': 1450.5338375824183, 'no2_ppm': 5.7977464403227845, 'so2_ppm': 5.667859926333115, 'temperature_c': 19.892419830469475, 'humidity_percent': 64.11021529741944, 'aqi': 5, 'environmental_score': 55.60907125290031, 'social_score': 68.23986921102338, 'governance_score': 81.38512129158997}\n",
      "Sent: {'timestamp': '2025-03-22 22:23:50', 'co2_ppm': 2636.621351574415, 'no2_ppm': 15.648630324584875, 'so2_ppm': 7.956304394524352, 'temperature_c': 37.78956062777229, 'humidity_percent': 42.48540314636588, 'aqi': 11, 'environmental_score': 56.35744363586102, 'social_score': 77.0206716325853, 'governance_score': 94.7130103697782}\n",
      "Sent: {'timestamp': '2025-03-22 22:15:29', 'co2_ppm': 2084.465462334865, 'no2_ppm': 4.162545977039269, 'so2_ppm': 3.1962355846496213, 'temperature_c': 34.59289736181154, 'humidity_percent': 69.66557922031427, 'aqi': 3, 'environmental_score': 91.70948379451308, 'social_score': 91.59565937264624, 'governance_score': 74.69678336190515}\n",
      "Sent: {'timestamp': '2025-03-22 22:20:51', 'co2_ppm': 856.9349522127683, 'no2_ppm': 7.195027866182229, 'so2_ppm': 7.898858662600194, 'temperature_c': 22.27271146971573, 'humidity_percent': 33.88320603415063, 'aqi': 7, 'environmental_score': 79.31873918827195, 'social_score': 76.67782589696219, 'governance_score': 71.56347443179749}\n",
      "Sent: {'timestamp': '2025-03-22 22:12:46', 'co2_ppm': 1616.884902409871, 'no2_ppm': 2.0315718152984155, 'so2_ppm': 1.3825003559324467, 'temperature_c': 24.73005937148632, 'humidity_percent': 31.383626906618098, 'aqi': 1, 'environmental_score': 73.95495033364114, 'social_score': 98.69815236241324, 'governance_score': 74.69034841569996}\n",
      "Sent: {'timestamp': '2025-03-22 22:26:25', 'co2_ppm': 4960.687436805166, 'no2_ppm': 5.300788645204227, 'so2_ppm': 7.694175164607707, 'temperature_c': 30.32195888451556, 'humidity_percent': 70.66308929901646, 'aqi': 6, 'environmental_score': 58.406030731402566, 'social_score': 88.16159406197997, 'governance_score': 90.97570831781948}\n",
      "Sent: {'timestamp': '2025-03-22 22:17:17', 'co2_ppm': 2827.9230885716506, 'no2_ppm': 14.831894270123936, 'so2_ppm': 4.5273207948143686, 'temperature_c': 38.52991972339861, 'humidity_percent': 48.52024078691157, 'aqi': 9, 'environmental_score': 67.59248372087453, 'social_score': 58.6436032071549, 'governance_score': 70.9561706539985}\n",
      "Sent: {'timestamp': '2025-03-22 22:25:49', 'co2_ppm': 4380.376452470282, 'no2_ppm': 5.324930399475618, 'so2_ppm': 7.02750813352242, 'temperature_c': 11.659358355438728, 'humidity_percent': 23.32657058149239, 'aqi': 6, 'environmental_score': 85.9357314357048, 'social_score': 67.51268818268558, 'governance_score': 67.04546716800432}\n",
      "Sent: {'timestamp': '2025-03-22 22:17:50', 'co2_ppm': 3872.875543290723, 'no2_ppm': 7.891554567006967, 'so2_ppm': 6.94737950079104, 'temperature_c': 13.05940778581039, 'humidity_percent': 27.45977986068528, 'aqi': 7, 'environmental_score': 56.69530170714791, 'social_score': 98.89161248552831, 'governance_score': 76.09784733910733}\n",
      "Sent: {'timestamp': '2025-03-22 22:15:17', 'co2_ppm': 1096.178177389962, 'no2_ppm': 8.131011926051556, 'so2_ppm': 3.856540244291461, 'temperature_c': 15.594509220344026, 'humidity_percent': 78.69876843473783, 'aqi': 5, 'environmental_score': 96.16241205980654, 'social_score': 63.70466045578638, 'governance_score': 80.29722964520147}\n",
      "Sent: {'timestamp': '2025-03-22 22:21:45', 'co2_ppm': 4673.928426842866, 'no2_ppm': 5.372202511766258, 'so2_ppm': 7.860639566821054, 'temperature_c': 20.66693203565038, 'humidity_percent': 61.71892631814698, 'aqi': 6, 'environmental_score': 91.0724206625791, 'social_score': 59.763696916445895, 'governance_score': 88.27521660817777}\n",
      "Sent: {'timestamp': '2025-03-22 22:11:08', 'co2_ppm': 1059.5534242038557, 'no2_ppm': 17.920750651185994, 'so2_ppm': 5.136910818318991, 'temperature_c': 13.483549768038303, 'humidity_percent': 43.79438482051099, 'aqi': 11, 'environmental_score': 73.3463749799756, 'social_score': 89.14713304309007, 'governance_score': 57.609911165401954}\n",
      "Sent: {'timestamp': '2025-03-22 22:24:52', 'co2_ppm': 3455.3003554351026, 'no2_ppm': 8.563311765979714, 'so2_ppm': 3.216750525739543, 'temperature_c': 14.39086234313314, 'humidity_percent': 81.1054459182097, 'aqi': 5, 'environmental_score': 71.26146877183913, 'social_score': 59.48805957580493, 'governance_score': 67.150554396584}\n",
      "Sent: {'timestamp': '2025-03-22 22:20:03', 'co2_ppm': 4515.380805512155, 'no2_ppm': 6.272264122990873, 'so2_ppm': 2.6364072372299163, 'temperature_c': 14.414336304928073, 'humidity_percent': 67.18527783993306, 'aqi': 4, 'environmental_score': 54.24754061651404, 'social_score': 92.44051783221936, 'governance_score': 74.27031937931669}\n",
      "Sent: {'timestamp': '2025-03-22 22:22:55', 'co2_ppm': 1278.8282411330051, 'no2_ppm': 5.961509932846666, 'so2_ppm': 9.41138533931404, 'temperature_c': 38.46602695468408, 'humidity_percent': 43.09744531697904, 'aqi': 7, 'environmental_score': 96.27347864242503, 'social_score': 97.70812355480224, 'governance_score': 85.62478223513014}\n",
      "Sent: {'timestamp': '2025-03-22 22:20:57', 'co2_ppm': 2770.636998237993, 'no2_ppm': 6.140232176430716, 'so2_ppm': 10.110003530556895, 'temperature_c': 15.27671280523574, 'humidity_percent': 65.63764872495847, 'aqi': 8, 'environmental_score': 79.71032058270698, 'social_score': 89.86965075020439, 'governance_score': 81.62941225518178}\n",
      "Sent: {'timestamp': '2025-03-22 22:20:58', 'co2_ppm': 1466.8072649360126, 'no2_ppm': 11.069587021952971, 'so2_ppm': 4.4213750201484086, 'temperature_c': 12.19506777086643, 'humidity_percent': 27.84994628165296, 'aqi': 7, 'environmental_score': 78.04399376980463, 'social_score': 64.09330259726156, 'governance_score': 68.564530869807}\n",
      "Sent: {'timestamp': '2025-03-22 22:24:33', 'co2_ppm': 1957.3805255916327, 'no2_ppm': 1.8804599001981244, 'so2_ppm': 4.061496511261395, 'temperature_c': 23.620593563279872, 'humidity_percent': 47.76834099575996, 'aqi': 2, 'environmental_score': 74.60154691098472, 'social_score': 78.82311471967977, 'governance_score': 90.44229401071983}\n",
      "Sent: {'timestamp': '2025-03-22 22:21:16', 'co2_ppm': 1761.837283465264, 'no2_ppm': 4.110186578125858, 'so2_ppm': 1.2536097340818777, 'temperature_c': 28.41222490253725, 'humidity_percent': 46.08494678022883, 'aqi': 2, 'environmental_score': 94.43463494144372, 'social_score': 92.90392376511484, 'governance_score': 54.62256027598097}\n",
      "Sent: {'timestamp': '2025-03-22 22:15:06', 'co2_ppm': 990.0011873291468, 'no2_ppm': 14.050040318701049, 'so2_ppm': 4.153193160966894, 'temperature_c': 28.362901349759547, 'humidity_percent': 51.31814650260184, 'aqi': 9, 'environmental_score': 56.24404860129501, 'social_score': 50.757637910984506, 'governance_score': 93.4049320617006}\n",
      "Sent: {'timestamp': '2025-03-22 22:21:37', 'co2_ppm': 2580.102957601669, 'no2_ppm': 19.3162950106704, 'so2_ppm': 8.1248880004475, 'temperature_c': 30.2515574403376, 'humidity_percent': 69.50750191863635, 'aqi': 13, 'environmental_score': 95.2743327329616, 'social_score': 96.80764001130714, 'governance_score': 85.36474103228315}\n",
      "Sent: {'timestamp': '2025-03-22 22:15:22', 'co2_ppm': 1925.7399882703985, 'no2_ppm': 22.07675649089262, 'so2_ppm': 11.158123446195509, 'temperature_c': 27.907328339067472, 'humidity_percent': 36.096264742132135, 'aqi': 16, 'environmental_score': 76.13093310044762, 'social_score': 50.8036718925066, 'governance_score': 77.3661705908593}\n",
      "Sent: {'timestamp': '2025-03-22 22:15:18', 'co2_ppm': 3821.4162187627103, 'no2_ppm': 12.052254206163251, 'so2_ppm': 10.228257345752247, 'temperature_c': 13.856806169537688, 'humidity_percent': 62.7371082331476, 'aqi': 11, 'environmental_score': 58.97045032741583, 'social_score': 65.16559934699876, 'governance_score': 77.22078664233496}\n",
      "Sent: {'timestamp': '2025-03-22 22:16:09', 'co2_ppm': 1340.52472442958, 'no2_ppm': 6.166606888339583, 'so2_ppm': 5.951168895333547, 'temperature_c': 13.906387017676014, 'humidity_percent': 88.1626337047179, 'aqi': 6, 'environmental_score': 91.83784076994463, 'social_score': 93.77999511275635, 'governance_score': 72.9243364421796}\n",
      "Sent: {'timestamp': '2025-03-22 22:18:07', 'co2_ppm': 3749.1470526649614, 'no2_ppm': 4.06357478776397, 'so2_ppm': 5.04409976955027, 'temperature_c': 17.05918146776372, 'humidity_percent': 35.40684746295696, 'aqi': 4, 'environmental_score': 59.22915136831172, 'social_score': 65.92642244417038, 'governance_score': 99.79445793393278}\n",
      "Sent: {'timestamp': '2025-03-22 22:11:58', 'co2_ppm': 1640.3461853064605, 'no2_ppm': 23.035725524629328, 'so2_ppm': 4.41354630135026, 'temperature_c': 23.773315430956075, 'humidity_percent': 89.45566793553071, 'aqi': 13, 'environmental_score': 94.62673126455051, 'social_score': 83.45334240748326, 'governance_score': 72.17051545966598}\n",
      "Sent: {'timestamp': '2025-03-22 22:26:37', 'co2_ppm': 2100.092933902895, 'no2_ppm': 18.13350061886718, 'so2_ppm': 3.1976713596696804, 'temperature_c': 12.70811754220308, 'humidity_percent': 60.09627582341115, 'aqi': 10, 'environmental_score': 98.5681277847027, 'social_score': 76.5554513853086, 'governance_score': 63.37560676284838}\n",
      "Sent: {'timestamp': '2025-03-22 22:18:14', 'co2_ppm': 2537.3614674353003, 'no2_ppm': 2.345151794327292, 'so2_ppm': 2.63466442629301, 'temperature_c': 33.200422741259125, 'humidity_percent': 86.30024420929291, 'aqi': 2, 'environmental_score': 72.90682495472659, 'social_score': 66.99238436375857, 'governance_score': 76.81591095737329}\n",
      "Sent: {'timestamp': '2025-03-22 22:16:11', 'co2_ppm': 3867.47080067656, 'no2_ppm': 8.555912643910636, 'so2_ppm': 11.005787985128537, 'temperature_c': 11.578422257612882, 'humidity_percent': 73.83803646174692, 'aqi': 9, 'environmental_score': 82.27683219811644, 'social_score': 50.016493233556005, 'governance_score': 97.21836529839608}\n",
      "Sent: {'timestamp': '2025-03-22 22:14:22', 'co2_ppm': 3217.4103294269657, 'no2_ppm': 3.6528773086635136, 'so2_ppm': 4.229942195467369, 'temperature_c': 21.69477795861393, 'humidity_percent': 53.65233749826836, 'aqi': 3, 'environmental_score': 79.43181536104962, 'social_score': 98.72392400494806, 'governance_score': 87.22070188873604}\n",
      "Sent: {'timestamp': '2025-03-22 22:14:27', 'co2_ppm': 1130.4288044080688, 'no2_ppm': 10.996926725025352, 'so2_ppm': 10.602011446657873, 'temperature_c': 35.858203390725336, 'humidity_percent': 85.28038435245902, 'aqi': 10, 'environmental_score': 82.53837013205245, 'social_score': 68.71960873821901, 'governance_score': 86.45574786446467}\n",
      "Sent: {'timestamp': '2025-03-22 22:23:30', 'co2_ppm': 1158.9533216614968, 'no2_ppm': 3.4976533914247536, 'so2_ppm': 3.134749942757595, 'temperature_c': 24.03259339786868, 'humidity_percent': 53.00221935340616, 'aqi': 3, 'environmental_score': 57.797729669060594, 'social_score': 95.55231262812045, 'governance_score': 68.21892292281575}\n",
      "Sent: {'timestamp': '2025-03-22 22:21:25', 'co2_ppm': 2888.806576701993, 'no2_ppm': 9.41062610647624, 'so2_ppm': 2.340316883723433, 'temperature_c': 39.69582707208786, 'humidity_percent': 50.74244117132942, 'aqi': 5, 'environmental_score': 62.71592444999859, 'social_score': 99.79910432521456, 'governance_score': 55.09878767321901}\n",
      "Sent: {'timestamp': '2025-03-22 22:18:22', 'co2_ppm': 1469.700464582142, 'no2_ppm': 6.998960569302846, 'so2_ppm': 5.3996604841058735, 'temperature_c': 24.891406852139006, 'humidity_percent': 38.61748378818767, 'aqi': 6, 'environmental_score': 70.61954223430041, 'social_score': 83.9093315085031, 'governance_score': 74.663316266865}\n",
      "Sent: {'timestamp': '2025-03-22 22:21:18', 'co2_ppm': 3241.768961599017, 'no2_ppm': 21.510056815137457, 'so2_ppm': 11.47199689317234, 'temperature_c': 22.07793734913505, 'humidity_percent': 38.82408975504683, 'aqi': 16, 'environmental_score': 97.10878771315151, 'social_score': 80.88676269636258, 'governance_score': 61.93326508582384}\n",
      "Sent: {'timestamp': '2025-03-22 22:16:25', 'co2_ppm': 2300.6075758836905, 'no2_ppm': 11.587260349507911, 'so2_ppm': 14.138026302286868, 'temperature_c': 28.121265131928965, 'humidity_percent': 64.59070981799765, 'aqi': 12, 'environmental_score': 71.04571866020237, 'social_score': 59.00598339596267, 'governance_score': 53.376110280166095}\n",
      "Sent: {'timestamp': '2025-03-22 22:18:54', 'co2_ppm': 3656.5477235536578, 'no2_ppm': 12.25731102940781, 'so2_ppm': 3.7212387624817858, 'temperature_c': 34.14030749427771, 'humidity_percent': 32.683159375633444, 'aqi': 7, 'environmental_score': 57.15341326725922, 'social_score': 64.07983305528441, 'governance_score': 72.57691295392567}\n",
      "Sent: {'timestamp': '2025-03-22 22:24:13', 'co2_ppm': 700.8033220462263, 'no2_ppm': 1.8107247350760045, 'so2_ppm': 1.915277655777091, 'temperature_c': 38.295358766563794, 'humidity_percent': 52.68983886551512, 'aqi': 1, 'environmental_score': 66.46689936115229, 'social_score': 53.24144471207936, 'governance_score': 54.83506556037907}\n",
      "Sent: {'timestamp': '2025-03-22 22:12:25', 'co2_ppm': 879.5985275185315, 'no2_ppm': 7.630607938704813, 'so2_ppm': 9.093459323208483, 'temperature_c': 35.60918892180297, 'humidity_percent': 57.99871216439406, 'aqi': 8, 'environmental_score': 53.85429591100364, 'social_score': 56.14628572903164, 'governance_score': 85.35179472866419}\n",
      "Sent: {'timestamp': '2025-03-22 22:16:03', 'co2_ppm': 2195.2058798449643, 'no2_ppm': 5.030875988960574, 'so2_ppm': 7.480346622398354, 'temperature_c': 34.25353009053885, 'humidity_percent': 89.39261631268353, 'aqi': 6, 'environmental_score': 81.4818341167795, 'social_score': 77.67155618125074, 'governance_score': 67.57150649507301}\n",
      "Sent: {'timestamp': '2025-03-22 22:16:13', 'co2_ppm': 987.9886349324491, 'no2_ppm': 3.9783486429792143, 'so2_ppm': 1.0628909606646268, 'temperature_c': 24.115916644899272, 'humidity_percent': 89.62053775874689, 'aqi': 2, 'environmental_score': 51.79521126070011, 'social_score': 72.54267727870638, 'governance_score': 55.82195867474729}\n",
      "Sent: {'timestamp': '2025-03-22 22:25:50', 'co2_ppm': 2680.967100706003, 'no2_ppm': 6.099467911970452, 'so2_ppm': 3.1421981337079203, 'temperature_c': 29.376057851304264, 'humidity_percent': 52.93362039910232, 'aqi': 4, 'environmental_score': 59.85314595985777, 'social_score': 97.45125876243507, 'governance_score': 53.52504287523575}\n",
      "Sent: {'timestamp': '2025-03-22 22:22:37', 'co2_ppm': 2771.960162366613, 'no2_ppm': 10.28443263398265, 'so2_ppm': 5.798607448192272, 'temperature_c': 15.808668746884422, 'humidity_percent': 61.55676216671964, 'aqi': 8, 'environmental_score': 59.99456065534788, 'social_score': 52.33284241586853, 'governance_score': 66.23818959189943}\n",
      "Sent: {'timestamp': '2025-03-22 22:25:56', 'co2_ppm': 2764.5305040735125, 'no2_ppm': 7.257175144982166, 'so2_ppm': 2.165677967970779, 'temperature_c': 14.286614434614291, 'humidity_percent': 84.93960209047262, 'aqi': 4, 'environmental_score': 66.75165939989861, 'social_score': 83.82416026127494, 'governance_score': 78.35637719062895}\n",
      "Sent: {'timestamp': '2025-03-22 22:23:57', 'co2_ppm': 4997.729786817071, 'no2_ppm': 17.78782725470481, 'so2_ppm': 3.42441648546226, 'temperature_c': 27.54568682337594, 'humidity_percent': 82.44583764250169, 'aqi': 10, 'environmental_score': 53.99440936119924, 'social_score': 85.23403118175554, 'governance_score': 90.8560774482546}\n",
      "Sent: {'timestamp': '2025-03-22 22:21:04', 'co2_ppm': 752.6403780240236, 'no2_ppm': 19.86371860624485, 'so2_ppm': 7.060315369732631, 'temperature_c': 37.50550466599645, 'humidity_percent': 75.441883425655, 'aqi': 13, 'environmental_score': 53.68274964916477, 'social_score': 91.25565279647734, 'governance_score': 86.43958084952413}\n",
      "Sent: {'timestamp': '2025-03-22 22:13:27', 'co2_ppm': 3416.535064025459, 'no2_ppm': 6.843020578285749, 'so2_ppm': 4.374997059680082, 'temperature_c': 16.11998678765758, 'humidity_percent': 79.57723662860234, 'aqi': 5, 'environmental_score': 64.17262802510928, 'social_score': 65.91261162211887, 'governance_score': 73.25045454791119}\n",
      "Sent: {'timestamp': '2025-03-22 22:13:41', 'co2_ppm': 1419.971126503592, 'no2_ppm': 4.63941033510427, 'so2_ppm': 2.38222024008732, 'temperature_c': 10.696311819172056, 'humidity_percent': 52.00724685207947, 'aqi': 3, 'environmental_score': 52.73575810209378, 'social_score': 69.79049933376778, 'governance_score': 64.69397103006757}\n",
      "Sent: {'timestamp': '2025-03-22 22:16:56', 'co2_ppm': 1756.6917413815352, 'no2_ppm': 4.525153296932976, 'so2_ppm': 11.380316815900596, 'temperature_c': 28.68086438975942, 'humidity_percent': 89.98146316865105, 'aqi': 7, 'environmental_score': 81.83169677788645, 'social_score': 96.84450522149596, 'governance_score': 68.53201626041677}\n",
      "Sent: {'timestamp': '2025-03-22 22:15:40', 'co2_ppm': 4615.234712618988, 'no2_ppm': 13.42341944119264, 'so2_ppm': 6.361233633411296, 'temperature_c': 35.10634611335213, 'humidity_percent': 33.512638528678124, 'aqi': 9, 'environmental_score': 69.50030754725671, 'social_score': 78.42215992233157, 'governance_score': 98.44447309886304}\n",
      "Sent: {'timestamp': '2025-03-22 22:13:36', 'co2_ppm': 1217.894170954606, 'no2_ppm': 2.120981296250302, 'so2_ppm': 3.0261831707346643, 'temperature_c': 26.501814197089697, 'humidity_percent': 52.860003450522655, 'aqi': 2, 'environmental_score': 63.66638534630484, 'social_score': 53.19022896434688, 'governance_score': 83.335062347737}\n",
      "Sent: {'timestamp': '2025-03-22 22:20:21', 'co2_ppm': 2723.038247725867, 'no2_ppm': 12.139416561740992, 'so2_ppm': 3.3377039138832543, 'temperature_c': 15.05888289429296, 'humidity_percent': 78.76363070945851, 'aqi': 7, 'environmental_score': 98.50033269321456, 'social_score': 81.58683305309964, 'governance_score': 94.55300382610062}\n",
      "Sent: {'timestamp': '2025-03-22 22:11:26', 'co2_ppm': 1022.7232418131672, 'no2_ppm': 12.764150813062844, 'so2_ppm': 5.137615286794107, 'temperature_c': 31.227235102879376, 'humidity_percent': 68.05193012898253, 'aqi': 8, 'environmental_score': 93.3005929942912, 'social_score': 56.56211068000498, 'governance_score': 77.20170949045863}\n"
     ]
    }
   ],
   "source": [
    "from kafka import KafkaProducer\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Initialize Kafka Producer\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=['localhost:9092'],\n",
    "    value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    ")\n",
    "\n",
    "# Function to process CSV and prepare data\n",
    "def process_csv(file_path):\n",
    "    try:\n",
    "        # Read CSV\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Ensure required columns are present\n",
    "        required_columns = [\n",
    "            'Timestamp', 'CO2_ppm', 'NO2_ppm', 'SO2_ppm', \n",
    "            'Temperature_C', 'Humidity_%', 'AQI', \n",
    "            'Environmental_Score', 'Social_Score', 'Governance_Score'\n",
    "        ]\n",
    "        if not all(col in df.columns for col in required_columns):\n",
    "            raise ValueError(f\"CSV file must contain the following columns: {required_columns}\")\n",
    "        \n",
    "        # Convert Timestamp to string if not already\n",
    "        df['Timestamp'] = df['Timestamp'].astype(str)\n",
    "        \n",
    "        # Yield each row as a dictionary\n",
    "        for _, row in df.iterrows():\n",
    "            yield {\n",
    "                \"timestamp\": row['Timestamp'],\n",
    "                \"co2_ppm\": float(row['CO2_ppm']),\n",
    "                \"no2_ppm\": float(row['NO2_ppm']),\n",
    "                \"so2_ppm\": float(row['SO2_ppm']),\n",
    "                \"temperature_c\": float(row['Temperature_C']),\n",
    "                \"humidity_percent\": float(row['Humidity_%']),\n",
    "                \"aqi\": int(row['AQI']),\n",
    "                \"environmental_score\": float(row['Environmental_Score']),\n",
    "                \"social_score\": float(row['Social_Score']),\n",
    "                \"governance_score\": float(row['Governance_Score'])\n",
    "            }\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing CSV: {e}\")\n",
    "        return []\n",
    "\n",
    "# Main block to send data to Kafka topic\n",
    "def send_to_kafka(file_path):\n",
    "    try:\n",
    "        for data in process_csv(file_path):\n",
    "            producer.send('environmental_data', data)\n",
    "            print(f\"Sent: {data}\")\n",
    "            time.sleep(0.5)  # Throttle to avoid overwhelming Kafka\n",
    "    except Exception as e:\n",
    "        print(f\"Error sending data: {e}\")\n",
    "    finally:\n",
    "        producer.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = \"/Users/kdn_aikothalavanya/Desktop/KPMG Projects/carbonfootprint/real_time_monitoring_data.csv\"\n",
    "    send_to_kafka(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading and sending data to Kafka...\n",
      "Sent: {'timestamp': '2025-03-22 22:18:49', 'co2_ppm': 548.606014687361, 'no2_ppm': 9.916281225491558, 'so2_ppm': 1.0403628061379906, 'temperature_c': 11.633600152610738, 'humidity_percent': 25.74550592759281, 'aqi': 5, 'environmental_score': 72.6752611103381, 'social_score': 68.55838175232356, 'governance_score': 73.97769473652139}\n",
      "Sent: {'timestamp': '2025-03-22 22:22:25', 'co2_ppm': 4042.507826442367, 'no2_ppm': 3.8811434858951226, 'so2_ppm': 2.17174804405351, 'temperature_c': 10.892209271710792, 'humidity_percent': 21.947806541725253, 'aqi': 3, 'environmental_score': 56.73257793373792, 'social_score': 85.43385040728906, 'governance_score': 83.41359404561356}\n",
      "Sent: {'timestamp': '2025-03-22 22:22:28', 'co2_ppm': 2113.433202346877, 'no2_ppm': 9.487869562551154, 'so2_ppm': 13.260369291039805, 'temperature_c': 14.98161692420653, 'humidity_percent': 70.22854343990032, 'aqi': 11, 'environmental_score': 86.87958824345276, 'social_score': 93.98969293653772, 'governance_score': 94.10240900574682}\n",
      "Sent: {'timestamp': '2025-03-22 22:21:08', 'co2_ppm': 2649.863336489712, 'no2_ppm': 4.616146593645984, 'so2_ppm': 2.050842029037262, 'temperature_c': 18.138708917022782, 'humidity_percent': 45.753685494787945, 'aqi': 3, 'environmental_score': 72.39262282389235, 'social_score': 78.44983268985763, 'governance_score': 74.06574263137972}\n",
      "Sent: {'timestamp': '2025-03-22 22:16:59', 'co2_ppm': 2559.726884328139, 'no2_ppm': 5.867326496727233, 'so2_ppm': 3.410072842590965, 'temperature_c': 32.62479967398987, 'humidity_percent': 56.04941684861812, 'aqi': 4, 'environmental_score': 62.40374419068303, 'social_score': 84.02592185028125, 'governance_score': 91.8539416551973}\n",
      "Sent: {'timestamp': '2025-03-22 22:21:26', 'co2_ppm': 2027.5330877674053, 'no2_ppm': 13.5044524223126, 'so2_ppm': 6.721068359769908, 'temperature_c': 38.80074902707496, 'humidity_percent': 77.96417293344713, 'aqi': 10, 'environmental_score': 87.16541749805435, 'social_score': 68.01503002804736, 'governance_score': 50.04443381291835}\n",
      "Sent: {'timestamp': '2025-03-22 22:20:34', 'co2_ppm': 1336.441205875631, 'no2_ppm': 8.464969799368124, 'so2_ppm': 5.035778658749465, 'temperature_c': 30.63781040003608, 'humidity_percent': 20.818329119670032, 'aqi': 6, 'environmental_score': 69.04404292215901, 'social_score': 51.84270332744079, 'governance_score': 81.34986794644207}\n",
      "Sent: {'timestamp': '2025-03-22 22:18:41', 'co2_ppm': 4735.861831418409, 'no2_ppm': 9.003863322561438, 'so2_ppm': 4.036257382132527, 'temperature_c': 14.863558141416858, 'humidity_percent': 34.70056051297634, 'aqi': 6, 'environmental_score': 84.47091454180071, 'social_score': 91.96892376552007, 'governance_score': 67.96189732813859}\n",
      "Sent: {'timestamp': '2025-03-22 22:24:27', 'co2_ppm': 1061.0548352341943, 'no2_ppm': 19.09297677464767, 'so2_ppm': 10.73073030220898, 'temperature_c': 32.23754840754769, 'humidity_percent': 31.650533186795048, 'aqi': 14, 'environmental_score': 74.91987781077401, 'social_score': 53.83690000249132, 'governance_score': 69.13119199420395}\n",
      "Sent: {'timestamp': '2025-03-22 22:12:24', 'co2_ppm': 3515.755886876567, 'no2_ppm': 17.78790227661813, 'so2_ppm': 9.854142754383536, 'temperature_c': 36.33346656544849, 'humidity_percent': 28.437545260793232, 'aqi': 13, 'environmental_score': 51.9323258973441, 'social_score': 70.77189454065883, 'governance_score': 50.30702143048877}\n",
      "Sent: {'timestamp': '2025-03-22 22:23:39', 'co2_ppm': 3168.729086430389, 'no2_ppm': 6.6338451419460345, 'so2_ppm': 1.9552050572124569, 'temperature_c': 23.42308368570681, 'humidity_percent': 81.4515276192987, 'aqi': 4, 'environmental_score': 76.26375786138703, 'social_score': 88.54417251885371, 'governance_score': 98.01603423142744}\n",
      "Sent: {'timestamp': '2025-03-22 22:11:24', 'co2_ppm': 3275.6337673850408, 'no2_ppm': 4.734779217557265, 'so2_ppm': 4.140975424054764, 'temperature_c': 20.875499722883816, 'humidity_percent': 46.0274465380766, 'aqi': 4, 'environmental_score': 95.68744513908234, 'social_score': 87.97949949642594, 'governance_score': 77.34436559836647}\n",
      "Sent: {'timestamp': '2025-03-22 22:20:44', 'co2_ppm': 2589.5836487497504, 'no2_ppm': 14.926000825774343, 'so2_ppm': 9.365224630584557, 'temperature_c': 38.0129191813909, 'humidity_percent': 24.20879435068499, 'aqi': 12, 'environmental_score': 90.43700211649586, 'social_score': 87.25032013006428, 'governance_score': 59.37398274001}\n",
      "Sent: {'timestamp': '2025-03-22 22:26:21', 'co2_ppm': 745.5236311250394, 'no2_ppm': 5.145655989540688, 'so2_ppm': 3.2729154497152524, 'temperature_c': 10.130670807547602, 'humidity_percent': 79.46559428192379, 'aqi': 4, 'environmental_score': 73.56510972425059, 'social_score': 71.5326205273287, 'governance_score': 91.08162286618048}\n",
      "Sent: {'timestamp': '2025-03-22 22:25:46', 'co2_ppm': 3816.774187627254, 'no2_ppm': 5.08789923090447, 'so2_ppm': 4.435616661705975, 'temperature_c': 15.87314456815449, 'humidity_percent': 34.17447115179564, 'aqi': 4, 'environmental_score': 71.67051745510328, 'social_score': 57.32579050734851, 'governance_score': 73.77484055867585}\n",
      "Sent: {'timestamp': '2025-03-22 22:25:07', 'co2_ppm': 4245.73566072826, 'no2_ppm': 9.721594636423896, 'so2_ppm': 3.1723634196223727, 'temperature_c': 10.272307365198277, 'humidity_percent': 78.49670598002433, 'aqi': 6, 'environmental_score': 75.34243130328437, 'social_score': 81.25808165736223, 'governance_score': 98.09470638931468}\n",
      "Sent: {'timestamp': '2025-03-22 22:14:48', 'co2_ppm': 4044.569920227213, 'no2_ppm': 6.820979164668195, 'so2_ppm': 5.513080998878947, 'temperature_c': 27.040725567321783, 'humidity_percent': 79.98303998574269, 'aqi': 6, 'environmental_score': 95.5508120140446, 'social_score': 70.93050763408658, 'governance_score': 57.0198320758121}\n",
      "Sent: {'timestamp': '2025-03-22 22:24:51', 'co2_ppm': 1853.442169887888, 'no2_ppm': 9.598852598756936, 'so2_ppm': 3.753283979477385, 'temperature_c': 22.41442080764194, 'humidity_percent': 62.49190339291064, 'aqi': 6, 'environmental_score': 52.17906167288271, 'social_score': 54.66696152320097, 'governance_score': 87.05771597575347}\n",
      "Sent: {'timestamp': '2025-03-22 22:22:49', 'co2_ppm': 1253.39514484887, 'no2_ppm': 8.357440246853777, 'so2_ppm': 4.053422115248499, 'temperature_c': 34.02441779819715, 'humidity_percent': 52.06236600489416, 'aqi': 6, 'environmental_score': 52.1067252998978, 'social_score': 87.89526148381779, 'governance_score': 96.28828346310944}\n",
      "Sent: {'timestamp': '2025-03-22 22:15:27', 'co2_ppm': 1465.483053007763, 'no2_ppm': 10.822303314742117, 'so2_ppm': 5.303325184517293, 'temperature_c': 12.501205173863797, 'humidity_percent': 42.94018331652179, 'aqi': 8, 'environmental_score': 80.02347469632858, 'social_score': 90.1066738503052, 'governance_score': 70.56348898392494}\n",
      "Sent: {'timestamp': '2025-03-22 22:19:17', 'co2_ppm': 1109.2816901036745, 'no2_ppm': 7.95885643693517, 'so2_ppm': 6.880111694299692, 'temperature_c': 12.949550073702785, 'humidity_percent': 50.20490285201178, 'aqi': 7, 'environmental_score': 82.1485058217497, 'social_score': 66.66451593634136, 'governance_score': 79.44013498974384}\n",
      "Sent: {'timestamp': '2025-03-22 22:13:02', 'co2_ppm': 4623.6829363574925, 'no2_ppm': 13.109901679583572, 'so2_ppm': 3.5523832304120835, 'temperature_c': 31.92816866242679, 'humidity_percent': 88.76418883284678, 'aqi': 8, 'environmental_score': 96.64277352801508, 'social_score': 83.22668143365641, 'governance_score': 62.74547250402316}\n",
      "Sent: {'timestamp': '2025-03-22 22:16:19', 'co2_ppm': 2493.618202287258, 'no2_ppm': 6.004320013943711, 'so2_ppm': 1.906158869523713, 'temperature_c': 19.58780346037166, 'humidity_percent': 53.00059761899008, 'aqi': 3, 'environmental_score': 84.75604881240679, 'social_score': 93.77741417751346, 'governance_score': 81.6893330969801}\n",
      "Sent: {'timestamp': '2025-03-22 22:18:55', 'co2_ppm': 4268.721490788724, 'no2_ppm': 4.032018459323104, 'so2_ppm': 8.633604227573779, 'temperature_c': 26.112791996453463, 'humidity_percent': 71.74065391594394, 'aqi': 6, 'environmental_score': 92.08299038551708, 'social_score': 95.15422702056644, 'governance_score': 70.05836642096698}\n",
      "Sent: {'timestamp': '2025-03-22 22:13:38', 'co2_ppm': 3921.3742981765904, 'no2_ppm': 12.8945850898425, 'so2_ppm': 5.937372031798522, 'temperature_c': 31.32759745299034, 'humidity_percent': 78.65293405471206, 'aqi': 9, 'environmental_score': 72.91505709391404, 'social_score': 64.4551145269514, 'governance_score': 62.07067598381437}\n",
      "Sent: {'timestamp': '2025-03-22 22:14:03', 'co2_ppm': 4674.235566698722, 'no2_ppm': 8.924913097727739, 'so2_ppm': 6.276658506467278, 'temperature_c': 25.99056126020263, 'humidity_percent': 22.55788162231832, 'aqi': 7, 'environmental_score': 75.04232537653228, 'social_score': 77.07232563400294, 'governance_score': 93.29430200444348}\n",
      "Sent: {'timestamp': '2025-03-22 22:11:49', 'co2_ppm': 1052.5235335345535, 'no2_ppm': 6.178212432684412, 'so2_ppm': 7.052416991488138, 'temperature_c': 16.687966831340795, 'humidity_percent': 83.0937816490072, 'aqi': 6, 'environmental_score': 70.61623862365053, 'social_score': 82.19695452710735, 'governance_score': 85.07946773450142}\n",
      "Sent: {'timestamp': '2025-03-22 22:26:34', 'co2_ppm': 2829.700678350686, 'no2_ppm': 8.162528024165454, 'so2_ppm': 1.5647762173944144, 'temperature_c': 16.683117333447697, 'humidity_percent': 52.57283778411298, 'aqi': 4, 'environmental_score': 92.51439909602294, 'social_score': 96.56570009718764, 'governance_score': 96.40480160430658}\n",
      "Sent: {'timestamp': '2025-03-22 22:25:10', 'co2_ppm': 4316.838682798598, 'no2_ppm': 13.281423077766544, 'so2_ppm': 6.092021264267391, 'temperature_c': 10.419342173552746, 'humidity_percent': 50.70212783384598, 'aqi': 9, 'environmental_score': 88.19569901350184, 'social_score': 87.74753177600414, 'governance_score': 93.57764587303518}\n",
      "Sent: {'timestamp': '2025-03-22 22:25:55', 'co2_ppm': 2278.005871684849, 'no2_ppm': 7.664965891858906, 'so2_ppm': 1.2757445452695646, 'temperature_c': 30.48515833784541, 'humidity_percent': 63.45074953096064, 'aqi': 4, 'environmental_score': 96.43500209414208, 'social_score': 91.67794116943412, 'governance_score': 81.50310218256267}\n",
      "Sent: {'timestamp': '2025-03-22 22:15:54', 'co2_ppm': 3257.735115886926, 'no2_ppm': 12.417177357546166, 'so2_ppm': 5.916191692810107, 'temperature_c': 21.149836408246767, 'humidity_percent': 22.31973173539335, 'aqi': 9, 'environmental_score': 63.58746315296243, 'social_score': 73.95622609275559, 'governance_score': 70.81185769099666}\n",
      "Sent: {'timestamp': '2025-03-22 22:12:27', 'co2_ppm': 925.78189417899, 'no2_ppm': 16.956634034649714, 'so2_ppm': 11.004156382313823, 'temperature_c': 14.785884887676476, 'humidity_percent': 82.37723311972742, 'aqi': 13, 'environmental_score': 85.0588890477647, 'social_score': 91.63127545343472, 'governance_score': 74.71925903771553}\n",
      "Sent: {'timestamp': '2025-03-22 22:20:29', 'co2_ppm': 4405.373395953118, 'no2_ppm': 10.148556210261756, 'so2_ppm': 2.5774124817879027, 'temperature_c': 29.58049942492876, 'humidity_percent': 59.77284337355183, 'aqi': 6, 'environmental_score': 70.35194581147648, 'social_score': 94.38020805575806, 'governance_score': 56.49304418476621}\n",
      "Sent: {'timestamp': '2025-03-22 22:18:27', 'co2_ppm': 2684.828543241604, 'no2_ppm': 12.047669233948607, 'so2_ppm': 7.991835399795396, 'temperature_c': 16.771536649134212, 'humidity_percent': 69.64554389851699, 'aqi': 10, 'environmental_score': 77.503800719216, 'social_score': 93.06044602881632, 'governance_score': 72.54079047453351}\n",
      "Sent: {'timestamp': '2025-03-22 22:16:18', 'co2_ppm': 987.2733509251548, 'no2_ppm': 12.470050902871746, 'so2_ppm': 6.663662969681787, 'temperature_c': 26.31153678558468, 'humidity_percent': 85.80592654105196, 'aqi': 9, 'environmental_score': 63.58715050108678, 'social_score': 68.89112882023747, 'governance_score': 55.46598100857622}\n",
      "Sent: {'timestamp': '2025-03-22 22:13:26', 'co2_ppm': 2708.7896452190607, 'no2_ppm': 16.586214687159135, 'so2_ppm': 12.848040118793586, 'temperature_c': 38.03710923018734, 'humidity_percent': 84.18904081942172, 'aqi': 14, 'environmental_score': 78.99786872381162, 'social_score': 78.37813907153276, 'governance_score': 91.8800134460588}\n",
      "Sent: {'timestamp': '2025-03-22 22:21:35', 'co2_ppm': 3931.588332164564, 'no2_ppm': 12.009043530619351, 'so2_ppm': 5.322905266174805, 'temperature_c': 19.9354090728284, 'humidity_percent': 77.08777010397816, 'aqi': 8, 'environmental_score': 82.861030589384, 'social_score': 55.66249525196205, 'governance_score': 76.0521410572428}\n",
      "Sent: {'timestamp': '2025-03-22 22:19:52', 'co2_ppm': 3890.38895907256, 'no2_ppm': 11.9336730119958, 'so2_ppm': 3.620211263169094, 'temperature_c': 18.246514801525034, 'humidity_percent': 53.72228765414697, 'aqi': 7, 'environmental_score': 86.58437304157648, 'social_score': 94.55425390042204, 'governance_score': 62.073419172935125}\n",
      "Sent: {'timestamp': '2025-03-22 22:25:09', 'co2_ppm': 3905.705052077592, 'no2_ppm': 13.253131770003472, 'so2_ppm': 12.80253067213212, 'temperature_c': 20.971422147313422, 'humidity_percent': 61.52501351530764, 'aqi': 13, 'environmental_score': 82.45035196361917, 'social_score': 55.03223263762764, 'governance_score': 87.75719882107296}\n",
      "Sent: {'timestamp': '2025-03-22 22:11:07', 'co2_ppm': 4129.512752877859, 'no2_ppm': 3.600722158524469, 'so2_ppm': 2.0648402504253283, 'temperature_c': 30.956225555523552, 'humidity_percent': 85.85160685059832, 'aqi': 2, 'environmental_score': 92.92723108953808, 'social_score': 60.167684393542615, 'governance_score': 50.6518074835429}\n",
      "Sent: {'timestamp': '2025-03-22 22:15:36', 'co2_ppm': 3731.707280430706, 'no2_ppm': 3.4758656044974785, 'so2_ppm': 2.8397180426763025, 'temperature_c': 30.79515180337707, 'humidity_percent': 21.46292781145407, 'aqi': 3, 'environmental_score': 71.48355127201967, 'social_score': 88.01519817346252, 'governance_score': 52.15172836291796}\n",
      "Sent: {'timestamp': '2025-03-22 22:11:44', 'co2_ppm': 1987.4896709533296, 'no2_ppm': 5.19993137022564, 'so2_ppm': 1.6612117954727883, 'temperature_c': 29.75896258145695, 'humidity_percent': 27.53355370974265, 'aqi': 3, 'environmental_score': 84.86843821259828, 'social_score': 62.91331878360594, 'governance_score': 92.44389244356933}\n",
      "Sent: {'timestamp': '2025-03-22 22:15:20', 'co2_ppm': 4052.31381884502, 'no2_ppm': 15.805942291674654, 'so2_ppm': 4.332842000159152, 'temperature_c': 22.12636973626952, 'humidity_percent': 60.88615645485791, 'aqi': 10, 'environmental_score': 92.51343305506644, 'social_score': 57.9607106525307, 'governance_score': 93.03267642404396}\n",
      "Sent: {'timestamp': '2025-03-22 22:26:22', 'co2_ppm': 779.3593430886858, 'no2_ppm': 3.494456145412804, 'so2_ppm': 4.135243988707242, 'temperature_c': 16.697526839286045, 'humidity_percent': 48.94052292439197, 'aqi': 3, 'environmental_score': 58.40931986757351, 'social_score': 57.81259464023636, 'governance_score': 60.62473819508966}\n",
      "Sent: {'timestamp': '2025-03-22 22:15:07', 'co2_ppm': 4837.426597473891, 'no2_ppm': 5.579359727629551, 'so2_ppm': 5.30047931825437, 'temperature_c': 19.02205266474865, 'humidity_percent': 70.02651043515692, 'aqi': 5, 'environmental_score': 53.498143112136816, 'social_score': 80.55279004655671, 'governance_score': 69.13703177388871}\n",
      "Sent: {'timestamp': '2025-03-22 22:14:45', 'co2_ppm': 3918.220012829032, 'no2_ppm': 21.40616651634604, 'so2_ppm': 2.354672264088481, 'temperature_c': 18.63797619589348, 'humidity_percent': 66.28278043804747, 'aqi': 11, 'environmental_score': 83.77687379234263, 'social_score': 84.92853878442989, 'governance_score': 76.30804358626804}\n",
      "Sent: {'timestamp': '2025-03-22 22:25:32', 'co2_ppm': 4200.514771562773, 'no2_ppm': 7.340132699095748, 'so2_ppm': 9.241946889485806, 'temperature_c': 24.98861943051132, 'humidity_percent': 28.66904313611642, 'aqi': 8, 'environmental_score': 65.45170908469294, 'social_score': 89.19477411155216, 'governance_score': 81.95089030354636}\n",
      "Sent: {'timestamp': '2025-03-22 22:20:09', 'co2_ppm': 2973.022573266912, 'no2_ppm': 13.16281051215469, 'so2_ppm': 1.7388834882154758, 'temperature_c': 30.930153734999585, 'humidity_percent': 25.00953835732298, 'aqi': 7, 'environmental_score': 63.77743230926278, 'social_score': 84.36874749449004, 'governance_score': 56.88186089148554}\n",
      "Sent: {'timestamp': '2025-03-22 22:17:27', 'co2_ppm': 4550.964645319837, 'no2_ppm': 15.460774092978484, 'so2_ppm': 4.254818313898477, 'temperature_c': 38.288681189042514, 'humidity_percent': 87.26560111683145, 'aqi': 9, 'environmental_score': 73.34272402181463, 'social_score': 59.412041103253365, 'governance_score': 82.72007523665641}\n",
      "Sent: {'timestamp': '2025-03-22 22:24:05', 'co2_ppm': 4196.329459765671, 'no2_ppm': 18.718593473348328, 'so2_ppm': 11.02915029503667, 'temperature_c': 20.379381524334025, 'humidity_percent': 49.13633263299157, 'aqi': 14, 'environmental_score': 67.59420298054556, 'social_score': 65.49391907748834, 'governance_score': 80.64927848571945}\n",
      "Sent: {'timestamp': '2025-03-22 22:19:38', 'co2_ppm': 3134.5138725199013, 'no2_ppm': 16.374199695381353, 'so2_ppm': 8.207056914349904, 'temperature_c': 20.29215762893225, 'humidity_percent': 62.00327414670791, 'aqi': 12, 'environmental_score': 81.59236298801275, 'social_score': 82.0470640389409, 'governance_score': 68.94718638556995}\n",
      "Sent: {'timestamp': '2025-03-22 22:24:47', 'co2_ppm': 4050.3188119927, 'no2_ppm': 10.672537574069688, 'so2_ppm': 9.431944599647728, 'temperature_c': 10.82918174048399, 'humidity_percent': 85.90602580058533, 'aqi': 10, 'environmental_score': 76.68482376717301, 'social_score': 89.28259011595561, 'governance_score': 90.29924156155955}\n",
      "Sent: {'timestamp': '2025-03-22 22:14:29', 'co2_ppm': 2825.0031308110138, 'no2_ppm': 11.57627502243503, 'so2_ppm': 6.133292604947864, 'temperature_c': 37.1976985572902, 'humidity_percent': 54.7503695749942, 'aqi': 8, 'environmental_score': 81.08600542790926, 'social_score': 78.38563519549122, 'governance_score': 50.15710045359532}\n",
      "Sent: {'timestamp': '2025-03-22 22:19:46', 'co2_ppm': 1680.7213283183387, 'no2_ppm': 20.41143460571074, 'so2_ppm': 9.008268225160071, 'temperature_c': 17.60790297948918, 'humidity_percent': 59.39921549435967, 'aqi': 14, 'environmental_score': 70.78202246162583, 'social_score': 50.83153327503525, 'governance_score': 71.06267149544998}\n",
      "Sent: {'timestamp': '2025-03-22 22:10:31', 'co2_ppm': 1518.7102086749674, 'no2_ppm': 2.574177023738026, 'so2_ppm': 2.239999108804385, 'temperature_c': 13.577895293970212, 'humidity_percent': 29.799051278618336, 'aqi': 2, 'environmental_score': 70.6951112000786, 'social_score': 50.71436562434273, 'governance_score': 83.02866154558518}\n",
      "Sent: {'timestamp': '2025-03-22 22:10:38', 'co2_ppm': 3224.1844541234304, 'no2_ppm': 17.65700767302462, 'so2_ppm': 6.544333818818026, 'temperature_c': 28.13957494663289, 'humidity_percent': 67.11857747167697, 'aqi': 12, 'environmental_score': 79.10441807757864, 'social_score': 60.40061163963901, 'governance_score': 85.74276642882447}\n",
      "Sent: {'timestamp': '2025-03-22 22:20:25', 'co2_ppm': 450.3740588190934, 'no2_ppm': 7.06216173541911, 'so2_ppm': 3.1123377678503426, 'temperature_c': 22.221611413000154, 'humidity_percent': 62.47512348045388, 'aqi': 5, 'environmental_score': 90.11616775162211, 'social_score': 71.22570165351279, 'governance_score': 60.46939677933972}\n",
      "Sent: {'timestamp': '2025-03-22 22:10:18', 'co2_ppm': 479.14386535522175, 'no2_ppm': 9.41290542030278, 'so2_ppm': 5.633766213813517, 'temperature_c': 39.31935349150929, 'humidity_percent': 53.98547992251512, 'aqi': 7, 'environmental_score': 50.76149371366434, 'social_score': 50.68317726877153, 'governance_score': 86.16929564208033}\n",
      "Sent: {'timestamp': '2025-03-22 22:13:49', 'co2_ppm': 3690.267424555923, 'no2_ppm': 12.924748279579187, 'so2_ppm': 6.775851697578562, 'temperature_c': 26.44890129756003, 'humidity_percent': 53.99872088164974, 'aqi': 9, 'environmental_score': 56.92495082216384, 'social_score': 87.03759236125916, 'governance_score': 50.30094063141563}\n",
      "Sent: {'timestamp': '2025-03-22 22:23:48', 'co2_ppm': 2012.8553320665744, 'no2_ppm': 5.929551494893745, 'so2_ppm': 2.479102407202745, 'temperature_c': 16.87032440153593, 'humidity_percent': 77.18594924656554, 'aqi': 4, 'environmental_score': 86.7452879533336, 'social_score': 88.11349613179857, 'governance_score': 87.85127175333537}\n",
      "Sent: {'timestamp': '2025-03-22 22:15:04', 'co2_ppm': 4414.730183660153, 'no2_ppm': 11.178162302362356, 'so2_ppm': 2.176976023224398, 'temperature_c': 37.71379434683787, 'humidity_percent': 74.9425617694223, 'aqi': 6, 'environmental_score': 95.82792278926615, 'social_score': 96.72321560686622, 'governance_score': 79.44336156983857}\n",
      "Sent: {'timestamp': '2025-03-22 22:11:02', 'co2_ppm': 3956.116823959151, 'no2_ppm': 6.0379089359786775, 'so2_ppm': 2.6601852140690645, 'temperature_c': 30.67716430110401, 'humidity_percent': 75.98072465978544, 'aqi': 4, 'environmental_score': 54.24002444952031, 'social_score': 50.88551866581004, 'governance_score': 73.03830769827468}\n",
      "Sent: {'timestamp': '2025-03-22 22:19:10', 'co2_ppm': 812.8884313806033, 'no2_ppm': 15.545745047224903, 'so2_ppm': 6.772821031845237, 'temperature_c': 18.48202630561409, 'humidity_percent': 59.75054711223947, 'aqi': 11, 'environmental_score': 91.62266016545422, 'social_score': 54.22241489434242, 'governance_score': 91.25781400988416}\n",
      "Sent: {'timestamp': '2025-03-22 22:13:37', 'co2_ppm': 4835.077390194306, 'no2_ppm': 3.8029924567848754, 'so2_ppm': 7.149401277078544, 'temperature_c': 23.279163188184945, 'humidity_percent': 45.5879290018146, 'aqi': 5, 'environmental_score': 64.02039310091553, 'social_score': 92.81452780278202, 'governance_score': 74.99745380850051}\n",
      "Sent: {'timestamp': '2025-03-22 22:20:12', 'co2_ppm': 479.3440522688102, 'no2_ppm': 15.474817877109276, 'so2_ppm': 3.1207720794805383, 'temperature_c': 20.939199730149703, 'humidity_percent': 35.72888746440016, 'aqi': 9, 'environmental_score': 61.3832527446338, 'social_score': 93.83890026645857, 'governance_score': 64.76966214348353}\n",
      "Sent: {'timestamp': '2025-03-22 22:21:40', 'co2_ppm': 4354.315513868391, 'no2_ppm': 12.825375293431062, 'so2_ppm': 7.106934270264118, 'temperature_c': 23.794281477185244, 'humidity_percent': 25.42314034462069, 'aqi': 9, 'environmental_score': 53.92673732626649, 'social_score': 95.46562938464292, 'governance_score': 94.58205940796827}\n",
      "Sent: {'timestamp': '2025-03-22 22:21:10', 'co2_ppm': 523.2368408842307, 'no2_ppm': 3.4078489307954025, 'so2_ppm': 0.6963630929521877, 'temperature_c': 20.29500902198005, 'humidity_percent': 45.00955419234978, 'aqi': 2, 'environmental_score': 97.34450376688903, 'social_score': 89.92937919377312, 'governance_score': 51.44193222536369}\n",
      "Sent: {'timestamp': '2025-03-22 22:24:34', 'co2_ppm': 1616.406934209086, 'no2_ppm': 14.693946307323788, 'so2_ppm': 4.756479353187179, 'temperature_c': 18.939858121050428, 'humidity_percent': 67.25991158625101, 'aqi': 9, 'environmental_score': 75.99300294859077, 'social_score': 60.85646763720427, 'governance_score': 95.69011665717304}\n",
      "Sent: {'timestamp': '2025-03-22 22:11:18', 'co2_ppm': 3844.285472285419, 'no2_ppm': 11.27763134510952, 'so2_ppm': 9.02507737878515, 'temperature_c': 31.833912190738097, 'humidity_percent': 42.28069578419308, 'aqi': 10, 'environmental_score': 69.45622717795118, 'social_score': 72.1566088841886, 'governance_score': 59.68370241953698}\n",
      "Sent: {'timestamp': '2025-03-22 22:19:11', 'co2_ppm': 3183.440399621604, 'no2_ppm': 6.22184728269588, 'so2_ppm': 2.052079420177124, 'temperature_c': 10.87903499371278, 'humidity_percent': 52.32336725054131, 'aqi': 4, 'environmental_score': 64.39417632692614, 'social_score': 65.4886040860689, 'governance_score': 96.48816930007438}\n",
      "Sent: {'timestamp': '2025-03-22 22:11:55', 'co2_ppm': 1183.5189372692446, 'no2_ppm': 5.912422556238729, 'so2_ppm': 3.473746985512734, 'temperature_c': 28.529900622901263, 'humidity_percent': 28.50965337255668, 'aqi': 4, 'environmental_score': 79.11994899037832, 'social_score': 81.83589084116421, 'governance_score': 53.8435381864792}\n",
      "Sent: {'timestamp': '2025-03-22 22:18:21', 'co2_ppm': 2636.2730226524704, 'no2_ppm': 2.6685720515560503, 'so2_ppm': 5.249055887868805, 'temperature_c': 20.009595013876456, 'humidity_percent': 76.61268224118687, 'aqi': 3, 'environmental_score': 63.13179672243977, 'social_score': 70.06179636000141, 'governance_score': 71.4061494982766}\n",
      "Sent: {'timestamp': '2025-03-22 22:19:58', 'co2_ppm': 3943.50476729388, 'no2_ppm': 8.990660379245924, 'so2_ppm': 1.4844014190138048, 'temperature_c': 21.42019782662179, 'humidity_percent': 47.41741623812602, 'aqi': 5, 'environmental_score': 84.41354647819756, 'social_score': 69.98570699228485, 'governance_score': 76.46477401273184}\n",
      "Sent: {'timestamp': '2025-03-22 22:22:29', 'co2_ppm': 4420.169410293713, 'no2_ppm': 15.172814844809777, 'so2_ppm': 10.20500503813958, 'temperature_c': 16.60964648443553, 'humidity_percent': 79.05152230357851, 'aqi': 12, 'environmental_score': 98.6681101897637, 'social_score': 60.714138504868586, 'governance_score': 51.70412392756366}\n",
      "Sent: {'timestamp': '2025-03-22 22:15:00', 'co2_ppm': 903.872142200313, 'no2_ppm': 3.62248846879342, 'so2_ppm': 4.820192518651239, 'temperature_c': 33.92927424007905, 'humidity_percent': 87.61449698054841, 'aqi': 4, 'environmental_score': 61.78420286271607, 'social_score': 97.69127469132246, 'governance_score': 67.27303755901069}\n",
      "Sent: {'timestamp': '2025-03-22 22:14:57', 'co2_ppm': 4781.0870349710885, 'no2_ppm': 7.740093908799213, 'so2_ppm': 7.565231478089677, 'temperature_c': 32.19733016678579, 'humidity_percent': 37.39599469792552, 'aqi': 7, 'environmental_score': 89.91762829581717, 'social_score': 78.02578846935758, 'governance_score': 50.57723978146365}\n",
      "Sent: {'timestamp': '2025-03-22 22:21:00', 'co2_ppm': 2158.96433686768, 'no2_ppm': 6.51508036048262, 'so2_ppm': 7.598688662034669, 'temperature_c': 17.778642050072072, 'humidity_percent': 59.40476836070378, 'aqi': 7, 'environmental_score': 56.38568631031569, 'social_score': 54.4449638725072, 'governance_score': 77.94547259139357}\n",
      "Sent: {'timestamp': '2025-03-22 22:10:47', 'co2_ppm': 701.2515118050444, 'no2_ppm': 9.15296973675034, 'so2_ppm': 6.371151849785695, 'temperature_c': 11.762402409625924, 'humidity_percent': 37.60334686190807, 'aqi': 7, 'environmental_score': 62.282108835052895, 'social_score': 61.62782991779609, 'governance_score': 61.14547607566524}\n",
      "Sent: {'timestamp': '2025-03-22 22:19:57', 'co2_ppm': 435.52161876608704, 'no2_ppm': 19.5047266227024, 'so2_ppm': 2.4719220844503136, 'temperature_c': 12.86239677090992, 'humidity_percent': 27.34617291796293, 'aqi': 10, 'environmental_score': 62.7842436940715, 'social_score': 60.98300008317311, 'governance_score': 78.85937640282698}\n",
      "Sent: {'timestamp': '2025-03-22 22:15:15', 'co2_ppm': 3946.5025190970496, 'no2_ppm': 5.6358772686544905, 'so2_ppm': 3.783322097414637, 'temperature_c': 27.894039597060885, 'humidity_percent': 67.34719508245091, 'aqi': 4, 'environmental_score': 86.65332502936019, 'social_score': 60.89934421652772, 'governance_score': 61.405927968823626}\n",
      "Sent: {'timestamp': '2025-03-22 22:21:27', 'co2_ppm': 521.9773649915599, 'no2_ppm': 11.029702125946544, 'so2_ppm': 6.057558408743052, 'temperature_c': 24.799598394686218, 'humidity_percent': 64.8189997552883, 'aqi': 8, 'environmental_score': 58.34501894345679, 'social_score': 90.50662633974032, 'governance_score': 61.90062671911216}\n",
      "Sent: {'timestamp': '2025-03-22 22:11:14', 'co2_ppm': 4580.92359745142, 'no2_ppm': 13.30657659808243, 'so2_ppm': 9.336244871852296, 'temperature_c': 39.65474062146794, 'humidity_percent': 72.74179604024397, 'aqi': 11, 'environmental_score': 97.12318496918265, 'social_score': 64.67632330694606, 'governance_score': 82.01279403234435}\n",
      "Sent: {'timestamp': '2025-03-22 22:14:43', 'co2_ppm': 3634.8450572212873, 'no2_ppm': 18.137543698205008, 'so2_ppm': 13.084463364941753, 'temperature_c': 10.242117221890108, 'humidity_percent': 51.67496696318642, 'aqi': 15, 'environmental_score': 75.988151351827, 'social_score': 97.1692848484576, 'governance_score': 92.3071350193403}\n",
      "Sent: {'timestamp': '2025-03-22 22:11:15', 'co2_ppm': 4998.380459149806, 'no2_ppm': 10.938451395849093, 'so2_ppm': 2.666000283505497, 'temperature_c': 15.201525730075025, 'humidity_percent': 45.191360119057606, 'aqi': 6, 'environmental_score': 89.39742336185203, 'social_score': 74.78697107728911, 'governance_score': 85.38541994450385}\n",
      "Sent: {'timestamp': '2025-03-22 22:15:26', 'co2_ppm': 1395.1309419667928, 'no2_ppm': 13.780754462278768, 'so2_ppm': 5.338867230973416, 'temperature_c': 13.3737451251086, 'humidity_percent': 62.31668860049921, 'aqi': 9, 'environmental_score': 68.28035607057696, 'social_score': 88.47884385752099, 'governance_score': 60.3173696399441}\n",
      "Sent: {'timestamp': '2025-03-22 22:19:16', 'co2_ppm': 3178.2091569300346, 'no2_ppm': 2.3475571094048906, 'so2_ppm': 2.7277615350328506, 'temperature_c': 26.9231215231508, 'humidity_percent': 38.66745021331753, 'aqi': 2, 'environmental_score': 89.85315725591636, 'social_score': 75.66764104382712, 'governance_score': 62.78238261344713}\n",
      "Sent: {'timestamp': '2025-03-22 22:26:46', 'co2_ppm': 1316.5745523480805, 'no2_ppm': 4.238063520890242, 'so2_ppm': 2.180605632442433, 'temperature_c': 23.23971556197628, 'humidity_percent': 39.91758252640189, 'aqi': 3, 'environmental_score': 56.86197690582314, 'social_score': 57.02883828882072, 'governance_score': 96.51468962240844}\n",
      "Sent: {'timestamp': '2025-03-22 22:22:02', 'co2_ppm': 442.2778329058346, 'no2_ppm': 2.756498051333446, 'so2_ppm': 3.712449744880773, 'temperature_c': 15.56807951115776, 'humidity_percent': 87.38507093696614, 'aqi': 3, 'environmental_score': 88.81660611230608, 'social_score': 78.81715729627903, 'governance_score': 95.38569278251155}\n",
      "Sent: {'timestamp': '2025-03-22 22:22:41', 'co2_ppm': 533.3929690129654, 'no2_ppm': 2.7727717659233315, 'so2_ppm': 1.097555713888528, 'temperature_c': 35.69507275981336, 'humidity_percent': 72.54322536875424, 'aqi': 1, 'environmental_score': 92.63681362992877, 'social_score': 82.01963358795098, 'governance_score': 67.29285402239115}\n",
      "Sent: {'timestamp': '2025-03-22 22:15:35', 'co2_ppm': 653.4736296454233, 'no2_ppm': 18.37646436504977, 'so2_ppm': 2.1565335047202474, 'temperature_c': 18.1018223243535, 'humidity_percent': 82.72612437842899, 'aqi': 10, 'environmental_score': 93.14314587558168, 'social_score': 95.91283561991006, 'governance_score': 91.38585547372662}\n",
      "Sent: {'timestamp': '2025-03-22 22:16:30', 'co2_ppm': 2105.722691655518, 'no2_ppm': 22.70244811049232, 'so2_ppm': 7.239770310854105, 'temperature_c': 25.85003342922151, 'humidity_percent': 77.55666083627654, 'aqi': 14, 'environmental_score': 57.09041556560358, 'social_score': 82.6561761953307, 'governance_score': 79.8868807571834}\n",
      "Sent: {'timestamp': '2025-03-22 22:17:39', 'co2_ppm': 3501.8757729482463, 'no2_ppm': 4.761085755414244, 'so2_ppm': 10.757598567704788, 'temperature_c': 26.01008674377871, 'humidity_percent': 54.85251853788211, 'aqi': 7, 'environmental_score': 84.52574561265207, 'social_score': 75.0787951643534, 'governance_score': 97.24885793735255}\n",
      "Sent: {'timestamp': '2025-03-22 22:18:50', 'co2_ppm': 2759.0176362460284, 'no2_ppm': 3.00290690254217, 'so2_ppm': 2.856253598552234, 'temperature_c': 10.954469631631902, 'humidity_percent': 24.29233515620509, 'aqi': 2, 'environmental_score': 97.81865381919752, 'social_score': 70.42666707614273, 'governance_score': 54.84615425826873}\n",
      "Sent: {'timestamp': '2025-03-22 22:13:46', 'co2_ppm': 1846.720012675692, 'no2_ppm': 9.657687269554216, 'so2_ppm': 4.046895602292971, 'temperature_c': 21.626580782745123, 'humidity_percent': 72.60148521808028, 'aqi': 6, 'environmental_score': 56.15448049275592, 'social_score': 78.29016873242699, 'governance_score': 63.72414646422111}\n",
      "Sent: {'timestamp': '2025-03-22 22:23:15', 'co2_ppm': 4630.297222582515, 'no2_ppm': 6.0660738407621, 'so2_ppm': 3.507279532319461, 'temperature_c': 25.05141656415728, 'humidity_percent': 53.97356756151427, 'aqi': 4, 'environmental_score': 62.8478302319184, 'social_score': 73.09398895875862, 'governance_score': 90.28568151007472}\n",
      "Sent: {'timestamp': '2025-03-22 22:17:24', 'co2_ppm': 844.2409221666217, 'no2_ppm': 3.7626583678325134, 'so2_ppm': 3.1882041632737628, 'temperature_c': 23.278536075304785, 'humidity_percent': 85.93973974024267, 'aqi': 3, 'environmental_score': 66.61449857870231, 'social_score': 79.73535972790101, 'governance_score': 71.88322172088343}\n",
      "Sent: {'timestamp': '2025-03-22 22:22:52', 'co2_ppm': 4549.148228703995, 'no2_ppm': 8.410482672209632, 'so2_ppm': 7.2657078425230255, 'temperature_c': 15.410887823192263, 'humidity_percent': 42.61241230214421, 'aqi': 7, 'environmental_score': 61.61474315611192, 'social_score': 91.11679568987245, 'governance_score': 58.22421608050111}\n",
      "Sent: {'timestamp': '2025-03-22 22:11:36', 'co2_ppm': 612.8815134550518, 'no2_ppm': 12.18226503364244, 'so2_ppm': 11.49455653588341, 'temperature_c': 33.884790030974166, 'humidity_percent': 21.170483858712767, 'aqi': 11, 'environmental_score': 78.34013385451736, 'social_score': 51.30349227762628, 'governance_score': 63.48669697445667}\n",
      "Sent: {'timestamp': '2025-03-22 22:11:11', 'co2_ppm': 3232.976095537385, 'no2_ppm': 7.812734609136183, 'so2_ppm': 2.5309491269883684, 'temperature_c': 33.81466257913461, 'humidity_percent': 46.39046853800002, 'aqi': 5, 'environmental_score': 61.09511981229602, 'social_score': 90.33348022220468, 'governance_score': 83.49354489605122}\n",
      "Sent: {'timestamp': '2025-03-22 22:23:54', 'co2_ppm': 3300.288001754862, 'no2_ppm': 8.025149633050525, 'so2_ppm': 4.45856717039571, 'temperature_c': 12.870449538681836, 'humidity_percent': 63.99476323367496, 'aqi': 6, 'environmental_score': 82.99634716965473, 'social_score': 70.06029235637139, 'governance_score': 80.10213784924365}\n",
      "Sent: {'timestamp': '2025-03-22 22:22:04', 'co2_ppm': 4325.49523868341, 'no2_ppm': 9.60375385205854, 'so2_ppm': 4.997746855083566, 'temperature_c': 38.10931619199658, 'humidity_percent': 70.27603680664413, 'aqi': 7, 'environmental_score': 78.97607480623702, 'social_score': 81.88505451528682, 'governance_score': 69.56306299954049}\n",
      "Sent: {'timestamp': '2025-03-22 22:15:59', 'co2_ppm': 3321.8158904227985, 'no2_ppm': 5.833905781069828, 'so2_ppm': 2.3375640805750626, 'temperature_c': 16.059485408086317, 'humidity_percent': 41.375379077989415, 'aqi': 4, 'environmental_score': 61.063413039408026, 'social_score': 92.62390992371706, 'governance_score': 79.23068431377335}\n",
      "Sent: {'timestamp': '2025-03-22 22:25:44', 'co2_ppm': 2199.820515195579, 'no2_ppm': 10.72779199508943, 'so2_ppm': 6.208617785954285, 'temperature_c': 22.34322969509233, 'humidity_percent': 43.88381395128376, 'aqi': 8, 'environmental_score': 97.59159025308584, 'social_score': 62.17492303194771, 'governance_score': 55.024276390064905}\n",
      "Sent: {'timestamp': '2025-03-22 22:14:24', 'co2_ppm': 3585.8475759910943, 'no2_ppm': 14.22929443666794, 'so2_ppm': 3.9763458886992185, 'temperature_c': 15.689417723825056, 'humidity_percent': 56.19470891312673, 'aqi': 9, 'environmental_score': 77.73184588697634, 'social_score': 58.82839101925442, 'governance_score': 69.65564456630581}\n",
      "Sent: {'timestamp': '2025-03-22 22:20:43', 'co2_ppm': 878.5864359092981, 'no2_ppm': 1.4918860499095217, 'so2_ppm': 1.5338743457788435, 'temperature_c': 34.747883406564114, 'humidity_percent': 32.27150209137484, 'aqi': 1, 'environmental_score': 83.71602155171409, 'social_score': 51.424391702086766, 'governance_score': 72.3427157408893}\n",
      "Sent: {'timestamp': '2025-03-22 22:20:52', 'co2_ppm': 2290.417622368824, 'no2_ppm': 7.811397088548997, 'so2_ppm': 6.627846156808736, 'temperature_c': 12.608570875160051, 'humidity_percent': 88.09754322319583, 'aqi': 7, 'environmental_score': 97.71140475778222, 'social_score': 78.4911809949379, 'governance_score': 71.6641944869617}\n",
      "Sent: {'timestamp': '2025-03-22 22:19:22', 'co2_ppm': 2261.444048008296, 'no2_ppm': 6.3611734734738885, 'so2_ppm': 7.915029078601991, 'temperature_c': 10.559777585721047, 'humidity_percent': 38.99785026377703, 'aqi': 7, 'environmental_score': 90.43642027787875, 'social_score': 73.98112127827302, 'governance_score': 91.7853156157158}\n",
      "Sent: {'timestamp': '2025-03-22 22:26:07', 'co2_ppm': 3141.346729730718, 'no2_ppm': 18.52853235218184, 'so2_ppm': 5.295754896819359, 'temperature_c': 35.01634980549044, 'humidity_percent': 68.35584079561428, 'aqi': 11, 'environmental_score': 70.63975933298816, 'social_score': 94.9733235057374, 'governance_score': 78.32629775269606}\n",
      "Sent: {'timestamp': '2025-03-22 22:12:56', 'co2_ppm': 3396.85287539664, 'no2_ppm': 2.5740219482395053, 'so2_ppm': 4.246071002114396, 'temperature_c': 12.060748710840398, 'humidity_percent': 27.38306915657389, 'aqi': 3, 'environmental_score': 74.73308719390073, 'social_score': 94.4088650079627, 'governance_score': 73.49420022398976}\n",
      "Sent: {'timestamp': '2025-03-22 22:25:25', 'co2_ppm': 4819.215640026492, 'no2_ppm': 7.115729022163439, 'so2_ppm': 2.147722189849928, 'temperature_c': 30.59256290559881, 'humidity_percent': 20.00633618505108, 'aqi': 4, 'environmental_score': 74.84134358966143, 'social_score': 84.62032314451656, 'governance_score': 50.69137910858473}\n",
      "Sent: {'timestamp': '2025-03-22 22:18:56', 'co2_ppm': 936.2756590494482, 'no2_ppm': 14.712982535124729, 'so2_ppm': 10.451303780864398, 'temperature_c': 37.94460062696783, 'humidity_percent': 84.9057991089008, 'aqi': 12, 'environmental_score': 77.4805930441031, 'social_score': 96.285457157544, 'governance_score': 79.63416618242557}\n",
      "Sent: {'timestamp': '2025-03-22 22:23:51', 'co2_ppm': 810.7763653694236, 'no2_ppm': 21.489687872580376, 'so2_ppm': 5.436961570496642, 'temperature_c': 24.399050726799373, 'humidity_percent': 52.36200246161679, 'aqi': 13, 'environmental_score': 88.05915948843656, 'social_score': 85.53123271737954, 'governance_score': 87.51054089074677}\n",
      "Sent: {'timestamp': '2025-03-22 22:26:33', 'co2_ppm': 4521.745078933069, 'no2_ppm': 6.606222257732865, 'so2_ppm': 7.719075550219488, 'temperature_c': 19.83160933632356, 'humidity_percent': 65.29796633643764, 'aqi': 7, 'environmental_score': 71.53231722268535, 'social_score': 87.0478483773773, 'governance_score': 94.88312501388266}\n",
      "Sent: {'timestamp': '2025-03-22 22:23:44', 'co2_ppm': 3804.1007811366258, 'no2_ppm': 7.811854064443214, 'so2_ppm': 6.46566512120814, 'temperature_c': 37.402874294380254, 'humidity_percent': 68.07280768896305, 'aqi': 7, 'environmental_score': 90.07025012307963, 'social_score': 86.69825645690061, 'governance_score': 77.9265877335526}\n",
      "Sent: {'timestamp': '2025-03-22 22:11:34', 'co2_ppm': 4798.344092385888, 'no2_ppm': 11.671565045303645, 'so2_ppm': 9.277131123113866, 'temperature_c': 31.33283256804433, 'humidity_percent': 52.68114573085944, 'aqi': 10, 'environmental_score': 76.44257057755429, 'social_score': 82.47206960094988, 'governance_score': 81.20547771074297}\n",
      "Sent: {'timestamp': '2025-03-22 22:17:20', 'co2_ppm': 4186.023428242919, 'no2_ppm': 11.07247097466852, 'so2_ppm': 4.56495837824476, 'temperature_c': 25.27885234236695, 'humidity_percent': 79.31328122902401, 'aqi': 7, 'environmental_score': 60.17477669669717, 'social_score': 54.971088471429525, 'governance_score': 96.69492398342888}\n",
      "Sent: {'timestamp': '2025-03-22 22:13:12', 'co2_ppm': 4474.924329909332, 'no2_ppm': 4.194548575764949, 'so2_ppm': 2.3354813864719555, 'temperature_c': 30.172658738153885, 'humidity_percent': 71.97160514395559, 'aqi': 3, 'environmental_score': 75.30738965551605, 'social_score': 81.64789157130254, 'governance_score': 73.20303376998781}\n",
      "Sent: {'timestamp': '2025-03-22 22:26:26', 'co2_ppm': 3608.666449088549, 'no2_ppm': 4.3940653385786295, 'so2_ppm': 11.688685914500438, 'temperature_c': 14.365134195123243, 'humidity_percent': 89.1388787610544, 'aqi': 8, 'environmental_score': 59.263125631015896, 'social_score': 66.71663988557312, 'governance_score': 87.1180854636501}\n",
      "Sent: {'timestamp': '2025-03-22 22:19:02', 'co2_ppm': 4712.571595858267, 'no2_ppm': 7.784003399671016, 'so2_ppm': 5.67886599687649, 'temperature_c': 26.40130545325495, 'humidity_percent': 87.7491855286518, 'aqi': 6, 'environmental_score': 64.23967517348078, 'social_score': 77.26003406746369, 'governance_score': 80.56455575700984}\n",
      "Sent: {'timestamp': '2025-03-22 22:15:02', 'co2_ppm': 4490.424185156445, 'no2_ppm': 4.75536500251487, 'so2_ppm': 10.243845631789638, 'temperature_c': 18.330042242144472, 'humidity_percent': 56.71190009147678, 'aqi': 7, 'environmental_score': 90.81491966339912, 'social_score': 81.85271496973397, 'governance_score': 87.9423197188903}\n",
      "Sent: {'timestamp': '2025-03-22 22:25:00', 'co2_ppm': 783.1510345689207, 'no2_ppm': 9.306131034385654, 'so2_ppm': 1.930123774141271, 'temperature_c': 30.05363718622469, 'humidity_percent': 47.02543905395504, 'aqi': 5, 'environmental_score': 74.10785404886396, 'social_score': 53.36387064553897, 'governance_score': 71.10016072702231}\n",
      "Sent: {'timestamp': '2025-03-22 22:17:13', 'co2_ppm': 2743.0133429897537, 'no2_ppm': 9.720897610014688, 'so2_ppm': 3.4821937721664007, 'temperature_c': 31.166665962709345, 'humidity_percent': 34.74705774513247, 'aqi': 6, 'environmental_score': 56.70777277954415, 'social_score': 83.36573966789594, 'governance_score': 91.01701858284277}\n",
      "Sent: {'timestamp': '2025-03-22 22:22:01', 'co2_ppm': 3771.235094393408, 'no2_ppm': 8.768500540218023, 'so2_ppm': 5.220496129318249, 'temperature_c': 22.28150164079353, 'humidity_percent': 83.04410229977313, 'aqi': 6, 'environmental_score': 59.16332482575851, 'social_score': 77.96228278021225, 'governance_score': 57.23417799749631}\n",
      "Sent: {'timestamp': '2025-03-22 22:14:28', 'co2_ppm': 4191.385023410865, 'no2_ppm': 3.4455809730860603, 'so2_ppm': 3.0396677563110925, 'temperature_c': 12.245020801020212, 'humidity_percent': 51.089543202211615, 'aqi': 3, 'environmental_score': 72.76234297565358, 'social_score': 59.14803124173741, 'governance_score': 75.94262275152101}\n",
      "Sent: {'timestamp': '2025-03-22 22:14:05', 'co2_ppm': 1744.432941252716, 'no2_ppm': 1.38606298205362, 'so2_ppm': 1.2563921471152986, 'temperature_c': 34.477299573884665, 'humidity_percent': 76.06337339915979, 'aqi': 1, 'environmental_score': 74.56959833631404, 'social_score': 83.55353320038807, 'governance_score': 58.25746227475828}\n",
      "Sent: {'timestamp': '2025-03-22 22:19:27', 'co2_ppm': 1434.2166936512515, 'no2_ppm': 3.708154713486637, 'so2_ppm': 4.017950276451115, 'temperature_c': 19.253722123459216, 'humidity_percent': 28.70200369014248, 'aqi': 3, 'environmental_score': 62.02637708137927, 'social_score': 52.99604109055955, 'governance_score': 71.99391160587561}\n",
      "Sent: {'timestamp': '2025-03-22 22:19:51', 'co2_ppm': 1330.1021216824429, 'no2_ppm': 2.8808359223375386, 'so2_ppm': 3.3040898918784536, 'temperature_c': 39.85705264938558, 'humidity_percent': 41.78477746577756, 'aqi': 3, 'environmental_score': 69.54208323264899, 'social_score': 99.42248707361068, 'governance_score': 76.7687875318609}\n",
      "Sent: {'timestamp': '2025-03-22 22:17:33', 'co2_ppm': 1942.3184877633405, 'no2_ppm': 1.458820687850882, 'so2_ppm': 1.3747891217031163, 'temperature_c': 18.84203350854679, 'humidity_percent': 47.66374410411339, 'aqi': 1, 'environmental_score': 86.87440722600164, 'social_score': 64.57462699819041, 'governance_score': 79.45692980215289}\n",
      "Sent: {'timestamp': '2025-03-22 22:24:35', 'co2_ppm': 3576.09773451162, 'no2_ppm': 18.591005610672955, 'so2_ppm': 8.648332242610156, 'temperature_c': 21.39895545892375, 'humidity_percent': 51.768759900447286, 'aqi': 13, 'environmental_score': 82.84680537881563, 'social_score': 73.78579699719849, 'governance_score': 76.85222984027664}\n",
      "Sent: {'timestamp': '2025-03-22 22:23:28', 'co2_ppm': 962.3936645118896, 'no2_ppm': 6.977089254918937, 'so2_ppm': 3.2278054856804648, 'temperature_c': 22.790300047604955, 'humidity_percent': 32.76190668881231, 'aqi': 5, 'environmental_score': 84.67127882806903, 'social_score': 84.00643942181298, 'governance_score': 82.03392093125638}\n",
      "Sent: {'timestamp': '2025-03-22 22:20:07', 'co2_ppm': 4867.434490727268, 'no2_ppm': 7.064203304146003, 'so2_ppm': 5.91081327063077, 'temperature_c': 33.395866319165435, 'humidity_percent': 31.25436550111932, 'aqi': 6, 'environmental_score': 99.4260141159456, 'social_score': 86.27257460750643, 'governance_score': 59.09077695309611}\n",
      "Sent: {'timestamp': '2025-03-22 22:24:17', 'co2_ppm': 1410.5934812933433, 'no2_ppm': 3.8497728553063815, 'so2_ppm': 3.3000804979137377, 'temperature_c': 24.092539268252125, 'humidity_percent': 48.9206188902255, 'aqi': 3, 'environmental_score': 61.75729204966549, 'social_score': 61.90837353516767, 'governance_score': 52.30576114310732}\n",
      "Sent: {'timestamp': '2025-03-22 22:14:33', 'co2_ppm': 4948.336472920757, 'no2_ppm': 5.840706932272406, 'so2_ppm': 8.07865919877744, 'temperature_c': 25.125164578593072, 'humidity_percent': 72.85326091556523, 'aqi': 6, 'environmental_score': 68.62491530857992, 'social_score': 61.06119682032269, 'governance_score': 58.35250794877337}\n",
      "Sent: {'timestamp': '2025-03-22 22:26:43', 'co2_ppm': 4477.445902003026, 'no2_ppm': 2.287858792971642, 'so2_ppm': 2.979777801799592, 'temperature_c': 34.4317481532175, 'humidity_percent': 20.220422586285046, 'aqi': 2, 'environmental_score': 79.71883357938812, 'social_score': 59.43643018831675, 'governance_score': 76.0048314231073}\n",
      "Sent: {'timestamp': '2025-03-22 22:18:57', 'co2_ppm': 4071.324506311629, 'no2_ppm': 9.517832546070771, 'so2_ppm': 6.956012532654199, 'temperature_c': 19.440075936511786, 'humidity_percent': 85.01041332996093, 'aqi': 8, 'environmental_score': 99.92582397444068, 'social_score': 77.22842251631354, 'governance_score': 69.15337037380894}\n",
      "Sent: {'timestamp': '2025-03-22 22:11:03', 'co2_ppm': 3244.17489279912, 'no2_ppm': 6.699228612341006, 'so2_ppm': 4.735955757422283, 'temperature_c': 36.818675373505634, 'humidity_percent': 40.88454072600181, 'aqi': 5, 'environmental_score': 69.4238195433932, 'social_score': 63.31065464064376, 'governance_score': 62.5320522847428}\n",
      "Sent: {'timestamp': '2025-03-22 22:12:08', 'co2_ppm': 713.956711076935, 'no2_ppm': 4.4157571450267135, 'so2_ppm': 1.339525501531251, 'temperature_c': 30.12199153019898, 'humidity_percent': 53.40747801775313, 'aqi': 2, 'environmental_score': 71.64003651708444, 'social_score': 51.36729562570834, 'governance_score': 50.56600489555471}\n",
      "Sent: {'timestamp': '2025-03-22 22:13:43', 'co2_ppm': 1806.640223960202, 'no2_ppm': 3.28453136551576, 'so2_ppm': 2.645780230463214, 'temperature_c': 31.446968837335987, 'humidity_percent': 60.16746567680923, 'aqi': 2, 'environmental_score': 61.27209218147948, 'social_score': 52.19050773987685, 'governance_score': 98.8191046036974}\n",
      "Sent: {'timestamp': '2025-03-22 22:10:33', 'co2_ppm': 4999.735838825597, 'no2_ppm': 4.905125562019912, 'so2_ppm': 6.360023198917505, 'temperature_c': 21.450333511345548, 'humidity_percent': 67.34642769185632, 'aqi': 5, 'environmental_score': 50.68757419574482, 'social_score': 92.61178913717204, 'governance_score': 73.51100960514547}\n",
      "Sent: {'timestamp': '2025-03-22 22:11:20', 'co2_ppm': 1593.9164253750666, 'no2_ppm': 5.324344798208631, 'so2_ppm': 1.7428054085657605, 'temperature_c': 20.218151390655592, 'humidity_percent': 35.48373777421657, 'aqi': 3, 'environmental_score': 67.07269014013883, 'social_score': 94.01903053815356, 'governance_score': 55.20672759205728}\n",
      "Sent: {'timestamp': '2025-03-22 22:10:52', 'co2_ppm': 734.6559315872316, 'no2_ppm': 11.87111494256627, 'so2_ppm': 7.757136526134562, 'temperature_c': 35.77816358337348, 'humidity_percent': 88.58316164075008, 'aqi': 9, 'environmental_score': 77.83888558526927, 'social_score': 99.45026532829124, 'governance_score': 81.57398687427715}\n",
      "Sent: {'timestamp': '2025-03-22 22:14:15', 'co2_ppm': 1658.0292709803373, 'no2_ppm': 15.205195549170002, 'so2_ppm': 7.787932209353388, 'temperature_c': 37.02754421163138, 'humidity_percent': 79.13321595604593, 'aqi': 11, 'environmental_score': 87.86567883326707, 'social_score': 61.55254492261773, 'governance_score': 63.76361040736619}\n",
      "Sent: {'timestamp': '2025-03-22 22:22:09', 'co2_ppm': 1430.649002918623, 'no2_ppm': 2.0214600326905483, 'so2_ppm': 1.1822495959517614, 'temperature_c': 15.205292804865628, 'humidity_percent': 36.95477272709379, 'aqi': 1, 'environmental_score': 72.85214067064547, 'social_score': 51.84728667965068, 'governance_score': 87.80177713994499}\n",
      "Sent: {'timestamp': '2025-03-22 22:14:49', 'co2_ppm': 4913.985352602445, 'no2_ppm': 12.970332874086235, 'so2_ppm': 5.6731180811369954, 'temperature_c': 26.75201490600138, 'humidity_percent': 52.76677588573297, 'aqi': 9, 'environmental_score': 68.38874252102667, 'social_score': 89.69537148915771, 'governance_score': 66.12846063405864}\n",
      "Sent: {'timestamp': '2025-03-22 22:25:01', 'co2_ppm': 3044.1715392620563, 'no2_ppm': 2.85251075783407, 'so2_ppm': 3.837268511604514, 'temperature_c': 10.915933520834916, 'humidity_percent': 36.24228839655065, 'aqi': 3, 'environmental_score': 64.18094568581733, 'social_score': 50.463364391727936, 'governance_score': 80.45411545353181}\n",
      "Sent: {'timestamp': '2025-03-22 22:25:22', 'co2_ppm': 3453.04171846945, 'no2_ppm': 6.210686816941451, 'so2_ppm': 5.197224532418478, 'temperature_c': 10.339780449347732, 'humidity_percent': 70.93093985996171, 'aqi': 5, 'environmental_score': 66.78673942020454, 'social_score': 50.4929753419482, 'governance_score': 81.17343934258959}\n",
      "Sent: {'timestamp': '2025-03-22 22:23:38', 'co2_ppm': 4592.996217566541, 'no2_ppm': 16.077431476975825, 'so2_ppm': 2.473793417597331, 'temperature_c': 20.636828192241584, 'humidity_percent': 34.174242719781034, 'aqi': 9, 'environmental_score': 78.38194965049159, 'social_score': 89.37029159179406, 'governance_score': 93.76064673704948}\n",
      "Sent: {'timestamp': '2025-03-22 22:14:12', 'co2_ppm': 1450.5338375824183, 'no2_ppm': 5.7977464403227845, 'so2_ppm': 5.667859926333115, 'temperature_c': 19.892419830469475, 'humidity_percent': 64.11021529741944, 'aqi': 5, 'environmental_score': 55.60907125290031, 'social_score': 68.23986921102338, 'governance_score': 81.38512129158997}\n",
      "Sent: {'timestamp': '2025-03-22 22:23:50', 'co2_ppm': 2636.621351574415, 'no2_ppm': 15.648630324584875, 'so2_ppm': 7.956304394524352, 'temperature_c': 37.78956062777229, 'humidity_percent': 42.48540314636588, 'aqi': 11, 'environmental_score': 56.35744363586102, 'social_score': 77.0206716325853, 'governance_score': 94.7130103697782}\n",
      "Sent: {'timestamp': '2025-03-22 22:15:29', 'co2_ppm': 2084.465462334865, 'no2_ppm': 4.162545977039269, 'so2_ppm': 3.1962355846496213, 'temperature_c': 34.59289736181154, 'humidity_percent': 69.66557922031427, 'aqi': 3, 'environmental_score': 91.70948379451308, 'social_score': 91.59565937264624, 'governance_score': 74.69678336190515}\n",
      "Sent: {'timestamp': '2025-03-22 22:20:51', 'co2_ppm': 856.9349522127683, 'no2_ppm': 7.195027866182229, 'so2_ppm': 7.898858662600194, 'temperature_c': 22.27271146971573, 'humidity_percent': 33.88320603415063, 'aqi': 7, 'environmental_score': 79.31873918827195, 'social_score': 76.67782589696219, 'governance_score': 71.56347443179749}\n",
      "Sent: {'timestamp': '2025-03-22 22:12:46', 'co2_ppm': 1616.884902409871, 'no2_ppm': 2.0315718152984155, 'so2_ppm': 1.3825003559324467, 'temperature_c': 24.73005937148632, 'humidity_percent': 31.383626906618098, 'aqi': 1, 'environmental_score': 73.95495033364114, 'social_score': 98.69815236241324, 'governance_score': 74.69034841569996}\n",
      "Sent: {'timestamp': '2025-03-22 22:26:25', 'co2_ppm': 4960.687436805166, 'no2_ppm': 5.300788645204227, 'so2_ppm': 7.694175164607707, 'temperature_c': 30.32195888451556, 'humidity_percent': 70.66308929901646, 'aqi': 6, 'environmental_score': 58.406030731402566, 'social_score': 88.16159406197997, 'governance_score': 90.97570831781948}\n",
      "Sent: {'timestamp': '2025-03-22 22:17:17', 'co2_ppm': 2827.9230885716506, 'no2_ppm': 14.831894270123936, 'so2_ppm': 4.5273207948143686, 'temperature_c': 38.52991972339861, 'humidity_percent': 48.52024078691157, 'aqi': 9, 'environmental_score': 67.59248372087453, 'social_score': 58.6436032071549, 'governance_score': 70.9561706539985}\n",
      "Sent: {'timestamp': '2025-03-22 22:25:49', 'co2_ppm': 4380.376452470282, 'no2_ppm': 5.324930399475618, 'so2_ppm': 7.02750813352242, 'temperature_c': 11.659358355438728, 'humidity_percent': 23.32657058149239, 'aqi': 6, 'environmental_score': 85.9357314357048, 'social_score': 67.51268818268558, 'governance_score': 67.04546716800432}\n",
      "Sent: {'timestamp': '2025-03-22 22:17:50', 'co2_ppm': 3872.875543290723, 'no2_ppm': 7.891554567006967, 'so2_ppm': 6.94737950079104, 'temperature_c': 13.05940778581039, 'humidity_percent': 27.45977986068528, 'aqi': 7, 'environmental_score': 56.69530170714791, 'social_score': 98.89161248552831, 'governance_score': 76.09784733910733}\n",
      "Sent: {'timestamp': '2025-03-22 22:15:17', 'co2_ppm': 1096.178177389962, 'no2_ppm': 8.131011926051556, 'so2_ppm': 3.856540244291461, 'temperature_c': 15.594509220344026, 'humidity_percent': 78.69876843473783, 'aqi': 5, 'environmental_score': 96.16241205980654, 'social_score': 63.70466045578638, 'governance_score': 80.29722964520147}\n",
      "Sent: {'timestamp': '2025-03-22 22:21:45', 'co2_ppm': 4673.928426842866, 'no2_ppm': 5.372202511766258, 'so2_ppm': 7.860639566821054, 'temperature_c': 20.66693203565038, 'humidity_percent': 61.71892631814698, 'aqi': 6, 'environmental_score': 91.0724206625791, 'social_score': 59.763696916445895, 'governance_score': 88.27521660817777}\n",
      "Sent: {'timestamp': '2025-03-22 22:11:08', 'co2_ppm': 1059.5534242038557, 'no2_ppm': 17.920750651185994, 'so2_ppm': 5.136910818318991, 'temperature_c': 13.483549768038303, 'humidity_percent': 43.79438482051099, 'aqi': 11, 'environmental_score': 73.3463749799756, 'social_score': 89.14713304309007, 'governance_score': 57.609911165401954}\n",
      "Sent: {'timestamp': '2025-03-22 22:24:52', 'co2_ppm': 3455.3003554351026, 'no2_ppm': 8.563311765979714, 'so2_ppm': 3.216750525739543, 'temperature_c': 14.39086234313314, 'humidity_percent': 81.1054459182097, 'aqi': 5, 'environmental_score': 71.26146877183913, 'social_score': 59.48805957580493, 'governance_score': 67.150554396584}\n",
      "Sent: {'timestamp': '2025-03-22 22:20:03', 'co2_ppm': 4515.380805512155, 'no2_ppm': 6.272264122990873, 'so2_ppm': 2.6364072372299163, 'temperature_c': 14.414336304928073, 'humidity_percent': 67.18527783993306, 'aqi': 4, 'environmental_score': 54.24754061651404, 'social_score': 92.44051783221936, 'governance_score': 74.27031937931669}\n",
      "Sent: {'timestamp': '2025-03-22 22:22:55', 'co2_ppm': 1278.8282411330051, 'no2_ppm': 5.961509932846666, 'so2_ppm': 9.41138533931404, 'temperature_c': 38.46602695468408, 'humidity_percent': 43.09744531697904, 'aqi': 7, 'environmental_score': 96.27347864242503, 'social_score': 97.70812355480224, 'governance_score': 85.62478223513014}\n",
      "Sent: {'timestamp': '2025-03-22 22:20:57', 'co2_ppm': 2770.636998237993, 'no2_ppm': 6.140232176430716, 'so2_ppm': 10.110003530556895, 'temperature_c': 15.27671280523574, 'humidity_percent': 65.63764872495847, 'aqi': 8, 'environmental_score': 79.71032058270698, 'social_score': 89.86965075020439, 'governance_score': 81.62941225518178}\n",
      "Sent: {'timestamp': '2025-03-22 22:20:58', 'co2_ppm': 1466.8072649360126, 'no2_ppm': 11.069587021952971, 'so2_ppm': 4.4213750201484086, 'temperature_c': 12.19506777086643, 'humidity_percent': 27.84994628165296, 'aqi': 7, 'environmental_score': 78.04399376980463, 'social_score': 64.09330259726156, 'governance_score': 68.564530869807}\n",
      "Sent: {'timestamp': '2025-03-22 22:24:33', 'co2_ppm': 1957.3805255916327, 'no2_ppm': 1.8804599001981244, 'so2_ppm': 4.061496511261395, 'temperature_c': 23.620593563279872, 'humidity_percent': 47.76834099575996, 'aqi': 2, 'environmental_score': 74.60154691098472, 'social_score': 78.82311471967977, 'governance_score': 90.44229401071983}\n",
      "Sent: {'timestamp': '2025-03-22 22:21:16', 'co2_ppm': 1761.837283465264, 'no2_ppm': 4.110186578125858, 'so2_ppm': 1.2536097340818777, 'temperature_c': 28.41222490253725, 'humidity_percent': 46.08494678022883, 'aqi': 2, 'environmental_score': 94.43463494144372, 'social_score': 92.90392376511484, 'governance_score': 54.62256027598097}\n",
      "Sent: {'timestamp': '2025-03-22 22:15:06', 'co2_ppm': 990.0011873291468, 'no2_ppm': 14.050040318701049, 'so2_ppm': 4.153193160966894, 'temperature_c': 28.362901349759547, 'humidity_percent': 51.31814650260184, 'aqi': 9, 'environmental_score': 56.24404860129501, 'social_score': 50.757637910984506, 'governance_score': 93.4049320617006}\n",
      "Sent: {'timestamp': '2025-03-22 22:21:37', 'co2_ppm': 2580.102957601669, 'no2_ppm': 19.3162950106704, 'so2_ppm': 8.1248880004475, 'temperature_c': 30.2515574403376, 'humidity_percent': 69.50750191863635, 'aqi': 13, 'environmental_score': 95.2743327329616, 'social_score': 96.80764001130714, 'governance_score': 85.36474103228315}\n",
      "Sent: {'timestamp': '2025-03-22 22:15:22', 'co2_ppm': 1925.7399882703985, 'no2_ppm': 22.07675649089262, 'so2_ppm': 11.158123446195509, 'temperature_c': 27.907328339067472, 'humidity_percent': 36.096264742132135, 'aqi': 16, 'environmental_score': 76.13093310044762, 'social_score': 50.8036718925066, 'governance_score': 77.3661705908593}\n",
      "Sent: {'timestamp': '2025-03-22 22:15:18', 'co2_ppm': 3821.4162187627103, 'no2_ppm': 12.052254206163251, 'so2_ppm': 10.228257345752247, 'temperature_c': 13.856806169537688, 'humidity_percent': 62.7371082331476, 'aqi': 11, 'environmental_score': 58.97045032741583, 'social_score': 65.16559934699876, 'governance_score': 77.22078664233496}\n",
      "Sent: {'timestamp': '2025-03-22 22:16:09', 'co2_ppm': 1340.52472442958, 'no2_ppm': 6.166606888339583, 'so2_ppm': 5.951168895333547, 'temperature_c': 13.906387017676014, 'humidity_percent': 88.1626337047179, 'aqi': 6, 'environmental_score': 91.83784076994463, 'social_score': 93.77999511275635, 'governance_score': 72.9243364421796}\n",
      "Sent: {'timestamp': '2025-03-22 22:18:07', 'co2_ppm': 3749.1470526649614, 'no2_ppm': 4.06357478776397, 'so2_ppm': 5.04409976955027, 'temperature_c': 17.05918146776372, 'humidity_percent': 35.40684746295696, 'aqi': 4, 'environmental_score': 59.22915136831172, 'social_score': 65.92642244417038, 'governance_score': 99.79445793393278}\n",
      "Sent: {'timestamp': '2025-03-22 22:11:58', 'co2_ppm': 1640.3461853064605, 'no2_ppm': 23.035725524629328, 'so2_ppm': 4.41354630135026, 'temperature_c': 23.773315430956075, 'humidity_percent': 89.45566793553071, 'aqi': 13, 'environmental_score': 94.62673126455051, 'social_score': 83.45334240748326, 'governance_score': 72.17051545966598}\n",
      "Sent: {'timestamp': '2025-03-22 22:26:37', 'co2_ppm': 2100.092933902895, 'no2_ppm': 18.13350061886718, 'so2_ppm': 3.1976713596696804, 'temperature_c': 12.70811754220308, 'humidity_percent': 60.09627582341115, 'aqi': 10, 'environmental_score': 98.5681277847027, 'social_score': 76.5554513853086, 'governance_score': 63.37560676284838}\n",
      "Sent: {'timestamp': '2025-03-22 22:18:14', 'co2_ppm': 2537.3614674353003, 'no2_ppm': 2.345151794327292, 'so2_ppm': 2.63466442629301, 'temperature_c': 33.200422741259125, 'humidity_percent': 86.30024420929291, 'aqi': 2, 'environmental_score': 72.90682495472659, 'social_score': 66.99238436375857, 'governance_score': 76.81591095737329}\n",
      "Sent: {'timestamp': '2025-03-22 22:16:11', 'co2_ppm': 3867.47080067656, 'no2_ppm': 8.555912643910636, 'so2_ppm': 11.005787985128537, 'temperature_c': 11.578422257612882, 'humidity_percent': 73.83803646174692, 'aqi': 9, 'environmental_score': 82.27683219811644, 'social_score': 50.016493233556005, 'governance_score': 97.21836529839608}\n",
      "Sent: {'timestamp': '2025-03-22 22:14:22', 'co2_ppm': 3217.4103294269657, 'no2_ppm': 3.6528773086635136, 'so2_ppm': 4.229942195467369, 'temperature_c': 21.69477795861393, 'humidity_percent': 53.65233749826836, 'aqi': 3, 'environmental_score': 79.43181536104962, 'social_score': 98.72392400494806, 'governance_score': 87.22070188873604}\n",
      "Sent: {'timestamp': '2025-03-22 22:14:27', 'co2_ppm': 1130.4288044080688, 'no2_ppm': 10.996926725025352, 'so2_ppm': 10.602011446657873, 'temperature_c': 35.858203390725336, 'humidity_percent': 85.28038435245902, 'aqi': 10, 'environmental_score': 82.53837013205245, 'social_score': 68.71960873821901, 'governance_score': 86.45574786446467}\n",
      "Sent: {'timestamp': '2025-03-22 22:23:30', 'co2_ppm': 1158.9533216614968, 'no2_ppm': 3.4976533914247536, 'so2_ppm': 3.134749942757595, 'temperature_c': 24.03259339786868, 'humidity_percent': 53.00221935340616, 'aqi': 3, 'environmental_score': 57.797729669060594, 'social_score': 95.55231262812045, 'governance_score': 68.21892292281575}\n",
      "Sent: {'timestamp': '2025-03-22 22:21:25', 'co2_ppm': 2888.806576701993, 'no2_ppm': 9.41062610647624, 'so2_ppm': 2.340316883723433, 'temperature_c': 39.69582707208786, 'humidity_percent': 50.74244117132942, 'aqi': 5, 'environmental_score': 62.71592444999859, 'social_score': 99.79910432521456, 'governance_score': 55.09878767321901}\n",
      "Sent: {'timestamp': '2025-03-22 22:18:22', 'co2_ppm': 1469.700464582142, 'no2_ppm': 6.998960569302846, 'so2_ppm': 5.3996604841058735, 'temperature_c': 24.891406852139006, 'humidity_percent': 38.61748378818767, 'aqi': 6, 'environmental_score': 70.61954223430041, 'social_score': 83.9093315085031, 'governance_score': 74.663316266865}\n",
      "Sent: {'timestamp': '2025-03-22 22:21:18', 'co2_ppm': 3241.768961599017, 'no2_ppm': 21.510056815137457, 'so2_ppm': 11.47199689317234, 'temperature_c': 22.07793734913505, 'humidity_percent': 38.82408975504683, 'aqi': 16, 'environmental_score': 97.10878771315151, 'social_score': 80.88676269636258, 'governance_score': 61.93326508582384}\n",
      "Sent: {'timestamp': '2025-03-22 22:16:25', 'co2_ppm': 2300.6075758836905, 'no2_ppm': 11.587260349507911, 'so2_ppm': 14.138026302286868, 'temperature_c': 28.121265131928965, 'humidity_percent': 64.59070981799765, 'aqi': 12, 'environmental_score': 71.04571866020237, 'social_score': 59.00598339596267, 'governance_score': 53.376110280166095}\n",
      "Sent: {'timestamp': '2025-03-22 22:18:54', 'co2_ppm': 3656.5477235536578, 'no2_ppm': 12.25731102940781, 'so2_ppm': 3.7212387624817858, 'temperature_c': 34.14030749427771, 'humidity_percent': 32.683159375633444, 'aqi': 7, 'environmental_score': 57.15341326725922, 'social_score': 64.07983305528441, 'governance_score': 72.57691295392567}\n",
      "Sent: {'timestamp': '2025-03-22 22:24:13', 'co2_ppm': 700.8033220462263, 'no2_ppm': 1.8107247350760045, 'so2_ppm': 1.915277655777091, 'temperature_c': 38.295358766563794, 'humidity_percent': 52.68983886551512, 'aqi': 1, 'environmental_score': 66.46689936115229, 'social_score': 53.24144471207936, 'governance_score': 54.83506556037907}\n",
      "Sent: {'timestamp': '2025-03-22 22:12:25', 'co2_ppm': 879.5985275185315, 'no2_ppm': 7.630607938704813, 'so2_ppm': 9.093459323208483, 'temperature_c': 35.60918892180297, 'humidity_percent': 57.99871216439406, 'aqi': 8, 'environmental_score': 53.85429591100364, 'social_score': 56.14628572903164, 'governance_score': 85.35179472866419}\n",
      "Sent: {'timestamp': '2025-03-22 22:16:03', 'co2_ppm': 2195.2058798449643, 'no2_ppm': 5.030875988960574, 'so2_ppm': 7.480346622398354, 'temperature_c': 34.25353009053885, 'humidity_percent': 89.39261631268353, 'aqi': 6, 'environmental_score': 81.4818341167795, 'social_score': 77.67155618125074, 'governance_score': 67.57150649507301}\n",
      "Sent: {'timestamp': '2025-03-22 22:16:13', 'co2_ppm': 987.9886349324491, 'no2_ppm': 3.9783486429792143, 'so2_ppm': 1.0628909606646268, 'temperature_c': 24.115916644899272, 'humidity_percent': 89.62053775874689, 'aqi': 2, 'environmental_score': 51.79521126070011, 'social_score': 72.54267727870638, 'governance_score': 55.82195867474729}\n",
      "Sent: {'timestamp': '2025-03-22 22:25:50', 'co2_ppm': 2680.967100706003, 'no2_ppm': 6.099467911970452, 'so2_ppm': 3.1421981337079203, 'temperature_c': 29.376057851304264, 'humidity_percent': 52.93362039910232, 'aqi': 4, 'environmental_score': 59.85314595985777, 'social_score': 97.45125876243507, 'governance_score': 53.52504287523575}\n",
      "Sent: {'timestamp': '2025-03-22 22:22:37', 'co2_ppm': 2771.960162366613, 'no2_ppm': 10.28443263398265, 'so2_ppm': 5.798607448192272, 'temperature_c': 15.808668746884422, 'humidity_percent': 61.55676216671964, 'aqi': 8, 'environmental_score': 59.99456065534788, 'social_score': 52.33284241586853, 'governance_score': 66.23818959189943}\n",
      "Sent: {'timestamp': '2025-03-22 22:25:56', 'co2_ppm': 2764.5305040735125, 'no2_ppm': 7.257175144982166, 'so2_ppm': 2.165677967970779, 'temperature_c': 14.286614434614291, 'humidity_percent': 84.93960209047262, 'aqi': 4, 'environmental_score': 66.75165939989861, 'social_score': 83.82416026127494, 'governance_score': 78.35637719062895}\n",
      "Sent: {'timestamp': '2025-03-22 22:23:57', 'co2_ppm': 4997.729786817071, 'no2_ppm': 17.78782725470481, 'so2_ppm': 3.42441648546226, 'temperature_c': 27.54568682337594, 'humidity_percent': 82.44583764250169, 'aqi': 10, 'environmental_score': 53.99440936119924, 'social_score': 85.23403118175554, 'governance_score': 90.8560774482546}\n",
      "Sent: {'timestamp': '2025-03-22 22:21:04', 'co2_ppm': 752.6403780240236, 'no2_ppm': 19.86371860624485, 'so2_ppm': 7.060315369732631, 'temperature_c': 37.50550466599645, 'humidity_percent': 75.441883425655, 'aqi': 13, 'environmental_score': 53.68274964916477, 'social_score': 91.25565279647734, 'governance_score': 86.43958084952413}\n",
      "Sent: {'timestamp': '2025-03-22 22:13:27', 'co2_ppm': 3416.535064025459, 'no2_ppm': 6.843020578285749, 'so2_ppm': 4.374997059680082, 'temperature_c': 16.11998678765758, 'humidity_percent': 79.57723662860234, 'aqi': 5, 'environmental_score': 64.17262802510928, 'social_score': 65.91261162211887, 'governance_score': 73.25045454791119}\n",
      "Sent: {'timestamp': '2025-03-22 22:13:41', 'co2_ppm': 1419.971126503592, 'no2_ppm': 4.63941033510427, 'so2_ppm': 2.38222024008732, 'temperature_c': 10.696311819172056, 'humidity_percent': 52.00724685207947, 'aqi': 3, 'environmental_score': 52.73575810209378, 'social_score': 69.79049933376778, 'governance_score': 64.69397103006757}\n",
      "Sent: {'timestamp': '2025-03-22 22:16:56', 'co2_ppm': 1756.6917413815352, 'no2_ppm': 4.525153296932976, 'so2_ppm': 11.380316815900596, 'temperature_c': 28.68086438975942, 'humidity_percent': 89.98146316865105, 'aqi': 7, 'environmental_score': 81.83169677788645, 'social_score': 96.84450522149596, 'governance_score': 68.53201626041677}\n",
      "Sent: {'timestamp': '2025-03-22 22:15:40', 'co2_ppm': 4615.234712618988, 'no2_ppm': 13.42341944119264, 'so2_ppm': 6.361233633411296, 'temperature_c': 35.10634611335213, 'humidity_percent': 33.512638528678124, 'aqi': 9, 'environmental_score': 69.50030754725671, 'social_score': 78.42215992233157, 'governance_score': 98.44447309886304}\n",
      "Sent: {'timestamp': '2025-03-22 22:13:36', 'co2_ppm': 1217.894170954606, 'no2_ppm': 2.120981296250302, 'so2_ppm': 3.0261831707346643, 'temperature_c': 26.501814197089697, 'humidity_percent': 52.860003450522655, 'aqi': 2, 'environmental_score': 63.66638534630484, 'social_score': 53.19022896434688, 'governance_score': 83.335062347737}\n",
      "Sent: {'timestamp': '2025-03-22 22:20:21', 'co2_ppm': 2723.038247725867, 'no2_ppm': 12.139416561740992, 'so2_ppm': 3.3377039138832543, 'temperature_c': 15.05888289429296, 'humidity_percent': 78.76363070945851, 'aqi': 7, 'environmental_score': 98.50033269321456, 'social_score': 81.58683305309964, 'governance_score': 94.55300382610062}\n",
      "Sent: {'timestamp': '2025-03-22 22:11:26', 'co2_ppm': 1022.7232418131672, 'no2_ppm': 12.764150813062844, 'so2_ppm': 5.137615286794107, 'temperature_c': 31.227235102879376, 'humidity_percent': 68.05193012898253, 'aqi': 8, 'environmental_score': 93.3005929942912, 'social_score': 56.56211068000498, 'governance_score': 77.20170949045863}\n"
     ]
    }
   ],
   "source": [
    "from kafka import KafkaProducer\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Initialize Kafka Producer\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=['localhost:9092'],\n",
    "    value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    ")\n",
    "\n",
    "# Function to process CSV and prepare data\n",
    "def process_csv(file_path):\n",
    "    try:\n",
    "        # Read CSV\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Ensure required columns are present\n",
    "        required_columns = [\n",
    "            'Timestamp', 'CO2_ppm', 'NO2_ppm', 'SO2_ppm', \n",
    "            'Temperature_C', 'Humidity_%', 'AQI', \n",
    "            'Environmental_Score', 'Social_Score', 'Governance_Score'\n",
    "        ]\n",
    "        if not all(col in df.columns for col in required_columns):\n",
    "            raise ValueError(f\"CSV file must contain the following columns: {required_columns}\")\n",
    "        \n",
    "        # Convert Timestamp to string if not already\n",
    "        df['Timestamp'] = df['Timestamp'].astype(str)\n",
    "        \n",
    "        # Yield each row as a dictionary\n",
    "        for _, row in df.iterrows():\n",
    "            yield {\n",
    "                \"timestamp\": row['Timestamp'],\n",
    "                \"co2_ppm\": float(row['CO2_ppm']),\n",
    "                \"no2_ppm\": float(row['NO2_ppm']),\n",
    "                \"so2_ppm\": float(row['SO2_ppm']),\n",
    "                \"temperature_c\": float(row['Temperature_C']),\n",
    "                \"humidity_percent\": float(row['Humidity_%']),\n",
    "                \"aqi\": int(row['AQI']),\n",
    "                \"environmental_score\": float(row['Environmental_Score']),\n",
    "                \"social_score\": float(row['Social_Score']),\n",
    "                \"governance_score\": float(row['Governance_Score'])\n",
    "            }\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing CSV: {e}\")\n",
    "        return []\n",
    "\n",
    "# Main block to send data to Kafka topic\n",
    "def send_to_kafka(file_path):\n",
    "    try:\n",
    "        print(\"Reading and sending data to Kafka...\")\n",
    "        for data in process_csv(file_path):\n",
    "            producer.send('carbonfootprint', data)\n",
    "            print(f\"Sent: {data}\")\n",
    "            time.sleep(0.5)  # Throttle to avoid overwhelming Kafka\n",
    "    except Exception as e:\n",
    "        print(f\"Error sending data: {e}\")\n",
    "    finally:\n",
    "        producer.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = \"/Users/kdn_aikothalavanya/Desktop/KPMG Projects/carbonfootprint/real_time_monitoring_data.csv\"\n",
    "    send_to_kafka(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consuming messages from Kafka...\n",
      "Stopping consumer...\n"
     ]
    }
   ],
   "source": [
    "from kafka import KafkaConsumer\n",
    "import json\n",
    "\n",
    "# Initialize Kafka Consumer\n",
    "consumer = KafkaConsumer(\n",
    "    'carbonfootprint',\n",
    "    bootstrap_servers=['localhost:9092'],\n",
    "    value_deserializer=lambda v: json.loads(v.decode('utf-8')),\n",
    "    auto_offset_reset='earliest',  # Start reading from the beginning\n",
    "    enable_auto_commit=True,\n",
    "    group_id='carbonfootprint_group'\n",
    ")\n",
    "\n",
    "def consume_from_kafka():\n",
    "    try:\n",
    "        print(\"Consuming messages from Kafka...\")\n",
    "        for message in consumer:\n",
    "            data = message.value\n",
    "            print(f\"Received: {data}\")\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Stopping consumer...\")\n",
    "    finally:\n",
    "        consumer.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    consume_from_kafka()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "class IoTEnvironment(gym.Env):\n",
    "    def __init__(self, sensor_data):\n",
    "        super(IoTEnvironment, self).__init__()\n",
    "        self.sensor_data = sensor_data\n",
    "        self.current_step = 0\n",
    "\n",
    "        # Define observation and action spaces\n",
    "        self.observation_space = spaces.Box(low=np.min(sensor_data), high=np.max(sensor_data), shape=(len(sensor_data[0]),), dtype=np.float32)\n",
    "        self.action_space = spaces.Discrete(2)  # Example: 0 = Normal, 1 = Alert\n",
    "\n",
    "    def step(self, action):\n",
    "        # Apply the action and calculate the reward\n",
    "        done = self.current_step >= len(self.sensor_data) - 1\n",
    "        reward = 1 if (action == 1 and self.is_anomaly()) else 0\n",
    "        self.current_step += 1\n",
    "\n",
    "        return self.sensor_data[self.current_step], reward, done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        return self.sensor_data[self.current_step]\n",
    "\n",
    "    def is_anomaly(self):\n",
    "        # Example logic for anomaly detection\n",
    "        return np.random.random() < 0.1  # Replace with domain-specific logic\n",
    "\n",
    "# Example usage:\n",
    "# sensor_data = np.array([doc[\"_source\"][\"sensor_value\"] for doc in retrieved_data])  # Extract IoT sensor values\n",
    "# env = IoTEnvironment(sensor_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting stable_baselines3\n",
      "  Using cached stable_baselines3-2.5.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting gymnasium<1.1.0,>=0.29.1 (from stable_baselines3)\n",
      "  Using cached gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.20 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from stable_baselines3) (1.26.4)\n",
      "INFO: pip is looking at multiple versions of stable-baselines3 to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting stable_baselines3\n",
      "  Using cached stable_baselines3-2.4.1-py3-none-any.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: torch>=1.13 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from stable_baselines3) (2.2.2)\n",
      "Requirement already satisfied: cloudpickle in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from stable_baselines3) (3.1.1)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from stable_baselines3) (2.2.3)\n",
      "Collecting matplotlib (from stable_baselines3)\n",
      "  Downloading matplotlib-3.10.1-cp310-cp310-macosx_10_12_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from gymnasium<1.1.0,>=0.29.1->stable_baselines3) (4.12.2)\n",
      "Collecting farama-notifications>=0.0.1 (from gymnasium<1.1.0,>=0.29.1->stable_baselines3)\n",
      "  Using cached Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from torch>=1.13->stable_baselines3) (3.13.1)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from torch>=1.13->stable_baselines3) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from torch>=1.13->stable_baselines3) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from torch>=1.13->stable_baselines3) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from torch>=1.13->stable_baselines3) (2024.12.0)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib->stable_baselines3)\n",
      "  Downloading contourpy-1.3.1-cp310-cp310-macosx_10_9_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib->stable_baselines3)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib->stable_baselines3)\n",
      "  Downloading fonttools-4.56.0-cp310-cp310-macosx_10_9_x86_64.whl.metadata (101 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib->stable_baselines3)\n",
      "  Downloading kiwisolver-1.4.8-cp310-cp310-macosx_10_9_x86_64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from matplotlib->stable_baselines3) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from matplotlib->stable_baselines3) (11.1.0)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib->stable_baselines3)\n",
      "  Downloading pyparsing-3.2.2-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from matplotlib->stable_baselines3) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from pandas->stable_baselines3) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from pandas->stable_baselines3) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->stable_baselines3) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from jinja2->torch>=1.13->stable_baselines3) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from sympy->torch>=1.13->stable_baselines3) (1.3.0)\n",
      "Using cached stable_baselines3-2.4.1-py3-none-any.whl (183 kB)\n",
      "Using cached gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
      "Downloading matplotlib-3.10.1-cp310-cp310-macosx_10_12_x86_64.whl (8.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.1-cp310-cp310-macosx_10_9_x86_64.whl (268 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
      "Downloading fonttools-4.56.0-cp310-cp310-macosx_10_9_x86_64.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.8-cp310-cp310-macosx_10_9_x86_64.whl (66 kB)\n",
      "Downloading pyparsing-3.2.2-py3-none-any.whl (111 kB)\n",
      "Installing collected packages: farama-notifications, pyparsing, kiwisolver, gymnasium, fonttools, cycler, contourpy, matplotlib, stable_baselines3\n",
      "Successfully installed contourpy-1.3.1 cycler-0.12.1 farama-notifications-0.0.4 fonttools-4.56.0 gymnasium-1.0.0 kiwisolver-1.4.8 matplotlib-3.10.1 pyparsing-3.2.2 stable_baselines3-2.4.1\n"
     ]
    }
   ],
   "source": [
    "! pip install stable_baselines3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stable-baselines3 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (2.4.1)\n",
      "Collecting stable-baselines3\n",
      "  Using cached stable_baselines3-2.5.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: gymnasium in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (1.0.0)\n",
      "Collecting gymnasium\n",
      "  Downloading gymnasium-1.1.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.20 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from stable-baselines3) (1.26.4)\n",
      "INFO: pip is looking at multiple versions of stable-baselines3 to determine which version is compatible with other requirements. This could take a while.\n",
      "Requirement already satisfied: torch>=1.13 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from stable-baselines3) (2.2.2)\n",
      "Requirement already satisfied: cloudpickle in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from stable-baselines3) (3.1.1)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from stable-baselines3) (2.2.3)\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from stable-baselines3) (3.10.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from gymnasium) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from gymnasium) (0.0.4)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3) (3.13.1)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3) (2024.12.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from matplotlib->stable-baselines3) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from matplotlib->stable-baselines3) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from matplotlib->stable-baselines3) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from matplotlib->stable-baselines3) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from matplotlib->stable-baselines3) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from matplotlib->stable-baselines3) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from matplotlib->stable-baselines3) (3.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from matplotlib->stable-baselines3) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from pandas->stable-baselines3) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from pandas->stable-baselines3) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from jinja2->torch>=1.13->stable-baselines3) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from sympy->torch>=1.13->stable-baselines3) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install --upgrade stable-baselines3 gymnasium\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting shimmy\n",
      "  Using cached Shimmy-2.0.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from shimmy) (1.26.4)\n",
      "Requirement already satisfied: gymnasium>=1.0.0a1 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from shimmy) (1.0.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from gymnasium>=1.0.0a1->shimmy) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from gymnasium>=1.0.0a1->shimmy) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from gymnasium>=1.0.0a1->shimmy) (0.0.4)\n",
      "Using cached Shimmy-2.0.0-py3-none-any.whl (30 kB)\n",
      "Installing collected packages: shimmy\n",
      "Successfully installed shimmy-2.0.0\n"
     ]
    }
   ],
   "source": [
    "! pip install shimmy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from gymnasium) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from gymnasium) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from gymnasium) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from gymnasium) (0.0.4)\n"
     ]
    }
   ],
   "source": [
    "! pip install gymnasium\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving data from Elasticsearch index 'iot_sensor'...\n",
      "Retrieved 200 documents from index 'iot_sensor'.\n",
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8m/8kxc8bdn5dzfsqxv0r5qsqxw0000gp/T/ipykernel_28393/38850630.py:28: DeprecationWarning: The 'body' parameter is deprecated for the 'search' API and will be removed in a future version. Instead use API parameters directly. See https://github.com/elastic/elasticsearch-py/issues/1698 for more information\n",
      "  response = es_client.search(index=index, body=query, size=size)\n",
      "/opt/anaconda3/envs/tf_env/lib/python3.10/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -99.6    |\n",
      "| time/              |          |\n",
      "|    fps             | 1376     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -97.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 913         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009565471 |\n",
      "|    clip_fraction        | 0.123       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | -0.0223     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.27        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0084     |\n",
      "|    value_loss           | 16.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -94.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 914         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011784143 |\n",
      "|    clip_fraction        | 0.0617      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | 0.0343      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.33        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.005      |\n",
      "|    value_loss           | 15.9        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -93.1        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 921          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 8            |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019950261 |\n",
      "|    clip_fraction        | 0.000391     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | 0.0352       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.52         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.000585    |\n",
      "|    value_loss           | 15.8         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -91.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 936         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003703986 |\n",
      "|    clip_fraction        | 0.0142      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 0.0366      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.88        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00217    |\n",
      "|    value_loss           | 19.4        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tf_env/lib/python3.10/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:243: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.\n",
      "  warnings.warn(\"You tried to call render() but no `render_mode` was passed to the env constructor.\")\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "import pandas as pd\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "# Elasticsearch connection\n",
    "es = Elasticsearch(\"http://localhost:9200\")  # Replace with your Elasticsearch host and port\n",
    "index_name = \"iot_sensor\"  # Replace with your index name\n",
    "\n",
    "def retrieve_data(es_client, index, size=1000):\n",
    "    \"\"\"\n",
    "    Retrieves documents from the specified Elasticsearch index.\n",
    "\n",
    "    :param es_client: Elasticsearch client instance.\n",
    "    :param index: Index name to query.\n",
    "    :param size: Number of documents to retrieve.\n",
    "    :return: Retrieved documents as a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Query to fetch all documents\n",
    "        query = {\n",
    "            \"query\": {\n",
    "                \"match_all\": {}\n",
    "            }\n",
    "        }\n",
    "        response = es_client.search(index=index, body=query, size=size)\n",
    "        documents = response.get(\"hits\", {}).get(\"hits\", [])\n",
    "        if documents:\n",
    "            print(f\"Retrieved {len(documents)} documents from index '{index}'.\")\n",
    "            data = [doc[\"_source\"] for doc in documents]\n",
    "            return pd.DataFrame(data)\n",
    "        else:\n",
    "            print(f\"No data found in index '{index}'.\")\n",
    "            return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving data: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "class CarbonFootprintEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Custom environment for monitoring and optimizing carbon footprint.\n",
    "    \"\"\"\n",
    "    def __init__(self, data):\n",
    "        super(CarbonFootprintEnv, self).__init__()\n",
    "        self.data = data\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0, high=1, shape=(data.shape[1],), dtype=np.float32\n",
    "        )\n",
    "        self.action_space = gym.spaces.Discrete(3)  # Actions: reduce, maintain, increase\n",
    "        self.current_step = 0\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        \"\"\"\n",
    "        Sets the seed for the environment's random number generator.\n",
    "\n",
    "        :param seed: Seed value.\n",
    "        \"\"\"\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        return self.data.iloc[self.current_step].values\n",
    "\n",
    "    def step(self, action):\n",
    "        reward = 0\n",
    "        done = False\n",
    "        if action == 0:  # Reduce emissions\n",
    "            reward = -self.data.iloc[self.current_step][\"CO2_ppm\"]\n",
    "        elif action == 1:  # Maintain\n",
    "            reward = -self.data.iloc[self.current_step][\"CO2_ppm\"] * 0.5\n",
    "        elif action == 2:  # Increase (penalty)\n",
    "            reward = -self.data.iloc[self.current_step][\"CO2_ppm\"] * 1.5\n",
    "\n",
    "        self.current_step += 1\n",
    "        if self.current_step >= len(self.data):\n",
    "            done = True\n",
    "        obs = self.data.iloc[self.current_step].values if not done else np.zeros(self.data.shape[1])\n",
    "        return obs, reward, done, {}\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        print(f\"Step: {self.current_step}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Retrieve data from Elasticsearch\n",
    "    print(f\"Retrieving data from Elasticsearch index '{index_name}'...\")\n",
    "    sensor_data = retrieve_data(es, index_name)\n",
    "\n",
    "    if not sensor_data.empty:\n",
    "        # Fill missing values\n",
    "        sensor_data.fillna(0, inplace=True)\n",
    "\n",
    "        # Convert relevant columns to numeric\n",
    "        numeric_columns = [\"CO2_ppm\", \"NO2_ppm\", \"SO2_ppm\", \"Environmental_Score\", \"Social_Score\", \"Governance_Score\"]\n",
    "        for col in numeric_columns:\n",
    "            sensor_data[col] = pd.to_numeric(sensor_data[col], errors='coerce')\n",
    "\n",
    "        # Handle NaN values\n",
    "        sensor_data.fillna(0, inplace=True)\n",
    "\n",
    "        # Normalize the data\n",
    "        sensor_data = sensor_data[numeric_columns]\n",
    "        sensor_data = (sensor_data - sensor_data.min()) / (sensor_data.max() - sensor_data.min())\n",
    "\n",
    "        # Create and train the RL agent\n",
    "        env = make_vec_env(lambda: CarbonFootprintEnv(sensor_data), n_envs=1)\n",
    "        model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "        model.learn(total_timesteps=10000)\n",
    "\n",
    "        # Evaluate the agent\n",
    "        obs = env.reset()\n",
    "        for _ in range(100):\n",
    "            action, _ = model.predict(obs)\n",
    "            obs, rewards, dones, _ = env.step(action)\n",
    "            env.render()\n",
    "    else:\n",
    "        print(\"No data available to train the AI agent.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from gymnasium) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from gymnasium) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from gymnasium) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from gymnasium) (0.0.4)\n"
     ]
    }
   ],
   "source": [
    "! pip install gymnasium\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 5         |\n",
      "|    ep_rew_mean     | -2.03e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 1745      |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 1         |\n",
      "|    total_timesteps | 2048      |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5           |\n",
      "|    ep_rew_mean          | -2.02e+03   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1241        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003695074 |\n",
      "|    clip_fraction        | 0.0169      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 3.96e-05    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.26e+05    |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00138    |\n",
      "|    std                  | 0.999       |\n",
      "|    value_loss           | 1.49e+06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5           |\n",
      "|    ep_rew_mean          | -2.02e+03   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1137        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003941821 |\n",
      "|    clip_fraction        | 0.015       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.04e+05    |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00142    |\n",
      "|    std                  | 0.997       |\n",
      "|    value_loss           | 1.47e+06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5           |\n",
      "|    ep_rew_mean          | -2.01e+03   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1119        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005048196 |\n",
      "|    clip_fraction        | 0.0324      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 5.96e-08    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.83e+05    |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00421    |\n",
      "|    std                  | 0.981       |\n",
      "|    value_loss           | 1.45e+06    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 5            |\n",
      "|    ep_rew_mean          | -2.01e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1103         |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 9            |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047221337 |\n",
      "|    clip_fraction        | 0.00625      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.24e+05     |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00081     |\n",
      "|    std                  | 0.969        |\n",
      "|    value_loss           | 1.43e+06     |\n",
      "------------------------------------------\n",
      "Step: 1, CO2_ppm: 420\n",
      "Step: 2, CO2_ppm: 390\n",
      "Step: 3, CO2_ppm: 410\n",
      "Step: 4, CO2_ppm: 405\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gym import Env\n",
    "from gym.spaces import Box\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "class IoTSensorEnv(Env):\n",
    "    def __init__(self, data):\n",
    "        super(IoTSensorEnv, self).__init__()\n",
    "        self.data = data\n",
    "        self.current_step = 0\n",
    "        self.max_steps = len(data)\n",
    "\n",
    "        # Define action and observation spaces\n",
    "        self.action_space = Box(low=-1, high=1, shape=(1,), dtype=np.float32)  # Example: Adjust CO2 levels\n",
    "        self.observation_space = Box(\n",
    "            low=-np.inf, \n",
    "            high=np.inf, \n",
    "            shape=(data.shape[1],), \n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        return self.data.iloc[self.current_step].values.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        # Get the current data row\n",
    "        current_data = self.data.iloc[self.current_step]\n",
    "\n",
    "        # Simulate the effect of the action\n",
    "        co2 = current_data[\"CO2_ppm\"]\n",
    "        adjusted_co2 = co2 - action[0] * 10  # Example: Action reduces CO2\n",
    "\n",
    "        # Reward inversely proportional to CO2 levels\n",
    "        reward = -np.abs(adjusted_co2)\n",
    "\n",
    "        # Move to the next step\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= self.max_steps\n",
    "\n",
    "        # Observation for the next step\n",
    "        if not done:\n",
    "            obs = self.data.iloc[self.current_step].values.astype(np.float32)\n",
    "        else:\n",
    "            obs = np.zeros(self.data.shape[1], dtype=np.float32)\n",
    "\n",
    "        return obs, reward, done, {}\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        if self.current_step < len(self.data):\n",
    "            print(f\"Step: {self.current_step}, CO2_ppm: {self.data.iloc[self.current_step]['CO2_ppm']}\")\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        np.random.seed(seed)\n",
    "\n",
    "\n",
    "# Example dataset\n",
    "data_dict = {\n",
    "    \"Temperature\": [22, 23, 21, 20, 22],\n",
    "    \"Humidity\": [30, 35, 40, 45, 50],\n",
    "    \"CO2_ppm\": [400, 420, 390, 410, 405]\n",
    "}\n",
    "data = pd.DataFrame(data_dict)\n",
    "\n",
    "# Create the environment\n",
    "env = IoTSensorEnv(data)\n",
    "\n",
    "# Train the PPO agent\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "# Test the trained agent\n",
    "obs = env.reset()\n",
    "for step in range(len(data)):\n",
    "    action, _ = model.predict(obs)\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    env.render()\n",
    "    if done:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving data from Elasticsearch index 'iot_sensor'...\n",
      "Retrieved 200 documents from index 'iot_sensor'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8m/8kxc8bdn5dzfsqxv0r5qsqxw0000gp/T/ipykernel_28393/321522191.py:16: DeprecationWarning: The 'body' parameter is deprecated for the 'search' API and will be removed in a future version. Instead use API parameters directly. See https://github.com/elastic/elasticsearch-py/issues/1698 for more information\n",
      "  response = es_client.search(index=index, body=query, size=1000)\n",
      "/opt/anaconda3/envs/tf_env/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/anaconda3/envs/tf_env/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0/100, score: 0, total_reward: -0.15\n",
      "Optimizations triggered:\n",
      "- Increased epsilon_decay to 0.9960\n",
      "episode: 1/100, score: 14, total_reward: 1.55\n",
      "Optimizations triggered:\n",
      "- Reduced epsilon_decay to 0.9940\n",
      "- Reduced learning_rate to 0.000900\n",
      "episode: 2/100, score: 3, total_reward: 1.05\n",
      "Optimizations triggered:\n",
      "- Reduced epsilon_decay to 0.9920\n",
      "- Reduced learning_rate to 0.000800\n",
      "episode: 3/100, score: 11, total_reward: 0.80\n",
      "Optimizations triggered:\n",
      "- Reduced epsilon_decay to 0.9900\n",
      "- Reduced learning_rate to 0.000700\n",
      "episode: 4/100, score: 12, total_reward: 1.00\n",
      "Optimizations triggered:\n",
      "- Reduced learning_rate to 0.000600\n",
      "episode: 5/100, score: 0, total_reward: -0.15\n",
      "Optimizations triggered:\n",
      "- Reduced learning_rate to 0.000500\n",
      "episode: 6/100, score: 8, total_reward: 1.05\n",
      "episode: 7/100, score: 4, total_reward: -0.45\n",
      "Optimizations triggered:\n",
      "- Increased epsilon_decay to 0.9910\n",
      "- Increased learning_rate to 0.000550\n",
      "episode: 8/100, score: 6, total_reward: 0.75\n",
      "Optimizations triggered:\n",
      "- Increased epsilon_decay to 0.9920\n",
      "- Increased learning_rate to 0.000600\n",
      "episode: 9/100, score: 2, total_reward: 0.05\n",
      "Optimizations triggered:\n",
      "- Increased epsilon_decay to 0.9930\n",
      "- Increased learning_rate to 0.000650\n",
      "episode: 10/100, score: 3, total_reward: 0.70\n",
      "Optimizations triggered:\n",
      "- Reduced epsilon_decay to 0.9910\n",
      "- Reduced learning_rate to 0.000550\n",
      "episode: 11/100, score: 36, total_reward: 4.40\n",
      "Optimizations triggered:\n",
      "- Reduced epsilon_decay to 0.9890\n",
      "- Reduced learning_rate to 0.000450\n",
      "episode: 12/100, score: 1, total_reward: 0.35\n",
      "episode: 13/100, score: 6, total_reward: 1.60\n",
      "episode: 14/100, score: 6, total_reward: 0.25\n",
      "episode: 15/100, score: 0, total_reward: 0.35\n",
      "episode: 16/100, score: 10, total_reward: 1.15\n",
      "episode: 17/100, score: 9, total_reward: 0.80\n",
      "episode: 18/100, score: 6, total_reward: 0.55\n",
      "episode: 19/100, score: 24, total_reward: 1.60\n",
      "episode: 20/100, score: 44, total_reward: 5.05\n",
      "episode: 21/100, score: 20, total_reward: 1.40\n",
      "episode: 22/100, score: 26, total_reward: 4.80\n",
      "episode: 23/100, score: 6, total_reward: 0.75\n",
      "episode: 24/100, score: 15, total_reward: 1.85\n",
      "episode: 25/100, score: 1, total_reward: 0.70\n",
      "episode: 26/100, score: 7, total_reward: 0.10\n",
      "episode: 27/100, score: 6, total_reward: 0.75\n",
      "episode: 28/100, score: 9, total_reward: 1.80\n",
      "episode: 29/100, score: 36, total_reward: 5.95\n",
      "episode: 30/100, score: 11, total_reward: 1.85\n",
      "episode: 31/100, score: 0, total_reward: -0.15\n",
      "episode: 32/100, score: 16, total_reward: 2.05\n",
      "episode: 33/100, score: 1, total_reward: 0.35\n",
      "episode: 34/100, score: 4, total_reward: 0.75\n",
      "episode: 35/100, score: 8, total_reward: 1.30\n",
      "episode: 36/100, score: 10, total_reward: 1.80\n",
      "episode: 37/100, score: 6, total_reward: 0.70\n",
      "episode: 38/100, score: 2, total_reward: 0.20\n",
      "episode: 39/100, score: 33, total_reward: 5.10\n",
      "episode: 40/100, score: 4, total_reward: 0.55\n",
      "episode: 41/100, score: 2, total_reward: 0.70\n",
      "episode: 42/100, score: 3, total_reward: 0.40\n",
      "Optimizations triggered:\n",
      "- Increased epsilon_decay to 0.9900\n",
      "- Increased learning_rate to 0.000500\n",
      "episode: 43/100, score: 1, total_reward: 0.35\n",
      "Optimizations triggered:\n",
      "- Increased epsilon_decay to 0.9910\n",
      "- Increased learning_rate to 0.000550\n",
      "episode: 44/100, score: 0, total_reward: 0.35\n",
      "Optimizations triggered:\n",
      "- Increased epsilon_decay to 0.9920\n",
      "- Increased learning_rate to 0.000600\n",
      "episode: 45/100, score: 3, total_reward: 0.70\n",
      "Optimizations triggered:\n",
      "- Increased epsilon_decay to 0.9930\n",
      "- Increased learning_rate to 0.000650\n",
      "episode: 46/100, score: 16, total_reward: 3.90\n",
      "Optimizations triggered:\n",
      "- Reduced epsilon_decay to 0.9910\n",
      "- Reduced learning_rate to 0.000550\n",
      "episode: 47/100, score: 8, total_reward: 1.80\n",
      "Optimizations triggered:\n",
      "- Reduced epsilon_decay to 0.9890\n",
      "- Reduced learning_rate to 0.000450\n",
      "episode: 48/100, score: 10, total_reward: 1.80\n",
      "episode: 49/100, score: 1, total_reward: 0.70\n",
      "Optimizations triggered:\n",
      "- Increased epsilon_decay to 0.9900\n",
      "- Increased learning_rate to 0.000500\n",
      "episode: 50/100, score: 35, total_reward: 5.95\n",
      "episode: 51/100, score: 4, total_reward: 0.55\n",
      "episode: 52/100, score: 10, total_reward: 2.50\n",
      "episode: 53/100, score: 8, total_reward: 2.30\n",
      "episode: 54/100, score: 3, total_reward: 0.55\n",
      "episode: 55/100, score: 8, total_reward: 1.80\n",
      "episode: 56/100, score: 19, total_reward: 3.45\n",
      "episode: 57/100, score: 1, total_reward: 0.35\n",
      "episode: 58/100, score: 22, total_reward: 5.00\n",
      "episode: 59/100, score: 2, total_reward: 0.55\n",
      "episode: 60/100, score: 29, total_reward: 8.95\n",
      "episode: 61/100, score: 8, total_reward: 1.75\n",
      "episode: 62/100, score: 1, total_reward: 0.70\n",
      "episode: 63/100, score: 11, total_reward: 2.30\n",
      "episode: 64/100, score: 1, total_reward: 0.70\n",
      "episode: 65/100, score: 2, total_reward: 1.05\n",
      "episode: 66/100, score: 0, total_reward: 0.35\n",
      "episode: 67/100, score: 2, total_reward: 0.70\n",
      "episode: 68/100, score: 29, total_reward: 6.40\n",
      "episode: 69/100, score: 8, total_reward: 2.65\n",
      "episode: 70/100, score: 0, total_reward: 0.35\n",
      "episode: 71/100, score: 3, total_reward: 0.90\n",
      "episode: 72/100, score: 4, total_reward: 1.40\n",
      "episode: 73/100, score: 2, total_reward: 0.70\n",
      "Optimizations triggered:\n",
      "- Increased epsilon_decay to 0.9910\n",
      "- Increased learning_rate to 0.000550\n",
      "episode: 74/100, score: 1, total_reward: 0.70\n",
      "Optimizations triggered:\n",
      "- Increased epsilon_decay to 0.9920\n",
      "- Increased learning_rate to 0.000600\n",
      "episode: 75/100, score: 7, total_reward: 1.30\n",
      "Optimizations triggered:\n",
      "- Reduced epsilon_decay to 0.9900\n",
      "- Reduced learning_rate to 0.000500\n",
      "episode: 76/100, score: 2, total_reward: 0.55\n",
      "episode: 77/100, score: 4, total_reward: 1.40\n",
      "episode: 78/100, score: 3, total_reward: 1.40\n",
      "Optimizations triggered:\n",
      "- Increased epsilon_decay to 0.9910\n",
      "- Increased learning_rate to 0.000550\n",
      "episode: 79/100, score: 11, total_reward: 2.70\n",
      "Optimizations triggered:\n",
      "- Increased epsilon_decay to 0.9920\n",
      "- Increased learning_rate to 0.000600\n",
      "episode: 80/100, score: 6, total_reward: 1.10\n",
      "Optimizations triggered:\n",
      "- Increased epsilon_decay to 0.9930\n",
      "- Increased learning_rate to 0.000650\n",
      "episode: 81/100, score: 30, total_reward: 7.90\n",
      "Optimizations triggered:\n",
      "- Reduced epsilon_decay to 0.9910\n",
      "- Reduced learning_rate to 0.000550\n",
      "episode: 82/100, score: 25, total_reward: 5.20\n",
      "Optimizations triggered:\n",
      "- Reduced epsilon_decay to 0.9890\n",
      "- Reduced learning_rate to 0.000450\n",
      "episode: 83/100, score: 26, total_reward: 6.60\n",
      "episode: 84/100, score: 3, total_reward: 0.20\n",
      "episode: 85/100, score: 1, total_reward: 0.70\n",
      "episode: 86/100, score: 4, total_reward: 1.75\n",
      "episode: 87/100, score: 1, total_reward: 0.70\n",
      "episode: 88/100, score: 4, total_reward: 1.40\n",
      "episode: 89/100, score: 4, total_reward: 1.75\n",
      "episode: 90/100, score: 19, total_reward: 5.65\n",
      "episode: 91/100, score: 3, total_reward: 1.05\n",
      "episode: 92/100, score: 1, total_reward: 0.70\n",
      "episode: 93/100, score: 28, total_reward: 6.90\n",
      "episode: 94/100, score: 12, total_reward: 3.85\n",
      "episode: 95/100, score: 13, total_reward: 2.20\n",
      "episode: 96/100, score: 4, total_reward: 1.75\n",
      "episode: 97/100, score: 2, total_reward: 0.20\n",
      "episode: 98/100, score: 0, total_reward: 0.35\n",
      "episode: 99/100, score: 16, total_reward: 3.90\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from tensorflow.keras import models, layers\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "# Elasticsearch connection\n",
    "es = Elasticsearch(\"http://localhost:9200\")\n",
    "index_name = \"iot_sensor\"\n",
    "\n",
    "# Function to retrieve IoT data from Elasticsearch\n",
    "def retrieve_data(es_client, index):\n",
    "    try:\n",
    "        query = {\"query\": {\"match_all\": {}}}\n",
    "        response = es_client.search(index=index, body=query, size=1000)\n",
    "        data = [hit[\"_source\"] for hit in response[\"hits\"][\"hits\"]]\n",
    "        print(f\"Retrieved {len(data)} documents from index '{index}'.\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving data: {e}\")\n",
    "        return []\n",
    "\n",
    "# DQNAgent definition\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "        self.performance_history = []\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = models.Sequential([\n",
    "            layers.Dense(24, input_dim=self.state_size, activation='relu'),\n",
    "            layers.Dense(24, activation='relu'),\n",
    "            layers.Dense(self.action_size, activation='linear')\n",
    "        ])\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate), loss='mse')\n",
    "        return model\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        q_values = self.model.predict(state, verbose=0)\n",
    "        return np.argmax(q_values[0])\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target += self.gamma * np.amax(self.model.predict(next_state, verbose=0)[0])\n",
    "            target_f = self.model.predict(state, verbose=0)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def monitor_and_optimize(self, recent_rewards):\n",
    "        avg_reward = np.mean(recent_rewards)\n",
    "        self.performance_history.append(avg_reward)\n",
    "        optimizations = []\n",
    "        if avg_reward > np.mean(self.performance_history[:-1]):\n",
    "            if self.epsilon_decay > 0.990:\n",
    "                self.epsilon_decay -= 0.002\n",
    "                optimizations.append(f\"Reduced epsilon_decay to {self.epsilon_decay:.4f}\")\n",
    "            if self.learning_rate > 0.0005:\n",
    "                self.learning_rate -= 0.0001\n",
    "                optimizations.append(f\"Reduced learning_rate to {self.learning_rate:.6f}\")\n",
    "        else:\n",
    "            if self.epsilon_decay < 0.999:\n",
    "                self.epsilon_decay += 0.001\n",
    "                optimizations.append(f\"Increased epsilon_decay to {self.epsilon_decay:.4f}\")\n",
    "            if self.learning_rate < 0.001:\n",
    "                self.learning_rate += 0.00005\n",
    "                optimizations.append(f\"Increased learning_rate to {self.learning_rate:.6f}\")\n",
    "        return optimizations\n",
    "\n",
    "# Simulated IoT environment\n",
    "class CarbonEmissionsEnv:\n",
    "    def __init__(self):\n",
    "        self.state = None\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.random.rand(4)\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        emission_reduction = action * 0.1\n",
    "        cost_penalty = (3 - action) * 0.05\n",
    "        efficiency_bonus = 0.2 if action == 2 else 0.0\n",
    "        reward = emission_reduction - cost_penalty + efficiency_bonus\n",
    "        next_state = np.random.rand(4)\n",
    "        done = np.random.rand() < 0.1\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"Retrieving data from Elasticsearch index '{index_name}'...\")\n",
    "    data = retrieve_data(es, index_name)\n",
    "\n",
    "    state_size = 4\n",
    "    action_size = 3\n",
    "    env = CarbonEmissionsEnv()\n",
    "    agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "    episodes = 100\n",
    "    batch_size = 32\n",
    "    reward_history = deque(maxlen=10)\n",
    "\n",
    "    for e in range(episodes):\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "        total_reward = 0\n",
    "        for time in range(500):\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        reward_history.append(total_reward)\n",
    "        optimizations = agent.monitor_and_optimize(reward_history)\n",
    "        print(f\"episode: {e}/{episodes}, score: {time}, total_reward: {total_reward:.2f}\")\n",
    "        if optimizations:\n",
    "            print(\"Optimizations triggered:\")\n",
    "            for opt in optimizations:\n",
    "                print(f\"- {opt}\")\n",
    "        agent.replay(batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=100, poly__degree=1; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=200, poly__degree=1; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=100, poly__degree=2; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=100, poly__degree=2; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=200, poly__degree=1; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=200, poly__degree=1; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=200, poly__degree=1; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=200, poly__degree=1; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=200, poly__degree=2; total time=   0.3s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=200, poly__degree=2; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=200, poly__degree=2; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=200, poly__degree=2; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=200, poly__degree=2; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=100, poly__degree=2; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=100, poly__degree=2; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=100, poly__degree=2; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=100, poly__degree=2; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=100, poly__degree=2; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=200, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=200, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=200, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=200, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=200, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=7, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=7, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=7, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=7, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=7, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=7, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=7, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=7, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=7, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=7, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=7, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=7, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=7, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=7, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=7, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=7, model__n_estimators=200, poly__degree=2; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=7, model__n_estimators=200, poly__degree=2; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=7, model__n_estimators=200, poly__degree=2; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=7, model__n_estimators=200, poly__degree=2; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=7, model__n_estimators=200, poly__degree=2; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=100, poly__degree=2; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=100, poly__degree=2; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=100, poly__degree=2; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=100, poly__degree=2; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=100, poly__degree=2; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=200, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=200, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=200, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=200, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=7, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=7, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=7, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=7, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=7, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=7, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=7, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=7, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=7, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=7, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=7, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=7, model__n_estimators=200, poly__degree=1; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=7, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=7, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=7, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=7, model__n_estimators=200, poly__degree=2; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=7, model__n_estimators=200, poly__degree=2; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=7, model__n_estimators=200, poly__degree=2; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=7, model__n_estimators=200, poly__degree=2; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=7, model__n_estimators=200, poly__degree=2; total time=   0.2s\n",
      "Best parameters: {'model__learning_rate': 0.01, 'model__max_depth': 3, 'model__n_estimators': 100, 'poly__degree': 1}\n",
      "Test MSE: 12.404565642448475\n",
      "Test R2 Score: -0.008844142387979925\n",
      "Cross-validation R2 scores: [-0.01543894 -0.06240309 -0.00396213  0.00301884 -0.03401076]\n",
      "Mean CV R2 score: -0.022559217421238963\n",
      "Scaler saved as 'scaler.pkl'\n",
      "Polynomial transformer saved as 'poly_transform.pkl'\n",
      "XGBoost model saved as 'xgb_model.pkl'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suggested Action: Environmental Score: 80, Social Score: 60, Governance Score: 70. What actions should be taken to improve air quality?\n",
      "\n",
      "The government has taken steps to reduce air pollution in the country. The government is working to increase the number of air-quality monitoring stations in every city and town.\n",
      ". . .\n",
      " (1) The Government of India has announced that it will increase air monitoring of the air in all cities and towns in India. (2) In the last two years\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "from transformers import pipeline, GPT2Tokenizer, GPT2LMHeadModel\n",
    "import joblib\n",
    "\n",
    "# Load dataset\n",
    "data_path = \"/Users/kdn_aikothalavanya/Desktop/KPMG Projects/carbonfootprint/esg_scores.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Data preprocessing\n",
    "data = data[['AQI', 'Environmental_Score', 'Social_Score', 'Governance_Score']].dropna()\n",
    "X = data[['Environmental_Score', 'Social_Score', 'Governance_Score']]\n",
    "y = data['AQI']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Pipeline creation\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('poly', PolynomialFeatures()),\n",
    "    ('model', XGBRegressor(objective='reg:squarederror', random_state=42))\n",
    "])\n",
    "\n",
    "# Hyperparameter tuning\n",
    "param_grid = {\n",
    "    'poly__degree': [1, 2],\n",
    "    'model__n_estimators': [100, 200],\n",
    "    'model__max_depth': [3, 5, 7],\n",
    "    'model__learning_rate': [0.01, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='neg_mean_squared_error', verbose=2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best model\n",
    "best_model = grid_search.best_estimator_\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Evaluation on test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"Test MSE:\", mse)\n",
    "print(\"Test R2 Score:\", r2)\n",
    "\n",
    "# Cross-validation scores\n",
    "cv_scores = cross_val_score(best_model, X_train, y_train, cv=5, scoring='r2')\n",
    "print(\"Cross-validation R2 scores:\", cv_scores)\n",
    "print(\"Mean CV R2 score:\", np.mean(cv_scores))\n",
    "\n",
    "# Save the scaler\n",
    "scaler = best_model.named_steps['scaler']\n",
    "joblib.dump(scaler, \"/Users/kdn_aikothalavanya/Desktop/KPMG Projects/carbonfootprint/scaler.pkl\")\n",
    "print(\"Scaler saved as 'scaler.pkl'\")\n",
    "\n",
    "# Save the PolynomialFeatures transformer\n",
    "poly_transformer = best_model.named_steps['poly']\n",
    "joblib.dump(poly_transformer, \"/Users/kdn_aikothalavanya/Desktop/KPMG Projects/carbonfootprint/poly_transform.pkl\")\n",
    "print(\"Polynomial transformer saved as 'poly_transform.pkl'\")\n",
    "\n",
    "# Save the trained XGBoost model\n",
    "xgb_model = best_model.named_steps['model']\n",
    "joblib.dump(xgb_model, \"/Users/kdn_aikothalavanya/Desktop/KPMG Projects/carbonfootprint/xgb_model.pkl\")\n",
    "print(\"XGBoost model saved as 'xgb_model.pkl'\")\n",
    "\n",
    "# Load the pre-trained GPT-2 model for ESG suggestions\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "def generate_ml_suggestions(environmental_score, social_score, governance_score):\n",
    "    \"\"\"\n",
    "    Generate suggestions for reducing AQI using a local LLM.\n",
    "    \"\"\"\n",
    "    # Format input as a prompt for the LLM\n",
    "    input_prompt = (\n",
    "        f\"Environmental Score: {environmental_score}, \"\n",
    "        f\"Social Score: {social_score}, \"\n",
    "        f\"Governance Score: {governance_score}. \"\n",
    "        f\"What actions should be taken to improve air quality?\"\n",
    "    )\n",
    "\n",
    "    # Tokenize input and generate text\n",
    "    inputs = tokenizer.encode(input_prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(\n",
    "        inputs,\n",
    "        max_length=100,\n",
    "        num_return_sequences=1,\n",
    "        no_repeat_ngram_size=2,\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "    # Decode the output\n",
    "    suggestion = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return suggestion\n",
    "\n",
    "# Example usage for generating suggestions\n",
    "example_esg_scores = {\n",
    "    'Environmental_Score': 80,\n",
    "    'Social_Score': 60,\n",
    "    'Governance_Score': 70\n",
    "}\n",
    "\n",
    "suggestion = generate_ml_suggestions(\n",
    "    environmental_score=example_esg_scores['Environmental_Score'],\n",
    "    social_score=example_esg_scores['Social_Score'],\n",
    "    governance_score=example_esg_scores['Governance_Score']\n",
    ")\n",
    "\n",
    "print(f\"Suggested Action: {suggestion}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=100, poly__degree=2; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=100, poly__degree=2; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=100, poly__degree=2; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=200, poly__degree=1; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=200, poly__degree=2; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=200, poly__degree=2; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=200, poly__degree=2; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=200, poly__degree=2; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=200, poly__degree=2; total time=   0.3s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=200, poly__degree=2; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=100, poly__degree=2; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=100, poly__degree=2; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=100, poly__degree=2; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=100, poly__degree=2; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=100, poly__degree=2; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=200, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=200, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=200, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=200, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=200, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=7, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=7, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=7, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=7, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=7, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=7, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=7, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=7, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=7, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=7, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=7, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=7, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=7, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=7, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=7, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=7, model__n_estimators=200, poly__degree=2; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=7, model__n_estimators=200, poly__degree=2; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=7, model__n_estimators=200, poly__degree=2; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=7, model__n_estimators=200, poly__degree=2; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=7, model__n_estimators=200, poly__degree=2; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=100, poly__degree=2; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=100, poly__degree=2; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=100, poly__degree=2; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=100, poly__degree=2; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=100, poly__degree=2; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=200, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=200, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=200, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=7, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=7, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=7, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=7, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=7, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=7, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=7, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=7, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=7, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=7, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=7, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=7, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=7, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=7, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=7, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=7, model__n_estimators=200, poly__degree=2; total time=   0.4s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=7, model__n_estimators=200, poly__degree=2; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=7, model__n_estimators=200, poly__degree=2; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=7, model__n_estimators=200, poly__degree=2; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=7, model__n_estimators=200, poly__degree=2; total time=   0.2s\n",
      "\n",
      "Best parameters: {'model__learning_rate': 0.01, 'model__max_depth': 3, 'model__n_estimators': 100, 'poly__degree': 1}\n",
      "\n",
      "Test MSE: 12.404565642448475\n",
      "Test R2 Score: -0.008844142387979925\n",
      "\n",
      "Cross-validation R2 scores: [-0.01543894 -0.06240309 -0.00396213  0.00301884 -0.03401076]\n",
      "Mean CV R2 score: -0.022559217421238963\n",
      "\n",
      "Scaler saved as '/Users/kdn_aikothalavanya/Desktop/KPMG Projects/carbonfootprint/scaler.pkl'\n",
      "Polynomial transformer saved as '/Users/kdn_aikothalavanya/Desktop/KPMG Projects/carbonfootprint/poly_transform.pkl'\n",
      "XGBoost model saved as '/Users/kdn_aikothalavanya/Desktop/KPMG Projects/carbonfootprint/xgb_model.pkl'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Suggested Action: Environmental Score: 80, Social Score: 60, Governance Score: 70. What actions should be taken to improve air quality?\n",
      "\n",
      "The government has taken steps to reduce air pollution in the country. The government is working to increase the number of air-quality monitoring stations in every city and town.\n",
      ". . .\n",
      " (1) The Government of India has announced that it will increase air monitoring of the air in all cities and towns in India. (2) In the last two years\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "from transformers import pipeline, GPT2Tokenizer, GPT2LMHeadModel\n",
    "import joblib\n",
    "\n",
    "# Load dataset\n",
    "data_path = \"/Users/kdn_aikothalavanya/Desktop/KPMG Projects/carbonfootprint/esg_scores.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Data preprocessing\n",
    "data = data[['AQI', 'Environmental_Score', 'Social_Score', 'Governance_Score']].dropna()\n",
    "X = data[['Environmental_Score', 'Social_Score', 'Governance_Score']]\n",
    "y = data['AQI']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Pipeline creation\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('poly', PolynomialFeatures()),\n",
    "    ('model', XGBRegressor(objective='reg:squarederror', random_state=42))\n",
    "])\n",
    "\n",
    "# Hyperparameter tuning\n",
    "param_grid = {\n",
    "    'poly__degree': [1, 2],\n",
    "    'model__n_estimators': [100, 200],\n",
    "    'model__max_depth': [3, 5, 7],\n",
    "    'model__learning_rate': [0.01, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='neg_mean_squared_error', verbose=2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best model\n",
    "best_model = grid_search.best_estimator_\n",
    "print(\"\\nBest parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Evaluation on test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"\\nTest MSE:\", mse)\n",
    "print(\"Test R2 Score:\", r2)\n",
    "\n",
    "# Cross-validation scores\n",
    "cv_scores = cross_val_score(best_model, X_train, y_train, cv=5, scoring='r2')\n",
    "print(\"\\nCross-validation R2 scores:\", cv_scores)\n",
    "print(\"Mean CV R2 score:\", np.mean(cv_scores))\n",
    "\n",
    "# Save the scaler\n",
    "scaler = best_model.named_steps['scaler']\n",
    "scaler_path = \"/Users/kdn_aikothalavanya/Desktop/KPMG Projects/carbonfootprint/scaler.pkl\"\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f\"\\nScaler saved as '{scaler_path}'\")\n",
    "\n",
    "# Save the PolynomialFeatures transformer\n",
    "poly_transformer = best_model.named_steps['poly']\n",
    "poly_path = \"/Users/kdn_aikothalavanya/Desktop/KPMG Projects/carbonfootprint/poly_transform.pkl\"\n",
    "joblib.dump(poly_transformer, poly_path)\n",
    "print(f\"Polynomial transformer saved as '{poly_path}'\")\n",
    "\n",
    "# Save the trained XGBoost model\n",
    "xgb_model = best_model.named_steps['model']\n",
    "xgb_path = \"/Users/kdn_aikothalavanya/Desktop/KPMG Projects/carbonfootprint/xgb_model.pkl\"\n",
    "joblib.dump(xgb_model, xgb_path)\n",
    "print(f\"XGBoost model saved as '{xgb_path}'\")\n",
    "\n",
    "# Load the pre-trained GPT-2 model for ESG suggestions\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Ensure proper padding is set\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "def generate_ml_suggestions(environmental_score, social_score, governance_score):\n",
    "    \"\"\"\n",
    "    Generate suggestions for improving air quality based on ESG scores using GPT-2.\n",
    "    \"\"\"\n",
    "    input_prompt = (\n",
    "        f\"Environmental Score: {environmental_score}, \"\n",
    "        f\"Social Score: {social_score}, \"\n",
    "        f\"Governance Score: {governance_score}. \"\n",
    "        f\"What actions should be taken to improve air quality?\"\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer.encode(input_prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(\n",
    "        inputs,\n",
    "        max_length=100,\n",
    "        num_return_sequences=1,\n",
    "        no_repeat_ngram_size=2,\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "    suggestion = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return suggestion\n",
    "\n",
    "# Example usage for generating suggestions\n",
    "example_esg_scores = {\n",
    "    'Environmental_Score': 80,\n",
    "    'Social_Score': 60,\n",
    "    'Governance_Score': 70\n",
    "}\n",
    "\n",
    "suggestion = generate_ml_suggestions(\n",
    "    environmental_score=example_esg_scores['Environmental_Score'],\n",
    "    social_score=example_esg_scores['Social_Score'],\n",
    "    governance_score=example_esg_scores['Governance_Score']\n",
    ")\n",
    "\n",
    "print(f\"\\nSuggested Action: {suggestion}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Downloading textblob-0.19.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting nltk>=3.9 (from textblob)\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: click in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from nltk>=3.9->textblob) (8.1.8)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from nltk>=3.9->textblob) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from nltk>=3.9->textblob) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from nltk>=3.9->textblob) (4.67.1)\n",
      "Downloading textblob-0.19.0-py3-none-any.whl (624 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m624.3/624.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Installing collected packages: nltk, textblob\n",
      "Successfully installed nltk-3.9.1 textblob-0.19.0\n"
     ]
    }
   ],
   "source": [
    "! pip install textblob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=100, poly__degree=1; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=100, poly__degree=1; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=100, poly__degree=2; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=100, poly__degree=2; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=100, poly__degree=2; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=200, poly__degree=1; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=200, poly__degree=2; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=200, poly__degree=2; total time=   0.3s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=200, poly__degree=2; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=200, poly__degree=2; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=200, poly__degree=2; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=200, poly__degree=2; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=200, poly__degree=2; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=200, poly__degree=2; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=200, poly__degree=2; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=100, poly__degree=2; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=100, poly__degree=2; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=100, poly__degree=2; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=100, poly__degree=2; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=200, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=200, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=200, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=200, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=200, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=7, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=7, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=7, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=7, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=7, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=7, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=7, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=7, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=7, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=7, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=7, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=7, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=7, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=7, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=7, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=7, model__n_estimators=200, poly__degree=2; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=7, model__n_estimators=200, poly__degree=2; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=7, model__n_estimators=200, poly__degree=2; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=7, model__n_estimators=200, poly__degree=2; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=7, model__n_estimators=200, poly__degree=2; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=100, poly__degree=2; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=100, poly__degree=2; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=100, poly__degree=2; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=100, poly__degree=2; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=100, poly__degree=2; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=200, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=200, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=200, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=7, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=7, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=7, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=7, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=7, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=7, model__n_estimators=100, poly__degree=2; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=7, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=7, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=7, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=7, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=7, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=7, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=7, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=7, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=7, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=7, model__n_estimators=200, poly__degree=2; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=7, model__n_estimators=200, poly__degree=2; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=7, model__n_estimators=200, poly__degree=2; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=7, model__n_estimators=200, poly__degree=2; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=7, model__n_estimators=200, poly__degree=2; total time=   0.2s\n",
      "\n",
      "Suggested Action: Your ESG scores are moderate. Work on improving governance policies and social initiatives to enhance performance.\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "import joblib\n",
    "\n",
    "# Load dataset\n",
    "data_path = \"/Users/kdn_aikothalavanya/Desktop/KPMG Projects/carbonfootprint/esg_scores.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Data preprocessing\n",
    "data = data[['AQI', 'Environmental_Score', 'Social_Score', 'Governance_Score']].dropna()\n",
    "X = data[['Environmental_Score', 'Social_Score', 'Governance_Score']]\n",
    "y = data['AQI']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Pipeline creation\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('poly', PolynomialFeatures()),\n",
    "    ('model', XGBRegressor(objective='reg:squarederror', random_state=42))\n",
    "])\n",
    "\n",
    "# Hyperparameter tuning\n",
    "param_grid = {\n",
    "    'poly__degree': [1, 2],\n",
    "    'model__n_estimators': [100, 200],\n",
    "    'model__max_depth': [3, 5, 7],\n",
    "    'model__learning_rate': [0.01, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='neg_mean_squared_error', verbose=2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Sentiment analysis function\n",
    "def sentiment_analysis(text):\n",
    "    \"\"\"\n",
    "    Perform sentiment analysis on a given text and return the polarity.\n",
    "    \"\"\"\n",
    "    blob = TextBlob(text)\n",
    "    return blob.sentiment.polarity\n",
    "\n",
    "# Suggestion generator based on ESG scores and sentiment analysis\n",
    "def generate_suggestions(environmental_score, social_score, governance_score):\n",
    "    \"\"\"\n",
    "    Generate suggestions based on ESG scores and sentiment polarity.\n",
    "    \"\"\"\n",
    "    input_prompt = (\n",
    "        f\"Environmental Score: {environmental_score}, \"\n",
    "        f\"Social Score: {social_score}, \"\n",
    "        f\"Governance Score: {governance_score}. \"\n",
    "    )\n",
    "\n",
    "    # Analyze sentiment\n",
    "    sentiment = sentiment_analysis(input_prompt)\n",
    "\n",
    "    # Provide hardcoded suggestions based on sentiment polarity\n",
    "    if sentiment > 0.5:\n",
    "        suggestion = \"Your ESG scores are strong! Focus on maintaining transparency and leveraging clean energy solutions.\"\n",
    "    elif 0 <= sentiment <= 0.5:\n",
    "        suggestion = (\n",
    "            \"Your ESG scores are moderate. Work on improving governance policies and social initiatives to enhance performance.\"\n",
    "        )\n",
    "    else:\n",
    "        suggestion = (\n",
    "            \"Your ESG scores need improvement. Consider reducing emissions, implementing fair labor practices, and improving oversight.\"\n",
    "        )\n",
    "\n",
    "    return suggestion\n",
    "\n",
    "# Example usage for generating suggestions\n",
    "example_esg_scores = {\n",
    "    'Environmental_Score': 80,\n",
    "    'Social_Score': 60,\n",
    "    'Governance_Score': 70\n",
    "}\n",
    "\n",
    "suggestion = generate_suggestions(\n",
    "    environmental_score=example_esg_scores['Environmental_Score'],\n",
    "    social_score=example_esg_scores['Social_Score'],\n",
    "    governance_score=example_esg_scores['Governance_Score']\n",
    ")\n",
    "\n",
    "print(f\"\\nSuggested Action: {suggestion}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=100, poly__degree=2; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=100, poly__degree=2; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=100, poly__degree=2; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=200, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=200, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=200, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=100, poly__degree=1; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=200, poly__degree=1; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=200, poly__degree=1; total time=   0.3s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=200, poly__degree=1; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=200, poly__degree=2; total time=   0.5s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=200, poly__degree=2; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=200, poly__degree=2; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=200, poly__degree=2; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=200, poly__degree=2; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=100, poly__degree=2; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=100, poly__degree=2; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=100, poly__degree=2; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=100, poly__degree=2; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=100, poly__degree=2; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=200, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=200, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=200, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=200, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=200, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=100, poly__degree=2; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=7, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=7, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=7, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=7, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=7, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=7, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=7, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=7, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=7, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=7, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=7, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=7, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=7, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=7, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=7, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=7, model__n_estimators=200, poly__degree=2; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=7, model__n_estimators=200, poly__degree=2; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=7, model__n_estimators=200, poly__degree=2; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=7, model__n_estimators=200, poly__degree=2; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=7, model__n_estimators=200, poly__degree=2; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=100, poly__degree=2; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=100, poly__degree=2; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=100, poly__degree=2; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=100, poly__degree=2; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=100, poly__degree=2; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=200, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=200, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=200, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=200, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=200, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=200, poly__degree=2; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=100, poly__degree=1; total time=   0.0s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=200, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=7, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=7, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=7, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=7, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=7, model__n_estimators=100, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=7, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=7, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=7, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=7, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=7, model__n_estimators=100, poly__degree=2; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=7, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=7, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=7, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=7, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=7, model__n_estimators=200, poly__degree=1; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=7, model__n_estimators=200, poly__degree=2; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=7, model__n_estimators=200, poly__degree=2; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=7, model__n_estimators=200, poly__degree=2; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=7, model__n_estimators=200, poly__degree=2; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=7, model__n_estimators=200, poly__degree=2; total time=   0.2s\n",
      "Retrieving data from Elasticsearch index 'iot_sensor'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8m/8kxc8bdn5dzfsqxv0r5qsqxw0000gp/T/ipykernel_28393/3772921622.py:24: DeprecationWarning: The 'body' parameter is deprecated for the 'search' API and will be removed in a future version. Instead use API parameters directly. See https://github.com/elastic/elasticsearch-py/issues/1698 for more information\n",
      "  response = es_client.search(index=index, body=query, size=1000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 200 documents from index 'iot_sensor'.\n",
      "\n",
      "Suggested Action: Your ESG scores are moderate. Work on improving governance policies and social initiatives to enhance performance.\n",
      "Episode 1/100 completed.\n",
      "Episode 2/100 completed.\n",
      "Episode 3/100 completed.\n",
      "Episode 4/100 completed.\n",
      "Episode 5/100 completed.\n",
      "Episode 6/100 completed.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_env/lib/python3.10/site-packages/tensorflow/python/ops/array_ops.py:5306\u001b[0m, in \u001b[0;36mgather\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   5303\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   5304\u001b[0m   \u001b[38;5;66;03m# TODO(apassos) find a less bad way of detecting resource variables\u001b[39;00m\n\u001b[1;32m   5305\u001b[0m   \u001b[38;5;66;03m# without introducing a circular dependency.\u001b[39;00m\n\u001b[0;32m-> 5306\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse_read\u001b[49m(indices, name\u001b[38;5;241m=\u001b[39mname)\n\u001b[1;32m   5307\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_env/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:444\u001b[0m, in \u001b[0;36mTensor.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    438\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m    439\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;124m    If you are looking for numpy-related methods, please run the following:\u001b[39m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;124m    from tensorflow.python.ops.numpy_ops import np_config\u001b[39m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;124m    np_config.enable_numpy_behavior()\u001b[39m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;124m  \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m)\n\u001b[0;32m--> 444\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'sparse_read'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 196\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m    195\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpisode \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisodes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m completed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[54], line 151\u001b[0m, in \u001b[0;36mDQNAgent.replay\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m    149\u001b[0m target \u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m--> 151\u001b[0m     target \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mamax(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    152\u001b[0m target_f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpredict(state, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    153\u001b[0m target_f[\u001b[38;5;241m0\u001b[39m][action] \u001b[38;5;241m=\u001b[39m target\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_env/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_env/lib/python3.10/site-packages/keras/engine/training.py:2317\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   2308\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m   2309\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   2310\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing Model.predict with MultiWorkerMirroredStrategy \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2311\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor TPUStrategy and AutoShardPolicy.FILE might lead to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2314\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m   2315\u001b[0m         )\n\u001b[0;32m-> 2317\u001b[0m data_handler \u001b[38;5;241m=\u001b[39m \u001b[43mdata_adapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_data_handler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2318\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2320\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2321\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2322\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2323\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2325\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2327\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_execution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_steps_per_execution\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2328\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2330\u001b[0m \u001b[38;5;66;03m# Container that configures and calls `tf.keras.Callback`s.\u001b[39;00m\n\u001b[1;32m   2331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(callbacks, callbacks_module\u001b[38;5;241m.\u001b[39mCallbackList):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_env/lib/python3.10/site-packages/keras/engine/data_adapter.py:1579\u001b[0m, in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cluster_coordinator\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _ClusterCoordinatorDataHandler(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 1579\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataHandler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_env/lib/python3.10/site-packages/keras/engine/data_adapter.py:1259\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[1;32m   1256\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution \u001b[38;5;241m=\u001b[39m steps_per_execution\n\u001b[1;32m   1258\u001b[0m adapter_cls \u001b[38;5;241m=\u001b[39m select_data_adapter(x, y)\n\u001b[0;32m-> 1259\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_adapter \u001b[38;5;241m=\u001b[39m \u001b[43madapter_cls\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1260\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1261\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1262\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1263\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1265\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1269\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistribution_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_strategy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1272\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1274\u001b[0m strategy \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mget_strategy()\n\u001b[1;32m   1276\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_env/lib/python3.10/site-packages/keras/engine/data_adapter.py:347\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m flat_dataset\n\u001b[1;32m    345\u001b[0m indices_dataset \u001b[38;5;241m=\u001b[39m indices_dataset\u001b[38;5;241m.\u001b[39mflat_map(slice_batch_indices)\n\u001b[0;32m--> 347\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mslice_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mshuffle_batch\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_env/lib/python3.10/site-packages/keras/engine/data_adapter.py:388\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.slice_inputs\u001b[0;34m(self, indices_dataset, inputs)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgrab_batch\u001b[39m(i, data):\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[1;32m    385\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m d: tf\u001b[38;5;241m.\u001b[39mgather(d, i, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), data\n\u001b[1;32m    386\u001b[0m     )\n\u001b[0;32m--> 388\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrab_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_parallel_calls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAUTOTUNE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;66;03m# Default optimizations are disabled to avoid the overhead of\u001b[39;00m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;66;03m# (unnecessary) input pipeline graph serialization and deserialization\u001b[39;00m\n\u001b[1;32m    392\u001b[0m options \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mOptions()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_env/lib/python3.10/site-packages/tensorflow/python/data/ops/dataset_ops.py:2296\u001b[0m, in \u001b[0;36mDatasetV2.map\u001b[0;34m(self, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[1;32m   2294\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m MapDataset(\u001b[38;5;28mself\u001b[39m, map_func, preserve_cardinality\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, name\u001b[38;5;241m=\u001b[39mname)\n\u001b[1;32m   2295\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2296\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mParallelMapDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2297\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2298\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmap_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2299\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_parallel_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2300\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2301\u001b[0m \u001b[43m      \u001b[49m\u001b[43mpreserve_cardinality\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2302\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_env/lib/python3.10/site-packages/tensorflow/python/data/ops/dataset_ops.py:5540\u001b[0m, in \u001b[0;36mParallelMapDataset.__init__\u001b[0;34m(self, input_dataset, map_func, num_parallel_calls, deterministic, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\u001b[0m\n\u001b[1;32m   5538\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_dataset \u001b[38;5;241m=\u001b[39m input_dataset\n\u001b[1;32m   5539\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_inter_op_parallelism \u001b[38;5;241m=\u001b[39m use_inter_op_parallelism\n\u001b[0;32m-> 5540\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_func \u001b[38;5;241m=\u001b[39m \u001b[43mstructured_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStructuredFunctionWrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmap_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5542\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transformation_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5543\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5544\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_legacy_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_legacy_function\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5545\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m deterministic \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5546\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deterministic \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_env/lib/python3.10/site-packages/tensorflow/python/data/ops/structured_function.py:263\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m    256\u001b[0m       warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    257\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEven though the `tf.config.experimental_run_functions_eagerly` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    258\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moption is set, this option does not apply to tf.data functions. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    259\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo force eager execution of tf.data functions, please use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    260\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.data.experimental.enable_debug_mode()`.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    261\u001b[0m     fn_factory \u001b[38;5;241m=\u001b[39m trace_tf_function(defun_kwargs)\n\u001b[0;32m--> 263\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function \u001b[38;5;241m=\u001b[39m \u001b[43mfn_factory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# There is no graph to add in eager mode.\u001b[39;00m\n\u001b[1;32m    265\u001b[0m add_to_graph \u001b[38;5;241m&\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_env/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:226\u001b[0m, in \u001b[0;36mTracingCompiler.get_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_concrete_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    218\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Returns a `ConcreteFunction` specialized to inputs and execution context.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \n\u001b[1;32m    220\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;124;03m      `tf.Tensor` or `tf.TensorSpec`.\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 226\u001b[0m   concrete_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_concrete_function_garbage_collected\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m   concrete_function\u001b[38;5;241m.\u001b[39m_garbage_collector\u001b[38;5;241m.\u001b[39mrelease()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    229\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m concrete_function\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_env/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:192\u001b[0m, in \u001b[0;36mTracingCompiler._get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_spec\u001b[38;5;241m.\u001b[39mvalidate_inputs_with_signature(args, kwargs)\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m--> 192\u001b[0m   concrete_function, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_define_concrete_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m   seen_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m    194\u001b[0m   captured \u001b[38;5;241m=\u001b[39m object_identity\u001b[38;5;241m.\u001b[39mObjectIdentitySet(\n\u001b[1;32m    195\u001b[0m       concrete_function\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39minternal_captures)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_env/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:157\u001b[0m, in \u001b[0;36mTracingCompiler._maybe_define_concrete_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m   args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_signature\n\u001b[1;32m    155\u001b[0m   kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 157\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_define_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_env/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:360\u001b[0m, in \u001b[0;36mTracingCompiler._maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    357\u001b[0m   \u001b[38;5;66;03m# Only get placeholders for arguments, not captures\u001b[39;00m\n\u001b[1;32m    358\u001b[0m   args, kwargs \u001b[38;5;241m=\u001b[39m generalized_func_key\u001b[38;5;241m.\u001b[39m_placeholder_value()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m--> 360\u001b[0m concrete_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_concrete_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    362\u001b[0m graph_capture_container \u001b[38;5;241m=\u001b[39m concrete_function\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39m_capture_func_lib  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;66;03m# Maintain the list of all captures\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_env/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:284\u001b[0m, in \u001b[0;36mTracingCompiler._create_concrete_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m missing_arg_names \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (arg, i) \u001b[38;5;28;01mfor\u001b[39;00m i, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(missing_arg_names)\n\u001b[1;32m    281\u001b[0m ]\n\u001b[1;32m    282\u001b[0m arg_names \u001b[38;5;241m=\u001b[39m base_arg_names \u001b[38;5;241m+\u001b[39m missing_arg_names\n\u001b[1;32m    283\u001b[0m concrete_function \u001b[38;5;241m=\u001b[39m monomorphic_function\u001b[38;5;241m.\u001b[39mConcreteFunction(\n\u001b[0;32m--> 284\u001b[0m     \u001b[43mfunc_graph_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc_graph_from_py_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_python_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautograph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_autograph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautograph_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_autograph_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m        \u001b[49m\u001b[43marg_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marg_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapture_by_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_capture_by_value\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_attributes,\n\u001b[1;32m    295\u001b[0m     spec\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_spec,\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;66;03m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[39;00m\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;66;03m# scope. This is not the default behavior since it gets used in some\u001b[39;00m\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;66;03m# places (like Keras) where the FuncGraph lives longer than the\u001b[39;00m\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;66;03m# ConcreteFunction.\u001b[39;00m\n\u001b[1;32m    300\u001b[0m     shared_func_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m concrete_function\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_env/lib/python3.10/site-packages/tensorflow/python/framework/func_graph.py:1283\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1280\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1281\u001b[0m   _, original_func \u001b[38;5;241m=\u001b[39m tf_decorator\u001b[38;5;241m.\u001b[39munwrap(python_func)\n\u001b[0;32m-> 1283\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mpython_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfunc_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfunc_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[38;5;66;03m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[1;32m   1286\u001b[0m \u001b[38;5;66;03m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[1;32m   1287\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m variable_utils\u001b[38;5;241m.\u001b[39mconvert_variables_to_tensors(func_outputs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_env/lib/python3.10/site-packages/tensorflow/python/data/ops/structured_function.py:240\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__.<locals>.trace_tf_function.<locals>.wrapped_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;129m@eager_function\u001b[39m\u001b[38;5;241m.\u001b[39mdefun_with_attributes(\n\u001b[1;32m    235\u001b[0m     input_signature\u001b[38;5;241m=\u001b[39mstructure\u001b[38;5;241m.\u001b[39mget_flat_tensor_specs(\n\u001b[1;32m    236\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_structure),\n\u001b[1;32m    237\u001b[0m     autograph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    238\u001b[0m     attributes\u001b[38;5;241m=\u001b[39mdefun_kwargs)\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped_fn\u001b[39m(\u001b[38;5;241m*\u001b[39margs):  \u001b[38;5;66;03m# pylint: disable=missing-docstring\u001b[39;00m\n\u001b[0;32m--> 240\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mwrapper_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m   ret \u001b[38;5;241m=\u001b[39m structure\u001b[38;5;241m.\u001b[39mto_tensor_list(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_structure, ret)\n\u001b[1;32m    242\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m [ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m ret]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_env/lib/python3.10/site-packages/tensorflow/python/data/ops/structured_function.py:171\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__.<locals>.wrapper_helper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _should_unpack(nested_args):\n\u001b[1;32m    170\u001b[0m   nested_args \u001b[38;5;241m=\u001b[39m (nested_args,)\n\u001b[0;32m--> 171\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[43mautograph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtf_convert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag_ctx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnested_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m ret \u001b[38;5;241m=\u001b[39m variable_utils\u001b[38;5;241m.\u001b[39mconvert_variables_to_tensors(ret)\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _should_pack(ret):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_env/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:689\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    688\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m conversion_ctx:\n\u001b[0;32m--> 689\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m    691\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_env/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:377\u001b[0m, in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    374\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options)\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m options\u001b[38;5;241m.\u001b[39muser_requested \u001b[38;5;129;01mand\u001b[39;00m conversion\u001b[38;5;241m.\u001b[39mis_allowlisted(f):\n\u001b[0;32m--> 377\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_call_unconverted\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;66;03m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;66;03m# call conversion from generated code while in nonrecursive mode. In that\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;66;03m# case we evidently don't want to recurse, but we still have to convert\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;66;03m# things like builtins.\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m options\u001b[38;5;241m.\u001b[39minternal_convert_user_code:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_env/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:458\u001b[0m, in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    455\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__self__\u001b[39m\u001b[38;5;241m.\u001b[39mcall(args, kwargs)\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 458\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_env/lib/python3.10/site-packages/keras/engine/data_adapter.py:384\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.slice_inputs.<locals>.grab_batch\u001b[0;34m(i, data)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgrab_batch\u001b[39m(i, data):\n\u001b[0;32m--> 384\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgather\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_env/lib/python3.10/site-packages/tensorflow/python/util/nest.py:917\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    913\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m    914\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 917\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [func(\u001b[38;5;241m*\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m    918\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_env/lib/python3.10/site-packages/tensorflow/python/util/nest.py:917\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    913\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m    914\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 917\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m    918\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_env/lib/python3.10/site-packages/keras/engine/data_adapter.py:385\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.slice_inputs.<locals>.grab_batch.<locals>.<lambda>\u001b[0;34m(d)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgrab_batch\u001b[39m(i, data):\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[0;32m--> 385\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m d: \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgather\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m, data\n\u001b[1;32m    386\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_env/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_env/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1175\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1176\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch_target\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m   1178\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1179\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1180\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_env/lib/python3.10/site-packages/tensorflow/python/ops/array_ops.py:5319\u001b[0m, in \u001b[0;36mgather_v2\u001b[0;34m(params, indices, validate_indices, axis, batch_dims, name)\u001b[0m\n\u001b[1;32m   5311\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgather\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[1;32m   5312\u001b[0m \u001b[38;5;129m@dispatch\u001b[39m\u001b[38;5;241m.\u001b[39madd_dispatch_support\n\u001b[1;32m   5313\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgather_v2\u001b[39m(params,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5317\u001b[0m               batch_dims\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   5318\u001b[0m               name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m-> 5319\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgather\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5320\u001b[0m \u001b[43m      \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5321\u001b[0m \u001b[43m      \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5322\u001b[0m \u001b[43m      \u001b[49m\u001b[43mvalidate_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5323\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5324\u001b[0m \u001b[43m      \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5325\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbatch_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_dims\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_env/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_env/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1175\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1176\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch_target\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m   1178\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1179\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1180\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_env/lib/python3.10/site-packages/tensorflow/python/util/deprecation.py:561\u001b[0m, in \u001b[0;36mdeprecated_args.<locals>.deprecated_wrapper.<locals>.new_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    553\u001b[0m         _PRINTED_WARNING[(func, arg_name)] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    554\u001b[0m       logging\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    555\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrom \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: calling \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m (from \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) with \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is deprecated and will \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    556\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbe removed \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mInstructions for updating:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    559\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124min a future version\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m date \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m date),\n\u001b[1;32m    560\u001b[0m           instructions)\n\u001b[0;32m--> 561\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_env/lib/python3.10/site-packages/tensorflow/python/ops/array_ops.py:5308\u001b[0m, in \u001b[0;36mgather\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   5306\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m params\u001b[38;5;241m.\u001b[39msparse_read(indices, name\u001b[38;5;241m=\u001b[39mname)\n\u001b[1;32m   5307\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[0;32m-> 5308\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgen_array_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgather_v2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_env/lib/python3.10/site-packages/tensorflow/python/ops/gen_array_ops.py:3957\u001b[0m, in \u001b[0;36mgather_v2\u001b[0;34m(params, indices, axis, batch_dims, name)\u001b[0m\n\u001b[1;32m   3955\u001b[0m   batch_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3956\u001b[0m batch_dims \u001b[38;5;241m=\u001b[39m _execute\u001b[38;5;241m.\u001b[39mmake_int(batch_dims, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_dims\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 3957\u001b[0m _, _, _op, _outputs \u001b[38;5;241m=\u001b[39m \u001b[43m_op_def_library\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_op_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3958\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGatherV2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3959\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mbatch_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3960\u001b[0m _result \u001b[38;5;241m=\u001b[39m _outputs[:]\n\u001b[1;32m   3961\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _execute\u001b[38;5;241m.\u001b[39mmust_record_gradient():\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_env/lib/python3.10/site-packages/tensorflow/python/framework/op_def_library.py:777\u001b[0m, in \u001b[0;36m_apply_op_helper\u001b[0;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    775\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m g\u001b[38;5;241m.\u001b[39mas_default(), ops\u001b[38;5;241m.\u001b[39mname_scope(name) \u001b[38;5;28;01mas\u001b[39;00m scope:\n\u001b[1;32m    776\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m fallback:\n\u001b[0;32m--> 777\u001b[0m     \u001b[43m_ExtractInputsAndAttrs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop_type_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_def\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallowed_list_attr_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    778\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mkeywords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_type_attr_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[43m                           \u001b[49m\u001b[43minput_types\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    780\u001b[0m     _ExtractRemainingAttrs(op_type_name, op_def, keywords,\n\u001b[1;32m    781\u001b[0m                            default_type_attr_map, attrs)\n\u001b[1;32m    782\u001b[0m     _ExtractAttrProto(op_type_name, op_def, attrs, attr_protos)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_env/lib/python3.10/site-packages/tensorflow/python/framework/op_def_library.py:550\u001b[0m, in \u001b[0;36m_ExtractInputsAndAttrs\u001b[0;34m(op_type_name, op_def, allowed_list_attr_map, keywords, default_type_attr_map, attrs, inputs, input_types)\u001b[0m\n\u001b[1;32m    544\u001b[0m       values \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(\n\u001b[1;32m    545\u001b[0m           values,\n\u001b[1;32m    546\u001b[0m           name\u001b[38;5;241m=\u001b[39minput_arg\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    547\u001b[0m           as_ref\u001b[38;5;241m=\u001b[39minput_arg\u001b[38;5;241m.\u001b[39mis_ref,\n\u001b[1;32m    548\u001b[0m           preferred_dtype\u001b[38;5;241m=\u001b[39mdefault_dtype)\n\u001b[1;32m    549\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 550\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_arg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m        \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_arg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_ref\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreferred_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    557\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_env/lib/python3.10/site-packages/tensorflow/python/profiler/trace.py:183\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m Trace(trace_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrace_kwargs):\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_env/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1636\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1627\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1628\u001b[0m           _add_error_prefix(\n\u001b[1;32m   1629\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConversion function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconversion_func\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m for type \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1632\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactual = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mret\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mbase_dtype\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1633\u001b[0m               name\u001b[38;5;241m=\u001b[39mname))\n\u001b[1;32m   1635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1636\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mconversion_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_ref\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[1;32m   1639\u001b[0m   \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_env/lib/python3.10/site-packages/tensorflow/python/framework/tensor_conversion_registry.py:48\u001b[0m, in \u001b[0;36m_default_conversion_function\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_default_conversion_function\u001b[39m(value, dtype, name, as_ref):\n\u001b[1;32m     47\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m as_ref  \u001b[38;5;66;03m# Unused.\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconstant_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_env/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:267\u001b[0m, in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mconstant\u001b[39m(value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConst\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    172\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \n\u001b[1;32m    174\u001b[0m \u001b[38;5;124;03m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;124;03m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mallow_broadcast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_env/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:289\u001b[0m, in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    287\u001b[0m dtype_value \u001b[38;5;241m=\u001b[39m attr_value_pb2\u001b[38;5;241m.\u001b[39mAttrValue(\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39mtensor_value\u001b[38;5;241m.\u001b[39mtensor\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    288\u001b[0m attrs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: tensor_value, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: dtype_value}\n\u001b[0;32m--> 289\u001b[0m const_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_op_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mConst\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mdtype_value\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39moutputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m op_callbacks\u001b[38;5;241m.\u001b[39mshould_invoke_op_callbacks():\n\u001b[1;32m    293\u001b[0m   \u001b[38;5;66;03m# TODO(b/147670703): Once the special-op creation code paths\u001b[39;00m\n\u001b[1;32m    294\u001b[0m   \u001b[38;5;66;03m# are unified. Remove this `if` block.\u001b[39;00m\n\u001b[1;32m    295\u001b[0m   callback_outputs \u001b[38;5;241m=\u001b[39m op_callbacks\u001b[38;5;241m.\u001b[39minvoke_op_callbacks(\n\u001b[1;32m    296\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConst\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mtuple\u001b[39m(), attrs, (const_tensor,), op_name\u001b[38;5;241m=\u001b[39mname, graph\u001b[38;5;241m=\u001b[39mg)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_env/lib/python3.10/site-packages/tensorflow/python/framework/func_graph.py:749\u001b[0m, in \u001b[0;36mFuncGraph._create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m    747\u001b[0m   inp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcapture(inp)\n\u001b[1;32m    748\u001b[0m   captured_inputs\u001b[38;5;241m.\u001b[39mappend(inp)\n\u001b[0;32m--> 749\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mFuncGraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_op_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mop_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_types\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_def\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_device\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_env/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:3798\u001b[0m, in \u001b[0;36mGraph._create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m   3795\u001b[0m \u001b[38;5;66;03m# _create_op_helper mutates the new Operation. `_mutation_lock` ensures a\u001b[39;00m\n\u001b[1;32m   3796\u001b[0m \u001b[38;5;66;03m# Session.run call cannot occur between creating and mutating the op.\u001b[39;00m\n\u001b[1;32m   3797\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mutation_lock():\n\u001b[0;32m-> 3798\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mOperation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3799\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnode_def\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3800\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3801\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3802\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3803\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcontrol_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontrol_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3804\u001b[0m \u001b[43m      \u001b[49m\u001b[43minput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3805\u001b[0m \u001b[43m      \u001b[49m\u001b[43moriginal_op\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_default_original_op\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[43m      \u001b[49m\u001b[43mop_def\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mop_def\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3807\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_op_helper(ret, compute_device\u001b[38;5;241m=\u001b[39mcompute_device)\n\u001b[1;32m   3808\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_env/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:2106\u001b[0m, in \u001b[0;36mOperation.__init__\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   2103\u001b[0m     control_input_ops\u001b[38;5;241m.\u001b[39mappend(control_op)\n\u001b[1;32m   2105\u001b[0m \u001b[38;5;66;03m# Initialize c_op from node_def and other inputs\u001b[39;00m\n\u001b[0;32m-> 2106\u001b[0m c_op \u001b[38;5;241m=\u001b[39m \u001b[43m_create_c_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_def\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontrol_input_ops\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_def\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mop_def\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2107\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_from_c_op(c_op\u001b[38;5;241m=\u001b[39mc_op, g\u001b[38;5;241m=\u001b[39mg)\n\u001b[1;32m   2109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_op \u001b[38;5;241m=\u001b[39m original_op\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_env/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_env/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1957\u001b[0m, in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs, op_def, extract_traceback)\u001b[0m\n\u001b[1;32m   1953\u001b[0m \u001b[38;5;66;03m# pylint: enable=protected-access\u001b[39;00m\n\u001b[1;32m   1954\u001b[0m \n\u001b[1;32m   1955\u001b[0m \u001b[38;5;66;03m# Add attrs\u001b[39;00m\n\u001b[1;32m   1956\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, attr_value \u001b[38;5;129;01min\u001b[39;00m node_def\u001b[38;5;241m.\u001b[39mattr\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m-> 1957\u001b[0m   serialized \u001b[38;5;241m=\u001b[39m \u001b[43mattr_value\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSerializeToString\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1958\u001b[0m   \u001b[38;5;66;03m# TODO(skyewm): this creates and deletes a new TF_Status for every attr.\u001b[39;00m\n\u001b[1;32m   1959\u001b[0m   \u001b[38;5;66;03m# It might be worth creating a convenient way to re-use the same status.\u001b[39;00m\n\u001b[1;32m   1960\u001b[0m   pywrap_tf_session\u001b[38;5;241m.\u001b[39mTF_SetAttrValueProto(op_desc, compat\u001b[38;5;241m.\u001b[39mas_str(name),\n\u001b[1;32m   1961\u001b[0m                                          serialized)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "from elasticsearch import Elasticsearch\n",
    "from tensorflow.keras import models, layers\n",
    "from collections import deque\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "\n",
    "# Elasticsearch connection\n",
    "es = Elasticsearch(\"http://localhost:9200\")\n",
    "index_name = \"iot_sensor\"\n",
    "\n",
    "# Function to retrieve IoT data from Elasticsearch\n",
    "def retrieve_data(es_client, index):\n",
    "    try:\n",
    "        query = {\"query\": {\"match_all\": {}}}\n",
    "        response = es_client.search(index=index, body=query, size=1000)\n",
    "        data = [hit[\"_source\"] for hit in response[\"hits\"][\"hits\"]]\n",
    "        print(f\"Retrieved {len(data)} documents from index '{index}'.\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving data: {e}\")\n",
    "        return []\n",
    "\n",
    "# Load ESG dataset\n",
    "data_path = \"/Users/kdn_aikothalavanya/Desktop/KPMG Projects/carbonfootprint/esg_scores.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Data preprocessing\n",
    "data = data[['AQI', 'Environmental_Score', 'Social_Score', 'Governance_Score']].dropna()\n",
    "X = data[['Environmental_Score', 'Social_Score', 'Governance_Score']]\n",
    "y = data['AQI']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Pipeline creation\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('poly', PolynomialFeatures()),\n",
    "    ('model', XGBRegressor(objective='reg:squarederror', random_state=42))\n",
    "])\n",
    "\n",
    "# Hyperparameter tuning\n",
    "param_grid = {\n",
    "    'poly__degree': [1, 2],\n",
    "    'model__n_estimators': [100, 200],\n",
    "    'model__max_depth': [3, 5, 7],\n",
    "    'model__learning_rate': [0.01, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='neg_mean_squared_error', verbose=2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Sentiment analysis function\n",
    "def sentiment_analysis(text):\n",
    "    blob = TextBlob(text)\n",
    "    return blob.sentiment.polarity\n",
    "\n",
    "# Suggestion generator based on ESG scores and sentiment analysis\n",
    "def generate_suggestions(environmental_score, social_score, governance_score):\n",
    "    input_prompt = (\n",
    "        f\"Environmental Score: {environmental_score}, \"\n",
    "        f\"Social Score: {social_score}, \"\n",
    "        f\"Governance Score: {governance_score}. \"\n",
    "    )\n",
    "\n",
    "    # Analyze sentiment\n",
    "    sentiment = sentiment_analysis(input_prompt)\n",
    "\n",
    "    # Provide hardcoded suggestions based on sentiment polarity\n",
    "    if sentiment > 0.5:\n",
    "        suggestion = \"Your ESG scores are strong! Focus on maintaining transparency and leveraging clean energy solutions.\"\n",
    "    elif 0 <= sentiment <= 0.5:\n",
    "        suggestion = (\n",
    "            \"Your ESG scores are moderate. Work on improving governance policies and social initiatives to enhance performance.\"\n",
    "        )\n",
    "    else:\n",
    "        suggestion = (\n",
    "            \"Your ESG scores need improvement. Consider reducing emissions, implementing fair labor practices, and improving oversight.\"\n",
    "        )\n",
    "\n",
    "    return suggestion\n",
    "\n",
    "# Simulated IoT environment for carbon emissions\n",
    "class CarbonEmissionsEnv:\n",
    "    def __init__(self):\n",
    "        self.state = None\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.random.rand(4)\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        emission_reduction = action * 0.1\n",
    "        cost_penalty = (3 - action) * 0.05\n",
    "        efficiency_bonus = 0.2 if action == 2 else 0.0\n",
    "        reward = emission_reduction - cost_penalty + efficiency_bonus\n",
    "        next_state = np.random.rand(4)\n",
    "        done = np.random.rand() < 0.1\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "# DQNAgent definition\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = models.Sequential([\n",
    "            layers.Dense(24, input_dim=self.state_size, activation='relu'),\n",
    "            layers.Dense(24, activation='relu'),\n",
    "            layers.Dense(self.action_size, activation='linear')\n",
    "        ])\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate), loss='mse')\n",
    "        return model\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        q_values = self.model.predict(state, verbose=0)\n",
    "        return np.argmax(q_values[0])\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target += self.gamma * np.amax(self.model.predict(next_state, verbose=0)[0])\n",
    "            target_f = self.model.predict(state, verbose=0)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"Retrieving data from Elasticsearch index '{index_name}'...\")\n",
    "    data = retrieve_data(es, index_name)\n",
    "\n",
    "    # ESG example usage\n",
    "    example_esg_scores = {\n",
    "        'Environmental_Score': 80,\n",
    "        'Social_Score': 60,\n",
    "        'Governance_Score': 70\n",
    "    }\n",
    "    suggestion = generate_suggestions(\n",
    "        environmental_score=example_esg_scores['Environmental_Score'],\n",
    "        social_score=example_esg_scores['Social_Score'],\n",
    "        governance_score=example_esg_scores['Governance_Score']\n",
    "    )\n",
    "    print(f\"\\nSuggested Action: {suggestion}\")\n",
    "\n",
    "    # Reinforcement learning\n",
    "    state_size = 4\n",
    "    action_size = 3\n",
    "    env = CarbonEmissionsEnv()\n",
    "    agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "    episodes = 100\n",
    "    batch_size = 32\n",
    "\n",
    "    for e in range(episodes):\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "        for time in range(500):\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "        agent.replay(batch_size)\n",
    "        print(f\"Episode {e+1}/{episodes} completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving data from Elasticsearch index 'iot_sensor'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8m/8kxc8bdn5dzfsqxv0r5qsqxw0000gp/T/ipykernel_28393/2536325267.py:74: DeprecationWarning: The 'body' parameter is deprecated for the 'search' API and will be removed in a future version. Instead use API parameters directly. See https://github.com/elastic/elasticsearch-py/issues/1698 for more information\n",
      "  results = es.search(index=index_name, body={\"query\": {\"match_all\": {}}}, size=1000)\n",
      "/var/folders/8m/8kxc8bdn5dzfsqxv0r5qsqxw0000gp/T/ipykernel_28393/2536325267.py:114: RuntimeWarning: invalid value encountered in divide\n",
      "  states = (states - np.min(states, axis=0)) / (np.max(states, axis=0) - np.min(states, axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State 1: Action Taken: 1, Reward: 5.00, Suggestion: Maintain current efforts, but monitor for opportunities to improve efficiency.\n",
      "State 2: Action Taken: 1, Reward: 5.00, Suggestion: Maintain current efforts, but monitor for opportunities to improve efficiency.\n",
      "State 3: Action Taken: 2, Reward: -5.00, Suggestion: Increase efforts towards emission reduction to meet sustainability targets.\n",
      "State 4: Action Taken: 1, Reward: 5.00, Suggestion: Maintain current efforts, but monitor for opportunities to improve efficiency.\n",
      "State 5: Action Taken: 2, Reward: -5.00, Suggestion: Increase efforts towards emission reduction to meet sustainability targets.\n",
      "State 6: Action Taken: 2, Reward: -5.00, Suggestion: Increase efforts towards emission reduction to meet sustainability targets.\n",
      "State 7: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 8: Action Taken: 2, Reward: -5.00, Suggestion: Increase efforts towards emission reduction to meet sustainability targets.\n",
      "State 9: Action Taken: 2, Reward: -5.00, Suggestion: Increase efforts towards emission reduction to meet sustainability targets.\n",
      "State 10: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 11: Action Taken: 1, Reward: 5.00, Suggestion: Maintain current efforts, but monitor for opportunities to improve efficiency.\n",
      "State 12: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 13: Action Taken: 1, Reward: 5.00, Suggestion: Maintain current efforts, but monitor for opportunities to improve efficiency.\n",
      "State 14: Action Taken: 2, Reward: -5.00, Suggestion: Increase efforts towards emission reduction to meet sustainability targets.\n",
      "State 15: Action Taken: 1, Reward: 5.00, Suggestion: Maintain current efforts, but monitor for opportunities to improve efficiency.\n",
      "State 16: Action Taken: 2, Reward: -5.00, Suggestion: Increase efforts towards emission reduction to meet sustainability targets.\n",
      "State 17: Action Taken: 2, Reward: -5.00, Suggestion: Increase efforts towards emission reduction to meet sustainability targets.\n",
      "State 18: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 19: Action Taken: 2, Reward: -5.00, Suggestion: Increase efforts towards emission reduction to meet sustainability targets.\n",
      "State 20: Action Taken: 2, Reward: -5.00, Suggestion: Increase efforts towards emission reduction to meet sustainability targets.\n",
      "State 21: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 22: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 23: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 24: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 25: Action Taken: 1, Reward: 5.00, Suggestion: Maintain current efforts, but monitor for opportunities to improve efficiency.\n",
      "State 26: Action Taken: 1, Reward: 5.00, Suggestion: Maintain current efforts, but monitor for opportunities to improve efficiency.\n",
      "State 27: Action Taken: 1, Reward: 5.00, Suggestion: Maintain current efforts, but monitor for opportunities to improve efficiency.\n",
      "State 28: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 29: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 30: Action Taken: 1, Reward: 5.00, Suggestion: Maintain current efforts, but monitor for opportunities to improve efficiency.\n",
      "State 31: Action Taken: 1, Reward: 5.00, Suggestion: Maintain current efforts, but monitor for opportunities to improve efficiency.\n",
      "State 32: Action Taken: 2, Reward: -5.00, Suggestion: Increase efforts towards emission reduction to meet sustainability targets.\n",
      "State 33: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 34: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 35: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 36: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 37: Action Taken: 2, Reward: -5.00, Suggestion: Increase efforts towards emission reduction to meet sustainability targets.\n",
      "State 38: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 39: Action Taken: 2, Reward: -5.00, Suggestion: Increase efforts towards emission reduction to meet sustainability targets.\n",
      "State 40: Action Taken: 2, Reward: -5.00, Suggestion: Increase efforts towards emission reduction to meet sustainability targets.\n",
      "State 41: Action Taken: 2, Reward: -5.00, Suggestion: Increase efforts towards emission reduction to meet sustainability targets.\n",
      "State 42: Action Taken: 1, Reward: 5.00, Suggestion: Maintain current efforts, but monitor for opportunities to improve efficiency.\n",
      "State 43: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 44: Action Taken: 1, Reward: 5.00, Suggestion: Maintain current efforts, but monitor for opportunities to improve efficiency.\n",
      "State 45: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 46: Action Taken: 1, Reward: 5.00, Suggestion: Maintain current efforts, but monitor for opportunities to improve efficiency.\n",
      "State 47: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 48: Action Taken: 2, Reward: -5.00, Suggestion: Increase efforts towards emission reduction to meet sustainability targets.\n",
      "State 49: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 50: Action Taken: 2, Reward: -5.00, Suggestion: Increase efforts towards emission reduction to meet sustainability targets.\n",
      "State 51: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 52: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 53: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 54: Action Taken: 1, Reward: 5.00, Suggestion: Maintain current efforts, but monitor for opportunities to improve efficiency.\n",
      "State 55: Action Taken: 1, Reward: 5.00, Suggestion: Maintain current efforts, but monitor for opportunities to improve efficiency.\n",
      "State 56: Action Taken: 1, Reward: 5.00, Suggestion: Maintain current efforts, but monitor for opportunities to improve efficiency.\n",
      "State 57: Action Taken: 1, Reward: 5.00, Suggestion: Maintain current efforts, but monitor for opportunities to improve efficiency.\n",
      "State 58: Action Taken: 1, Reward: 5.00, Suggestion: Maintain current efforts, but monitor for opportunities to improve efficiency.\n",
      "State 59: Action Taken: 1, Reward: 5.00, Suggestion: Maintain current efforts, but monitor for opportunities to improve efficiency.\n",
      "State 60: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 61: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 62: Action Taken: 1, Reward: 5.00, Suggestion: Maintain current efforts, but monitor for opportunities to improve efficiency.\n",
      "State 63: Action Taken: 2, Reward: -5.00, Suggestion: Increase efforts towards emission reduction to meet sustainability targets.\n",
      "State 64: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 65: Action Taken: 2, Reward: -5.00, Suggestion: Increase efforts towards emission reduction to meet sustainability targets.\n",
      "State 66: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 67: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 68: Action Taken: 2, Reward: -5.00, Suggestion: Increase efforts towards emission reduction to meet sustainability targets.\n",
      "State 69: Action Taken: 2, Reward: -5.00, Suggestion: Increase efforts towards emission reduction to meet sustainability targets.\n",
      "State 70: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 71: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 72: Action Taken: 1, Reward: 5.00, Suggestion: Maintain current efforts, but monitor for opportunities to improve efficiency.\n",
      "State 73: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 74: Action Taken: 1, Reward: 5.00, Suggestion: Maintain current efforts, but monitor for opportunities to improve efficiency.\n",
      "State 75: Action Taken: 2, Reward: -5.00, Suggestion: Increase efforts towards emission reduction to meet sustainability targets.\n",
      "State 76: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 77: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 78: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 79: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 80: Action Taken: 1, Reward: 5.00, Suggestion: Maintain current efforts, but monitor for opportunities to improve efficiency.\n",
      "State 81: Action Taken: 1, Reward: 5.00, Suggestion: Maintain current efforts, but monitor for opportunities to improve efficiency.\n",
      "State 82: Action Taken: 1, Reward: 5.00, Suggestion: Maintain current efforts, but monitor for opportunities to improve efficiency.\n",
      "State 83: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 84: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 85: Action Taken: 1, Reward: 5.00, Suggestion: Maintain current efforts, but monitor for opportunities to improve efficiency.\n",
      "State 86: Action Taken: 2, Reward: -5.00, Suggestion: Increase efforts towards emission reduction to meet sustainability targets.\n",
      "State 87: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 88: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 89: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 90: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 91: Action Taken: 2, Reward: -5.00, Suggestion: Increase efforts towards emission reduction to meet sustainability targets.\n",
      "State 92: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 93: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 94: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 95: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 96: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 97: Action Taken: 1, Reward: 5.00, Suggestion: Maintain current efforts, but monitor for opportunities to improve efficiency.\n",
      "State 98: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 99: Action Taken: 1, Reward: 5.00, Suggestion: Maintain current efforts, but monitor for opportunities to improve efficiency.\n",
      "State 100: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 101: Action Taken: 2, Reward: -5.00, Suggestion: Increase efforts towards emission reduction to meet sustainability targets.\n",
      "State 102: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 103: Action Taken: 1, Reward: 5.00, Suggestion: Maintain current efforts, but monitor for opportunities to improve efficiency.\n",
      "State 104: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 105: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 106: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 107: Action Taken: 2, Reward: -5.00, Suggestion: Increase efforts towards emission reduction to meet sustainability targets.\n",
      "State 108: Action Taken: 1, Reward: 5.00, Suggestion: Maintain current efforts, but monitor for opportunities to improve efficiency.\n",
      "State 109: Action Taken: 2, Reward: -5.00, Suggestion: Increase efforts towards emission reduction to meet sustainability targets.\n",
      "State 110: Action Taken: 1, Reward: 5.00, Suggestion: Maintain current efforts, but monitor for opportunities to improve efficiency.\n",
      "State 111: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 112: Action Taken: 1, Reward: 5.00, Suggestion: Maintain current efforts, but monitor for opportunities to improve efficiency.\n",
      "State 113: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 114: Action Taken: 1, Reward: 5.00, Suggestion: Maintain current efforts, but monitor for opportunities to improve efficiency.\n",
      "State 115: Action Taken: 1, Reward: 5.00, Suggestion: Maintain current efforts, but monitor for opportunities to improve efficiency.\n",
      "State 116: Action Taken: 1, Reward: 5.00, Suggestion: Maintain current efforts, but monitor for opportunities to improve efficiency.\n",
      "State 117: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 118: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 119: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 120: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 121: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 122: Action Taken: 1, Reward: 5.00, Suggestion: Maintain current efforts, but monitor for opportunities to improve efficiency.\n",
      "State 123: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 124: Action Taken: 1, Reward: 5.00, Suggestion: Maintain current efforts, but monitor for opportunities to improve efficiency.\n",
      "State 125: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 126: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 127: Action Taken: 2, Reward: -5.00, Suggestion: Increase efforts towards emission reduction to meet sustainability targets.\n",
      "State 128: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 129: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 130: Action Taken: 2, Reward: -5.00, Suggestion: Increase efforts towards emission reduction to meet sustainability targets.\n",
      "State 131: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 132: Action Taken: 1, Reward: 5.00, Suggestion: Maintain current efforts, but monitor for opportunities to improve efficiency.\n",
      "State 133: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 134: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 135: Action Taken: 1, Reward: 5.00, Suggestion: Maintain current efforts, but monitor for opportunities to improve efficiency.\n",
      "State 136: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 137: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 138: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 139: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 140: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 141: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 142: Action Taken: 1, Reward: 5.00, Suggestion: Maintain current efforts, but monitor for opportunities to improve efficiency.\n",
      "State 143: Action Taken: 2, Reward: -5.00, Suggestion: Increase efforts towards emission reduction to meet sustainability targets.\n",
      "State 144: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 145: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 146: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 147: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 148: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 149: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 150: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 151: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 152: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 153: Action Taken: 1, Reward: 5.00, Suggestion: Maintain current efforts, but monitor for opportunities to improve efficiency.\n",
      "State 154: Action Taken: 2, Reward: -5.00, Suggestion: Increase efforts towards emission reduction to meet sustainability targets.\n",
      "State 155: Action Taken: 1, Reward: 5.00, Suggestion: Maintain current efforts, but monitor for opportunities to improve efficiency.\n",
      "State 156: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 157: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 158: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 159: Action Taken: 2, Reward: -5.00, Suggestion: Increase efforts towards emission reduction to meet sustainability targets.\n",
      "State 160: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 161: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 162: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 163: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 164: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 165: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 166: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 167: Action Taken: 2, Reward: -5.00, Suggestion: Increase efforts towards emission reduction to meet sustainability targets.\n",
      "State 168: Action Taken: 2, Reward: -5.00, Suggestion: Increase efforts towards emission reduction to meet sustainability targets.\n",
      "State 169: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 170: Action Taken: 2, Reward: -5.00, Suggestion: Increase efforts towards emission reduction to meet sustainability targets.\n",
      "State 171: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 172: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 173: Action Taken: 1, Reward: 5.00, Suggestion: Maintain current efforts, but monitor for opportunities to improve efficiency.\n",
      "State 174: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 175: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 176: Action Taken: 1, Reward: 5.00, Suggestion: Maintain current efforts, but monitor for opportunities to improve efficiency.\n",
      "State 177: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 178: Action Taken: 2, Reward: -5.00, Suggestion: Increase efforts towards emission reduction to meet sustainability targets.\n",
      "State 179: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 180: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 181: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 182: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 183: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 184: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 185: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 186: Action Taken: 2, Reward: -5.00, Suggestion: Increase efforts towards emission reduction to meet sustainability targets.\n",
      "State 187: Action Taken: 1, Reward: 5.00, Suggestion: Maintain current efforts, but monitor for opportunities to improve efficiency.\n",
      "State 188: Action Taken: 2, Reward: -5.00, Suggestion: Increase efforts towards emission reduction to meet sustainability targets.\n",
      "State 189: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 190: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 191: Action Taken: 1, Reward: 5.00, Suggestion: Maintain current efforts, but monitor for opportunities to improve efficiency.\n",
      "State 192: Action Taken: 1, Reward: 5.00, Suggestion: Maintain current efforts, but monitor for opportunities to improve efficiency.\n",
      "State 193: Action Taken: 1, Reward: 5.00, Suggestion: Maintain current efforts, but monitor for opportunities to improve efficiency.\n",
      "State 194: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 195: Action Taken: 1, Reward: 5.00, Suggestion: Maintain current efforts, but monitor for opportunities to improve efficiency.\n",
      "State 196: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 197: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 198: Action Taken: 2, Reward: -5.00, Suggestion: Increase efforts towards emission reduction to meet sustainability targets.\n",
      "State 199: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "State 200: Action Taken: 0, Reward: 10.00, Suggestion: Reduce emissions by optimizing processes and investing in cleaner technology.\n",
      "\n",
      "Suggested Action for ESG Scores: Good ESG performance. Consider investing in sustainability.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "\n",
    "class CarbonEmissionsEnv:\n",
    "    def __init__(self):\n",
    "        self.state = np.array([0.5, 0.5, 0.5, 0.5])  \n",
    "        self.action_space = 3  \n",
    "\n",
    "    def step(self, action):\n",
    "        reward = 0\n",
    "        if action == 0: \n",
    "            reward = 10\n",
    "        elif action == 1:  \n",
    "            reward = 5\n",
    "        elif action == 2:  \n",
    "            reward = -5\n",
    "\n",
    "        self.state = np.random.rand(4)  \n",
    "        done = random.random() > 0.95  \n",
    "\n",
    "        return self.state, reward, done, {}\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95  \n",
    "        self.epsilon = 1.0  \n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state, verbose=0)\n",
    "        return np.argmax(act_values[0])\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target += self.gamma * np.amax(self.model.predict(next_state, verbose=0)[0])\n",
    "            target_f = self.model.predict(state, verbose=0)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "\n",
    "def retrieve_data(es, index_name):\n",
    "    try:\n",
    "        results = es.search(index=index_name, body={\"query\": {\"match_all\": {}}}, size=1000)\n",
    "        return [hit[\"_source\"] for hit in results[\"hits\"][\"hits\"]]\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving data: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def generate_suggestions(environmental_score, social_score, governance_score):\n",
    "    avg_score = (environmental_score + social_score + governance_score) / 3\n",
    "    if avg_score > 80:\n",
    "        return \"Excellent ESG performance. Focus on maintaining leadership.\"\n",
    "    elif avg_score > 50:\n",
    "        return \"Good ESG performance. Consider investing in sustainability.\"\n",
    "    else:\n",
    "        return \"ESG performance needs improvement. Prioritize actionable goals.\"\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    es = Elasticsearch([\"http://localhost:9200\"])\n",
    "    index_name = \"iot_sensor\"\n",
    "\n",
    "    \n",
    "    print(f\"Retrieving data from Elasticsearch index '{index_name}'...\")\n",
    "    iot_data = retrieve_data(es, index_name)\n",
    "\n",
    "    if iot_data:\n",
    "        \n",
    "        states = []\n",
    "        for doc in iot_data:\n",
    "            state = [\n",
    "                doc.get('sensor1', 0.0),  \n",
    "                doc.get('sensor2', 0.0),\n",
    "                doc.get('sensor3', 0.0),\n",
    "                doc.get('sensor4', 0.0)\n",
    "            ]\n",
    "            states.append(state)\n",
    "\n",
    "        \n",
    "        states = np.array(states)\n",
    "        states = (states - np.min(states, axis=0)) / (np.max(states, axis=0) - np.min(states, axis=0))\n",
    "\n",
    "        \n",
    "        state_size = states.shape[1]\n",
    "        action_size = 3  \n",
    "        env = CarbonEmissionsEnv()\n",
    "        agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "       \n",
    "        for idx, state in enumerate(states):\n",
    "            state = np.reshape(state, [1, state_size])\n",
    "            action = agent.act(state)  \n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "\n",
    "           \n",
    "            if action == 0:\n",
    "                suggestion = \"Reduce emissions by optimizing processes and investing in cleaner technology.\"\n",
    "            elif action == 1:\n",
    "                suggestion = \"Maintain current efforts, but monitor for opportunities to improve efficiency.\"\n",
    "            elif action == 2:\n",
    "                suggestion = \"Increase efforts towards emission reduction to meet sustainability targets.\"\n",
    "\n",
    "            print(f\"State {idx+1}: Action Taken: {action}, Reward: {reward:.2f}, Suggestion: {suggestion}\")\n",
    "\n",
    "            \n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            if len(agent.memory) >= 32:  \n",
    "                agent.replay(32)\n",
    "    else:\n",
    "        print(\"No data retrieved from Elasticsearch. Ensure the index contains documents.\")\n",
    "\n",
    "    \n",
    "    example_esg_scores = {\n",
    "        'Environmental_Score': 80,\n",
    "        'Social_Score': 60,\n",
    "        'Governance_Score': 70\n",
    "    }\n",
    "    suggestion = generate_suggestions(\n",
    "        environmental_score=example_esg_scores['Environmental_Score'],\n",
    "        social_score=example_esg_scores['Social_Score'],\n",
    "        governance_score=example_esg_scores['Governance_Score']\n",
    "    )\n",
    "    print(f\"\\nSuggested Action for ESG Scores: {suggestion}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (0.19.0)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (1.6.1)\n",
      "Requirement already satisfied: xgboost in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (3.0.0)\n",
      "Requirement already satisfied: elasticsearch in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (7.17.0)\n",
      "Requirement already satisfied: tensorflow in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (2.11.0)\n",
      "Requirement already satisfied: web3 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (7.9.0)\n",
      "Requirement already satisfied: nltk>=3.9 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from textblob) (3.9.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from scikit-learn) (3.6.0)\n",
      "Collecting urllib3<2,>=1.21.1 (from elasticsearch)\n",
      "  Using cached urllib3-1.26.20-py2.py3-none-any.whl.metadata (50 kB)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from elasticsearch) (2025.1.31)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from tensorflow) (3.13.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from tensorflow) (4.21.12)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from tensorflow) (75.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from tensorflow) (1.51.1)\n",
      "Requirement already satisfied: tensorboard<2.12,>=2.11 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from tensorflow) (2.11.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from tensorflow) (2.11.0)\n",
      "Requirement already satisfied: keras<2.12,>=2.11.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from tensorflow) (2.11.0)\n",
      "Requirement already satisfied: eth-abi>=5.0.1 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from web3) (5.2.0)\n",
      "Requirement already satisfied: eth-account>=0.13.1 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from web3) (0.13.5)\n",
      "Requirement already satisfied: eth-hash>=0.5.1 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from eth-hash[pycryptodome]>=0.5.1->web3) (0.7.1)\n",
      "Requirement already satisfied: eth-typing>=5.0.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from web3) (5.2.0)\n",
      "Requirement already satisfied: eth-utils>=5.0.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from web3) (5.2.0)\n",
      "Requirement already satisfied: hexbytes>=1.2.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from web3) (1.3.0)\n",
      "Requirement already satisfied: aiohttp>=3.7.4.post0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from web3) (3.11.14)\n",
      "Requirement already satisfied: pydantic>=2.4.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from web3) (2.10.6)\n",
      "Requirement already satisfied: requests>=2.23.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from web3) (2.32.3)\n",
      "Requirement already satisfied: types-requests>=2.0.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from web3) (2.32.0.20250306)\n",
      "Requirement already satisfied: websockets<14.0.0,>=10.0.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from web3) (13.1)\n",
      "Requirement already satisfied: pyunormalize>=15.0.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from web3) (16.0.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from aiohttp>=3.7.4.post0->web3) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from aiohttp>=3.7.4.post0->web3) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from aiohttp>=3.7.4.post0->web3) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from aiohttp>=3.7.4.post0->web3) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from aiohttp>=3.7.4.post0->web3) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from aiohttp>=3.7.4.post0->web3) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from aiohttp>=3.7.4.post0->web3) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from aiohttp>=3.7.4.post0->web3) (1.18.3)\n",
      "Requirement already satisfied: parsimonious<0.11.0,>=0.10.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from eth-abi>=5.0.1->web3) (0.10.0)\n",
      "Requirement already satisfied: bitarray>=2.4.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from eth-account>=0.13.1->web3) (3.2.0)\n",
      "Requirement already satisfied: eth-keyfile<0.9.0,>=0.7.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from eth-account>=0.13.1->web3) (0.8.1)\n",
      "Requirement already satisfied: eth-keys>=0.4.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from eth-account>=0.13.1->web3) (0.6.1)\n",
      "Requirement already satisfied: eth-rlp>=2.1.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from eth-account>=0.13.1->web3) (2.2.0)\n",
      "Requirement already satisfied: rlp>=1.0.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from eth-account>=0.13.1->web3) (4.1.0)\n",
      "Requirement already satisfied: ckzg>=2.0.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from eth-account>=0.13.1->web3) (2.1.0)\n",
      "Requirement already satisfied: pycryptodome<4,>=3.6.6 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from eth-hash[pycryptodome]>=0.5.1->web3) (3.22.0)\n",
      "Requirement already satisfied: cytoolz>=0.10.1 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from eth-utils>=5.0.0->web3) (1.0.1)\n",
      "Requirement already satisfied: click in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from nltk>=3.9->textblob) (8.1.8)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from nltk>=3.9->textblob) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from nltk>=3.9->textblob) (4.67.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from pydantic>=2.4.0->web3) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from pydantic>=2.4.0->web3) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from requests>=2.23.0->web3) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from requests>=2.23.0->web3) (3.10)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.38.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.45.1)\n",
      "INFO: pip is looking at multiple versions of types-requests to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting types-requests>=2.0.0 (from web3)\n",
      "  Downloading types_requests-2.32.0.20250301-py3-none-any.whl.metadata (2.3 kB)\n",
      "  Downloading types_requests-2.32.0.20241016-py3-none-any.whl.metadata (1.9 kB)\n",
      "  Downloading types_requests-2.32.0.20240914-py3-none-any.whl.metadata (1.9 kB)\n",
      "  Downloading types_requests-2.32.0.20240907-py3-none-any.whl.metadata (1.9 kB)\n",
      "  Downloading types_requests-2.32.0.20240905-py3-none-any.whl.metadata (1.9 kB)\n",
      "  Downloading types_requests-2.32.0.20240712-py3-none-any.whl.metadata (1.9 kB)\n",
      "  Downloading types_requests-2.32.0.20240622-py3-none-any.whl.metadata (1.8 kB)\n",
      "INFO: pip is still looking at multiple versions of types-requests to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading types_requests-2.32.0.20240602-py3-none-any.whl.metadata (1.8 kB)\n",
      "  Downloading types_requests-2.32.0.20240523-py3-none-any.whl.metadata (1.8 kB)\n",
      "  Downloading types_requests-2.32.0.20240521-py3-none-any.whl.metadata (1.8 kB)\n",
      "  Downloading types_requests-2.31.0.20240406-py3-none-any.whl.metadata (1.8 kB)\n",
      "  Downloading types_requests-2.31.0.20240403-py3-none-any.whl.metadata (1.8 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading types_requests-2.31.0.20240402-py3-none-any.whl.metadata (1.8 kB)\n",
      "  Downloading types_requests-2.31.0.20240311-py3-none-any.whl.metadata (1.8 kB)\n",
      "  Downloading types_requests-2.31.0.20240310-py3-none-any.whl.metadata (1.8 kB)\n",
      "  Downloading types_requests-2.31.0.20240218-py3-none-any.whl.metadata (1.8 kB)\n",
      "  Downloading types_requests-2.31.0.20240125-py3-none-any.whl.metadata (1.8 kB)\n",
      "  Downloading types_requests-2.31.0.20240106-py3-none-any.whl.metadata (1.8 kB)\n",
      "  Downloading types_requests-2.31.0.20231231-py3-none-any.whl.metadata (1.8 kB)\n",
      "  Downloading types_requests-2.31.0.10-py3-none-any.whl.metadata (1.8 kB)\n",
      "  Downloading types_requests-2.31.0.9-py3-none-any.whl.metadata (1.8 kB)\n",
      "  Downloading types_requests-2.31.0.8-py3-none-any.whl.metadata (1.6 kB)\n",
      "  Downloading types_requests-2.31.0.7-py3-none-any.whl.metadata (1.4 kB)\n",
      "  Downloading types_requests-2.31.0.6-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting types-urllib3 (from types-requests>=2.0.0->web3)\n",
      "  Downloading types_urllib3-1.26.25.14-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: toolz>=0.8.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from cytoolz>=0.10.1->eth-utils>=5.0.0->web3) (1.0.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (3.2.2)\n",
      "Downloading types_requests-2.31.0.6-py3-none-any.whl (14 kB)\n",
      "Using cached urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\n",
      "Downloading types_urllib3-1.26.25.14-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: types-urllib3, urllib3, types-requests\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.3.0\n",
      "    Uninstalling urllib3-2.3.0:\n",
      "      Successfully uninstalled urllib3-2.3.0\n",
      "  Attempting uninstall: types-requests\n",
      "    Found existing installation: types-requests 2.32.0.20250306\n",
      "    Uninstalling types-requests-2.32.0.20250306:\n",
      "      Successfully uninstalled types-requests-2.32.0.20250306\n",
      "Successfully installed types-requests-2.31.0.6 types-urllib3-1.26.25.14 urllib3-1.26.20\n"
     ]
    }
   ],
   "source": [
    "! pip install textblob pandas numpy scikit-learn xgboost elasticsearch tensorflow web3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: web3 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (7.9.0)\n",
      "Requirement already satisfied: eth-abi>=5.0.1 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from web3) (5.2.0)\n",
      "Requirement already satisfied: eth-account>=0.13.1 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from web3) (0.13.5)\n",
      "Requirement already satisfied: eth-hash>=0.5.1 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from eth-hash[pycryptodome]>=0.5.1->web3) (0.7.1)\n",
      "Requirement already satisfied: eth-typing>=5.0.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from web3) (5.2.0)\n",
      "Requirement already satisfied: eth-utils>=5.0.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from web3) (5.2.0)\n",
      "Requirement already satisfied: hexbytes>=1.2.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from web3) (1.3.0)\n",
      "Requirement already satisfied: aiohttp>=3.7.4.post0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from web3) (3.11.14)\n",
      "Requirement already satisfied: pydantic>=2.4.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from web3) (2.10.6)\n",
      "Requirement already satisfied: requests>=2.23.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from web3) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=4.0.1 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from web3) (4.12.2)\n",
      "Requirement already satisfied: types-requests>=2.0.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from web3) (2.31.0.6)\n",
      "Requirement already satisfied: websockets<14.0.0,>=10.0.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from web3) (13.1)\n",
      "Requirement already satisfied: pyunormalize>=15.0.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from web3) (16.0.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from aiohttp>=3.7.4.post0->web3) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from aiohttp>=3.7.4.post0->web3) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from aiohttp>=3.7.4.post0->web3) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from aiohttp>=3.7.4.post0->web3) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from aiohttp>=3.7.4.post0->web3) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from aiohttp>=3.7.4.post0->web3) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from aiohttp>=3.7.4.post0->web3) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from aiohttp>=3.7.4.post0->web3) (1.18.3)\n",
      "Requirement already satisfied: parsimonious<0.11.0,>=0.10.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from eth-abi>=5.0.1->web3) (0.10.0)\n",
      "Requirement already satisfied: bitarray>=2.4.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from eth-account>=0.13.1->web3) (3.2.0)\n",
      "Requirement already satisfied: eth-keyfile<0.9.0,>=0.7.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from eth-account>=0.13.1->web3) (0.8.1)\n",
      "Requirement already satisfied: eth-keys>=0.4.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from eth-account>=0.13.1->web3) (0.6.1)\n",
      "Requirement already satisfied: eth-rlp>=2.1.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from eth-account>=0.13.1->web3) (2.2.0)\n",
      "Requirement already satisfied: rlp>=1.0.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from eth-account>=0.13.1->web3) (4.1.0)\n",
      "Requirement already satisfied: ckzg>=2.0.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from eth-account>=0.13.1->web3) (2.1.0)\n",
      "Requirement already satisfied: pycryptodome<4,>=3.6.6 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from eth-hash[pycryptodome]>=0.5.1->web3) (3.22.0)\n",
      "Requirement already satisfied: cytoolz>=0.10.1 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from eth-utils>=5.0.0->web3) (1.0.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from pydantic>=2.4.0->web3) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from pydantic>=2.4.0->web3) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from requests>=2.23.0->web3) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from requests>=2.23.0->web3) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from requests>=2.23.0->web3) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from requests>=2.23.0->web3) (2025.1.31)\n",
      "Requirement already satisfied: types-urllib3 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from types-requests>=2.0.0->web3) (1.26.25.14)\n",
      "Requirement already satisfied: toolz>=0.8.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from cytoolz>=0.10.1->eth-utils>=5.0.0->web3) (1.0.0)\n",
      "Requirement already satisfied: regex>=2022.3.15 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from parsimonious<0.11.0,>=0.10.0->eth-abi>=5.0.1->web3) (2024.11.6)\n"
     ]
    }
   ],
   "source": [
    "! pip install web3 --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: web3 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (7.9.0)\n",
      "Requirement already satisfied: eth-abi>=5.0.1 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from web3) (5.2.0)\n",
      "Requirement already satisfied: eth-account>=0.13.1 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from web3) (0.13.5)\n",
      "Requirement already satisfied: eth-hash>=0.5.1 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from eth-hash[pycryptodome]>=0.5.1->web3) (0.7.1)\n",
      "Requirement already satisfied: eth-typing>=5.0.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from web3) (5.2.0)\n",
      "Requirement already satisfied: eth-utils>=5.0.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from web3) (5.2.0)\n",
      "Requirement already satisfied: hexbytes>=1.2.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from web3) (1.3.0)\n",
      "Requirement already satisfied: aiohttp>=3.7.4.post0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from web3) (3.11.14)\n",
      "Requirement already satisfied: pydantic>=2.4.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from web3) (2.10.6)\n",
      "Requirement already satisfied: requests>=2.23.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from web3) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=4.0.1 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from web3) (4.12.2)\n",
      "Requirement already satisfied: types-requests>=2.0.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from web3) (2.31.0.6)\n",
      "Requirement already satisfied: websockets<14.0.0,>=10.0.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from web3) (13.1)\n",
      "Requirement already satisfied: pyunormalize>=15.0.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from web3) (16.0.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from aiohttp>=3.7.4.post0->web3) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from aiohttp>=3.7.4.post0->web3) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from aiohttp>=3.7.4.post0->web3) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from aiohttp>=3.7.4.post0->web3) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from aiohttp>=3.7.4.post0->web3) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from aiohttp>=3.7.4.post0->web3) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from aiohttp>=3.7.4.post0->web3) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from aiohttp>=3.7.4.post0->web3) (1.18.3)\n",
      "Requirement already satisfied: parsimonious<0.11.0,>=0.10.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from eth-abi>=5.0.1->web3) (0.10.0)\n",
      "Requirement already satisfied: bitarray>=2.4.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from eth-account>=0.13.1->web3) (3.2.0)\n",
      "Requirement already satisfied: eth-keyfile<0.9.0,>=0.7.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from eth-account>=0.13.1->web3) (0.8.1)\n",
      "Requirement already satisfied: eth-keys>=0.4.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from eth-account>=0.13.1->web3) (0.6.1)\n",
      "Requirement already satisfied: eth-rlp>=2.1.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from eth-account>=0.13.1->web3) (2.2.0)\n",
      "Requirement already satisfied: rlp>=1.0.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from eth-account>=0.13.1->web3) (4.1.0)\n",
      "Requirement already satisfied: ckzg>=2.0.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from eth-account>=0.13.1->web3) (2.1.0)\n",
      "Requirement already satisfied: pycryptodome<4,>=3.6.6 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from eth-hash[pycryptodome]>=0.5.1->web3) (3.22.0)\n",
      "Requirement already satisfied: cytoolz>=0.10.1 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from eth-utils>=5.0.0->web3) (1.0.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from pydantic>=2.4.0->web3) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from pydantic>=2.4.0->web3) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from requests>=2.23.0->web3) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from requests>=2.23.0->web3) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from requests>=2.23.0->web3) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from requests>=2.23.0->web3) (2025.1.31)\n",
      "Requirement already satisfied: types-urllib3 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from types-requests>=2.0.0->web3) (1.26.25.14)\n",
      "Requirement already satisfied: toolz>=0.8.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from cytoolz>=0.10.1->eth-utils>=5.0.0->web3) (1.0.0)\n",
      "Requirement already satisfied: regex>=2022.3.15 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from parsimonious<0.11.0,>=0.10.0->eth-abi>=5.0.1->web3) (2024.11.6)\n"
     ]
    }
   ],
   "source": [
    "! pip install web3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wallet Address: 0x0BB8E0d56C43CEda853F0FcdbEe5FdC1076d9f41\n",
      "Private Key: 237bada3d4eb8bebc86d9c06b2201fbcb7e14f04bcfc067a59c8435ddd9fc2fc\n"
     ]
    }
   ],
   "source": [
    "from web3 import Web3\n",
    "\n",
    "# Generate a new account\n",
    "account = Web3().eth.account.create()\n",
    "\n",
    "# Get the public wallet address and private key\n",
    "wallet_address = account.address\n",
    "private_key = account.key.hex()\n",
    "\n",
    "# Display the wallet details\n",
    "print(f\"Wallet Address: {wallet_address}\")\n",
    "print(f\"Private Key: {private_key}\")\n",
    "\n",
    "# Store these securely\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Deep Q-Learning with Blockchain and Elasticsearch integration\n",
      "Wallet Balance: 0 ETH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8m/8kxc8bdn5dzfsqxv0r5qsqxw0000gp/T/ipykernel_28393/2013425783.py:90: DeprecationWarning: The 'body' parameter is deprecated for the 'search' API and will be removed in a future version. Instead use API parameters directly. See https://github.com/elastic/elasticsearch-py/issues/1698 for more information\n",
      "  response = es_client.search(index=INDEX_NAME, body={\"query\": query}, size=1000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queried 200 records from Elasticsearch\n",
      "Episode 1/100 - Total Reward: 6\n",
      "Episode 2/100 - Total Reward: 7\n",
      "Episode 3/100 - Total Reward: 13\n",
      "Episode 4/100 - Total Reward: 0\n",
      "Episode 5/100 - Total Reward: 18\n",
      "Episode 6/100 - Total Reward: 5\n",
      "Episode 7/100 - Total Reward: 10\n",
      "Episode 8/100 - Total Reward: 2\n",
      "Episode 9/100 - Total Reward: 9\n",
      "Episode 10/100 - Total Reward: 7\n",
      "Episode 11/100 - Total Reward: 13\n",
      "Episode 12/100 - Total Reward: 13\n",
      "Episode 13/100 - Total Reward: 6\n",
      "Episode 14/100 - Total Reward: 0\n",
      "Episode 15/100 - Total Reward: 4\n",
      "Episode 16/100 - Total Reward: 27\n",
      "Episode 17/100 - Total Reward: 36\n",
      "Episode 18/100 - Total Reward: 22\n",
      "Episode 19/100 - Total Reward: 1\n",
      "Episode 20/100 - Total Reward: 15\n",
      "Episode 21/100 - Total Reward: 8\n",
      "Episode 22/100 - Total Reward: 12\n",
      "Episode 23/100 - Total Reward: 9\n",
      "Episode 24/100 - Total Reward: 10\n",
      "Episode 25/100 - Total Reward: 9\n",
      "Episode 26/100 - Total Reward: 6\n",
      "Episode 27/100 - Total Reward: 14\n",
      "Episode 28/100 - Total Reward: 7\n",
      "Episode 29/100 - Total Reward: 13\n",
      "Episode 30/100 - Total Reward: 17\n",
      "Episode 31/100 - Total Reward: 7\n",
      "Episode 32/100 - Total Reward: 46\n",
      "Episode 33/100 - Total Reward: 20\n",
      "Episode 34/100 - Total Reward: 9\n",
      "Episode 35/100 - Total Reward: 10\n",
      "Episode 36/100 - Total Reward: 6\n",
      "Episode 37/100 - Total Reward: 59\n",
      "Episode 38/100 - Total Reward: 3\n",
      "Episode 39/100 - Total Reward: 4\n",
      "Episode 40/100 - Total Reward: 9\n",
      "Episode 41/100 - Total Reward: 25\n",
      "Episode 42/100 - Total Reward: 21\n",
      "Episode 43/100 - Total Reward: 24\n",
      "Episode 44/100 - Total Reward: 1\n",
      "Episode 45/100 - Total Reward: 3\n",
      "Episode 46/100 - Total Reward: 4\n",
      "Episode 47/100 - Total Reward: 16\n",
      "Episode 48/100 - Total Reward: 8\n",
      "Episode 49/100 - Total Reward: 6\n",
      "Episode 50/100 - Total Reward: 10\n",
      "Episode 51/100 - Total Reward: 20\n",
      "Episode 52/100 - Total Reward: 47\n",
      "Episode 53/100 - Total Reward: 9\n",
      "Episode 54/100 - Total Reward: 12\n",
      "Episode 55/100 - Total Reward: 10\n",
      "Episode 56/100 - Total Reward: 19\n",
      "Episode 57/100 - Total Reward: 9\n",
      "Episode 58/100 - Total Reward: 17\n",
      "Episode 59/100 - Total Reward: 9\n",
      "Episode 60/100 - Total Reward: 0\n",
      "Episode 61/100 - Total Reward: 14\n",
      "Episode 62/100 - Total Reward: 3\n",
      "Episode 63/100 - Total Reward: 6\n",
      "Episode 64/100 - Total Reward: 8\n",
      "Episode 65/100 - Total Reward: 13\n",
      "Episode 66/100 - Total Reward: 3\n",
      "Episode 67/100 - Total Reward: 3\n",
      "Episode 68/100 - Total Reward: 14\n",
      "Episode 69/100 - Total Reward: 2\n",
      "Episode 70/100 - Total Reward: 14\n",
      "Episode 71/100 - Total Reward: 11\n",
      "Episode 72/100 - Total Reward: 5\n",
      "Episode 73/100 - Total Reward: 11\n",
      "Episode 74/100 - Total Reward: 6\n",
      "Episode 75/100 - Total Reward: 9\n",
      "Episode 76/100 - Total Reward: 10\n",
      "Episode 77/100 - Total Reward: 1\n",
      "Episode 78/100 - Total Reward: 38\n",
      "Episode 79/100 - Total Reward: 9\n",
      "Episode 80/100 - Total Reward: 10\n",
      "Episode 81/100 - Total Reward: 3\n",
      "Episode 82/100 - Total Reward: 10\n",
      "Episode 83/100 - Total Reward: 9\n",
      "Episode 84/100 - Total Reward: 1\n",
      "Episode 85/100 - Total Reward: 7\n",
      "Episode 86/100 - Total Reward: 10\n",
      "Episode 87/100 - Total Reward: 18\n",
      "Episode 88/100 - Total Reward: 0\n",
      "Episode 89/100 - Total Reward: 10\n",
      "Episode 90/100 - Total Reward: 1\n",
      "Episode 91/100 - Total Reward: 7\n",
      "Episode 92/100 - Total Reward: 3\n",
      "Episode 93/100 - Total Reward: 7\n",
      "Episode 94/100 - Total Reward: 4\n",
      "Episode 95/100 - Total Reward: 8\n",
      "Episode 96/100 - Total Reward: 8\n",
      "Episode 97/100 - Total Reward: 24\n",
      "Episode 98/100 - Total Reward: 27\n",
      "Episode 99/100 - Total Reward: 5\n",
      "Episode 100/100 - Total Reward: 10\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import elasticsearch\n",
    "from web3 import Web3\n",
    "from collections import deque\n",
    "\n",
    "# Blockchain Configuration\n",
    "INFURA_URL = \"https://rpc.ankr.com/bsc/48b208604a24d6b44887751595a5f16a82814b1aecdb818edfd89f7a12da96b8\"\n",
    "web3 = Web3(Web3.HTTPProvider(INFURA_URL))\n",
    "CONTRACT_ADDRESS = \"0x1a4c3b6c7a08c0989Ff780514568ed85786aa659\"\n",
    "WALLET_ADDRESS = \"0x1a4c3b6c7a08c0989Ff780514568ed85786aa659\"\n",
    "PRIVATE_KEY = \"4c8d67458d12d22d8aebf2f8d13cba69afe3c01f77ab808e37ecd319b13b0bb9\"\n",
    "\n",
    "# Load Contract ABI\n",
    "with open(\"CarbonCreditTokenABI.json\", \"r\") as abi_file:\n",
    "    CONTRACT_ABI = json.load(abi_file)\n",
    "contract = web3.eth.contract(address=CONTRACT_ADDRESS, abi=CONTRACT_ABI)\n",
    "\n",
    "# Elasticsearch Configuration\n",
    "ES_HOST = \"http://localhost:9200\"\n",
    "es_client = elasticsearch.Elasticsearch([ES_HOST])\n",
    "INDEX_NAME = \"iot_sensor\"\n",
    "\n",
    "# Define Deep Q-Learning Parameters\n",
    "action_space = [\"mint\", \"burn\", \"transfer\"]\n",
    "state_size = 4  # Example state size\n",
    "action_size = len(action_space)\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.1\n",
    "epsilon_decay = 0.995\n",
    "learning_rate = 0.001\n",
    "gamma = 0.95\n",
    "episodes = 100\n",
    "batch_size = 32\n",
    "memory = deque(maxlen=2000)\n",
    "\n",
    "# Helper Functions\n",
    "def get_wallet_balance():\n",
    "    try:\n",
    "        balance = web3.eth.get_balance(WALLET_ADDRESS)\n",
    "        eth_balance = web3.from_wei(balance, 'ether')  # Use the correct method\n",
    "        print(f\"Wallet Balance: {eth_balance} ETH\")\n",
    "        return eth_balance\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving wallet balance: {e}\")\n",
    "        return None\n",
    "\n",
    "def send_transaction(receiver_address, amount_eth):\n",
    "    try:\n",
    "        amount_wei = Web3.toWei(amount_eth, 'ether')\n",
    "        transaction = {\n",
    "            'to': receiver_address,\n",
    "            'value': amount_wei,\n",
    "            'gas': 21000,\n",
    "            'gasPrice': Web3.toWei('50', 'gwei'),\n",
    "            'nonce': web3.eth.get_transaction_count(WALLET_ADDRESS),\n",
    "        }\n",
    "        signed_tx = web3.eth.account.sign_transaction(transaction, private_key=PRIVATE_KEY)\n",
    "        txn_hash = web3.eth.send_raw_transaction(signed_tx.rawTransaction)\n",
    "        print(f\"Transaction sent! Hash: {web3.toHex(txn_hash)}\")\n",
    "        receipt = web3.eth.wait_for_transaction_receipt(txn_hash)\n",
    "        print(\"Transaction confirmed.\")\n",
    "        return receipt\n",
    "    except Exception as e:\n",
    "        print(f\"Error sending transaction: {e}\")\n",
    "        return None\n",
    "\n",
    "def mint_tokens(to_address, amount):\n",
    "    try:\n",
    "        mint_tx = contract.functions.mint(to_address, amount).build_transaction({\n",
    "            'from': WALLET_ADDRESS,\n",
    "            'nonce': web3.eth.get_transaction_count(WALLET_ADDRESS),\n",
    "            'gas': 2000000,\n",
    "            'gasPrice': Web3.toWei('50', 'gwei'),\n",
    "        })\n",
    "        signed_tx = web3.eth.account.sign_transaction(mint_tx, private_key=PRIVATE_KEY)\n",
    "        txn_hash = web3.eth.send_raw_transaction(signed_tx.rawTransaction)\n",
    "        print(f\"Mint transaction sent! Hash: {web3.toHex(txn_hash)}\")\n",
    "        receipt = web3.eth.wait_for_transaction_receipt(txn_hash)\n",
    "        print(\"Mint transaction confirmed.\")\n",
    "        return receipt\n",
    "    except Exception as e:\n",
    "        print(f\"Error minting tokens: {e}\")\n",
    "        return None\n",
    "\n",
    "def query_elasticsearch():\n",
    "    try:\n",
    "        query = {\"match_all\": {}}\n",
    "        response = es_client.search(index=INDEX_NAME, body={\"query\": query}, size=1000)\n",
    "        hits = response[\"hits\"][\"hits\"]\n",
    "        data = [hit[\"_source\"] for hit in hits]\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying Elasticsearch: {e}\")\n",
    "        return []\n",
    "\n",
    "def replay(memory, model):\n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "    batch = random.sample(memory, batch_size)\n",
    "    for state, action, reward, next_state, done in batch:\n",
    "        target = reward\n",
    "        if not done:\n",
    "            target += gamma * np.amax(model.predict(next_state)[0])\n",
    "        target_f = model.predict(state)\n",
    "        target_f[0][action] = target\n",
    "        model.fit(state, target_f, epochs=1, verbose=0)\n",
    "\n",
    "def deep_q_learning():\n",
    "    global epsilon\n",
    "    for episode in range(episodes):\n",
    "        state = np.random.rand(1, state_size)  # Replace with actual initial state\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            if np.random.rand() <= epsilon:\n",
    "                action = random.randrange(action_size)\n",
    "            else:\n",
    "                action = np.argmax(model.predict(state)[0])\n",
    "\n",
    "            next_state = np.random.rand(1, state_size)  # Replace with actual state transition\n",
    "            reward = random.randint(0, 10)  # Replace with actual reward calculation\n",
    "            done = random.choice([True, False])  # Replace with actual terminal state condition\n",
    "\n",
    "            memory.append((state, action, reward, next_state, done))\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            if done:\n",
    "                print(f\"Episode {episode+1}/{episodes} - Total Reward: {total_reward}\")\n",
    "                break\n",
    "\n",
    "        replay(memory, model)\n",
    "\n",
    "        if epsilon > epsilon_min:\n",
    "            epsilon *= epsilon_decay\n",
    "\n",
    "# Placeholder for DQN model\n",
    "class DummyModel:\n",
    "    def predict(self, state):\n",
    "        return np.random.rand(1, action_size)\n",
    "\n",
    "    def fit(self, state, target, epochs, verbose):\n",
    "        pass\n",
    "\n",
    "model = DummyModel()\n",
    "\n",
    "def main():\n",
    "    print(\"Starting Deep Q-Learning with Blockchain and Elasticsearch integration\")\n",
    "    wallet_balance = get_wallet_balance()\n",
    "    if wallet_balance:\n",
    "        print(f\"Wallet balance: {wallet_balance} ETH\")\n",
    "\n",
    "    data = query_elasticsearch()\n",
    "    print(f\"Queried {len(data)} records from Elasticsearch\")\n",
    "\n",
    "    deep_q_learning()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Deep Q-Learning with Blockchain and Elasticsearch integration\n",
      "ESG Suggestions: Moderate ESG scores. Improve governance policies and social initiatives.\n",
      "Wallet Balance: 0 ETH\n",
      "Queried 200 records from Elasticsearch\n",
      "Episode 1/100 - Total Reward: 8\n",
      "Episode 2/100 - Total Reward: 8\n",
      "Episode 3/100 - Total Reward: 7\n",
      "Episode 4/100 - Total Reward: 17\n",
      "Episode 5/100 - Total Reward: 7\n",
      "Episode 6/100 - Total Reward: 5\n",
      "Episode 7/100 - Total Reward: 3\n",
      "Episode 8/100 - Total Reward: 26\n",
      "Episode 9/100 - Total Reward: 5\n",
      "Episode 10/100 - Total Reward: 4\n",
      "Episode 11/100 - Total Reward: 1\n",
      "Episode 12/100 - Total Reward: 6\n",
      "Episode 13/100 - Total Reward: 3\n",
      "Episode 14/100 - Total Reward: 13\n",
      "Episode 15/100 - Total Reward: 2\n",
      "Episode 16/100 - Total Reward: 10\n",
      "Episode 17/100 - Total Reward: 3\n",
      "Episode 18/100 - Total Reward: 20\n",
      "Episode 19/100 - Total Reward: 6\n",
      "Episode 20/100 - Total Reward: 21\n",
      "Episode 21/100 - Total Reward: 23\n",
      "Episode 22/100 - Total Reward: 6\n",
      "Episode 23/100 - Total Reward: 17\n",
      "Episode 24/100 - Total Reward: 10\n",
      "Episode 25/100 - Total Reward: 6\n",
      "Episode 26/100 - Total Reward: 7\n",
      "Episode 27/100 - Total Reward: 4\n",
      "Episode 28/100 - Total Reward: 11\n",
      "Episode 29/100 - Total Reward: 5\n",
      "Episode 30/100 - Total Reward: 4\n",
      "Episode 31/100 - Total Reward: 39\n",
      "Episode 32/100 - Total Reward: 9\n",
      "Episode 33/100 - Total Reward: 27\n",
      "Episode 34/100 - Total Reward: 10\n",
      "Episode 35/100 - Total Reward: 0\n",
      "Episode 36/100 - Total Reward: 10\n",
      "Episode 37/100 - Total Reward: 13\n",
      "Episode 38/100 - Total Reward: 2\n",
      "Episode 39/100 - Total Reward: 6\n",
      "Episode 40/100 - Total Reward: 6\n",
      "Episode 41/100 - Total Reward: 10\n",
      "Episode 42/100 - Total Reward: 11\n",
      "Episode 43/100 - Total Reward: 4\n",
      "Episode 44/100 - Total Reward: 32\n",
      "Episode 45/100 - Total Reward: 7\n",
      "Episode 46/100 - Total Reward: 3\n",
      "Episode 47/100 - Total Reward: 3\n",
      "Episode 48/100 - Total Reward: 10\n",
      "Episode 49/100 - Total Reward: 31\n",
      "Episode 50/100 - Total Reward: 5\n",
      "Episode 51/100 - Total Reward: 7\n",
      "Episode 52/100 - Total Reward: 3\n",
      "Episode 53/100 - Total Reward: 1\n",
      "Episode 54/100 - Total Reward: 10\n",
      "Episode 55/100 - Total Reward: 7\n",
      "Episode 56/100 - Total Reward: 6\n",
      "Episode 57/100 - Total Reward: 0\n",
      "Episode 58/100 - Total Reward: 5\n",
      "Episode 59/100 - Total Reward: 5\n",
      "Episode 60/100 - Total Reward: 0\n",
      "Episode 61/100 - Total Reward: 7\n",
      "Episode 62/100 - Total Reward: 29\n",
      "Episode 63/100 - Total Reward: 1\n",
      "Episode 64/100 - Total Reward: 3\n",
      "Episode 65/100 - Total Reward: 2\n",
      "Episode 66/100 - Total Reward: 19\n",
      "Episode 67/100 - Total Reward: 7\n",
      "Episode 68/100 - Total Reward: 9\n",
      "Episode 69/100 - Total Reward: 19\n",
      "Episode 70/100 - Total Reward: 20\n",
      "Episode 71/100 - Total Reward: 8\n",
      "Episode 72/100 - Total Reward: 2\n",
      "Episode 73/100 - Total Reward: 10\n",
      "Episode 74/100 - Total Reward: 5\n",
      "Episode 75/100 - Total Reward: 3\n",
      "Episode 76/100 - Total Reward: 29\n",
      "Episode 77/100 - Total Reward: 7\n",
      "Episode 78/100 - Total Reward: 10\n",
      "Episode 79/100 - Total Reward: 9\n",
      "Episode 80/100 - Total Reward: 10\n",
      "Episode 81/100 - Total Reward: 0\n",
      "Episode 82/100 - Total Reward: 4\n",
      "Episode 83/100 - Total Reward: 31\n",
      "Episode 84/100 - Total Reward: 9\n",
      "Episode 85/100 - Total Reward: 4\n",
      "Episode 86/100 - Total Reward: 16\n",
      "Episode 87/100 - Total Reward: 9\n",
      "Episode 88/100 - Total Reward: 33\n",
      "Episode 89/100 - Total Reward: 10\n",
      "Episode 90/100 - Total Reward: 14\n",
      "Episode 91/100 - Total Reward: 19\n",
      "Episode 92/100 - Total Reward: 9\n",
      "Episode 93/100 - Total Reward: 22\n",
      "Episode 94/100 - Total Reward: 6\n",
      "Episode 95/100 - Total Reward: 8\n",
      "Episode 96/100 - Total Reward: 2\n",
      "Episode 97/100 - Total Reward: 7\n",
      "Episode 98/100 - Total Reward: 2\n",
      "Episode 99/100 - Total Reward: 16\n",
      "Episode 100/100 - Total Reward: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8m/8kxc8bdn5dzfsqxv0r5qsqxw0000gp/T/ipykernel_28393/4068968636.py:92: DeprecationWarning: The 'body' parameter is deprecated for the 'search' API and will be removed in a future version. Instead use API parameters directly. See https://github.com/elastic/elasticsearch-py/issues/1698 for more information\n",
      "  response = es_client.search(index=INDEX_NAME, body={\"query\": query}, size=1000)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import elasticsearch\n",
    "from web3 import Web3\n",
    "from collections import deque\n",
    "from textblob import TextBlob\n",
    "\n",
    "INFURA_URL = \"https://rpc.ankr.com/bsc/48b208604a24d6b44887751595a5f16a82814b1aecdb818edfd89f7a12da96b8\"\n",
    "web3 = Web3(Web3.HTTPProvider(INFURA_URL))\n",
    "CONTRACT_ADDRESS = \"0x1a4c3b6c7a08c0989Ff780514568ed85786aa659\"\n",
    "WALLET_ADDRESS = \"0x1a4c3b6c7a08c0989Ff780514568ed85786aa659\"\n",
    "PRIVATE_KEY = \"4c8d67458d12d22d8aebf2f8d13cba69afe3c01f77ab808e37ecd319b13b0bb9\"\n",
    "\n",
    "with open(\"CarbonCreditTokenABI.json\", \"r\") as abi_file:\n",
    "    CONTRACT_ABI = json.load(abi_file)\n",
    "contract = web3.eth.contract(address=CONTRACT_ADDRESS, abi=CONTRACT_ABI)\n",
    "\n",
    "ES_HOST = \"http://localhost:9200\"\n",
    "es_client = elasticsearch.Elasticsearch([ES_HOST])\n",
    "INDEX_NAME = \"iot_sensor\"\n",
    "\n",
    "\n",
    "action_space = [\"mint\", \"burn\", \"transfer\"]\n",
    "state_size = 4  # Example state size\n",
    "action_size = len(action_space)\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.1\n",
    "epsilon_decay = 0.995\n",
    "learning_rate = 0.001\n",
    "gamma = 0.95\n",
    "episodes = 100\n",
    "batch_size = 32\n",
    "memory = deque(maxlen=2000)\n",
    "\n",
    "def sentiment_analysis(text):\n",
    "    blob = TextBlob(text)\n",
    "    return blob.sentiment.polarity\n",
    "\n",
    "\n",
    "def generate_suggestions(environmental_score, social_score, governance_score):\n",
    "    input_prompt = (\n",
    "        f\"Environmental Score: {environmental_score}, \"\n",
    "        f\"Social Score: {social_score}, \"\n",
    "        f\"Governance Score: {governance_score}.\"\n",
    "    )\n",
    "    sentiment = sentiment_analysis(input_prompt)\n",
    "    if sentiment > 0.5:\n",
    "        return \"Strong ESG scores! Focus on maintaining transparency and leveraging clean energy solutions.\"\n",
    "    elif 0 <= sentiment <= 0.5:\n",
    "        return \"Moderate ESG scores. Improve governance policies and social initiatives.\"\n",
    "    else:\n",
    "        return \"Low ESG scores. Focus on emission reductions, fair labor practices, and oversight.\"\n",
    "\n",
    "def get_wallet_balance():\n",
    "    try:\n",
    "        balance = web3.eth.get_balance(WALLET_ADDRESS)\n",
    "        eth_balance = web3.from_wei(balance, 'ether')\n",
    "        print(f\"Wallet Balance: {eth_balance} ETH\")\n",
    "        return eth_balance\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving wallet balance: {e}\")\n",
    "        return None\n",
    "\n",
    "def mint_tokens(to_address, amount):\n",
    "    try:\n",
    "        mint_tx = contract.functions.mint(to_address, amount).build_transaction({\n",
    "            'from': WALLET_ADDRESS,\n",
    "            'nonce': web3.eth.get_transaction_count(WALLET_ADDRESS),\n",
    "            'gas': 2000000,\n",
    "            'gasPrice': Web3.toWei('50', 'gwei'),\n",
    "        })\n",
    "        signed_tx = web3.eth.account.sign_transaction(mint_tx, private_key=PRIVATE_KEY)\n",
    "        txn_hash = web3.eth.send_raw_transaction(signed_tx.rawTransaction)\n",
    "        print(f\"Mint transaction sent! Hash: {web3.toHex(txn_hash)}\")\n",
    "        receipt = web3.eth.wait_for_transaction_receipt(txn_hash)\n",
    "        print(\"Mint transaction confirmed.\")\n",
    "        return receipt\n",
    "    except Exception as e:\n",
    "        print(f\"Error minting tokens: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def query_elasticsearch():\n",
    "    try:\n",
    "        query = {\"match_all\": {}}\n",
    "        response = es_client.search(index=INDEX_NAME, body={\"query\": query}, size=1000)\n",
    "        hits = response[\"hits\"][\"hits\"]\n",
    "        data = [hit[\"_source\"] for hit in hits]\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying Elasticsearch: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "class DummyModel:\n",
    "    def predict(self, state):\n",
    "        return np.random.rand(1, action_size)\n",
    "\n",
    "    def fit(self, state, target, epochs, verbose):\n",
    "        pass\n",
    "\n",
    "model = DummyModel()\n",
    "\n",
    "# Deep Q-Learning\n",
    "def replay(memory, model):\n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "    batch = random.sample(memory, batch_size)\n",
    "    for state, action, reward, next_state, done in batch:\n",
    "        target = reward\n",
    "        if not done:\n",
    "            target += gamma * np.amax(model.predict(next_state)[0])\n",
    "        target_f = model.predict(state)\n",
    "        target_f[0][action] = target\n",
    "        model.fit(state, target_f, epochs=1, verbose=0)\n",
    "\n",
    "def deep_q_learning():\n",
    "    global epsilon\n",
    "    for episode in range(episodes):\n",
    "        state = np.random.rand(1, state_size)  \n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            if np.random.rand() <= epsilon:\n",
    "                action = random.randrange(action_size)\n",
    "            else:\n",
    "                action = np.argmax(model.predict(state)[0])\n",
    "\n",
    "            next_state = np.random.rand(1, state_size)  \n",
    "            reward = random.randint(0, 10)  \n",
    "            done = random.choice([True, False])  \n",
    "\n",
    "            memory.append((state, action, reward, next_state, done))\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            if done:\n",
    "                print(f\"Episode {episode+1}/{episodes} - Total Reward: {total_reward}\")\n",
    "                break\n",
    "\n",
    "        replay(memory, model)\n",
    "\n",
    "        if epsilon > epsilon_min:\n",
    "            epsilon *= epsilon_decay\n",
    "\n",
    "def main():\n",
    "    print(\"Starting Deep Q-Learning with Blockchain and Elasticsearch integration\")\n",
    "    \n",
    "    \n",
    "    environmental_score = 75\n",
    "    social_score = 60\n",
    "    governance_score = 65\n",
    "    suggestions = generate_suggestions(environmental_score, social_score, governance_score)\n",
    "    print(\"ESG Suggestions:\", suggestions)\n",
    "\n",
    "  \n",
    "    wallet_balance = get_wallet_balance()\n",
    "    if wallet_balance:\n",
    "        print(f\"Wallet balance: {wallet_balance} ETH\")\n",
    "\n",
    "    \n",
    "    data = query_elasticsearch()\n",
    "    print(f\"Queried {len(data)} records from Elasticsearch\")\n",
    "\n",
    "    \n",
    "    deep_q_learning()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
